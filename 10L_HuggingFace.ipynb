{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lcocks/DS6050-DeepLearning/blob/main/10L_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface/brand-assets/resolve/main/hf-logo-with-title.png\" width=\"400\">\n",
        "\n",
        "https://huggingface.co/models\n",
        "\n",
        "The **Transformers library** (by HuggingFace) is the **industry standard** for working with pre-trained language models. Think of it as:\n",
        "\n",
        "> **\"PyTorch is the framework, Transformers is the model zoo + utilities\"**\n",
        "\n",
        "The library is built on three key principles:\n",
        "\n",
        "#### 1. **Standardized Interface**\n",
        "Every model follows the same API pattern:\n",
        "```python\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"model-name\")\n",
        "model = AutoModel.from_pretrained(\"model-name\")\n",
        "```\n",
        "\n",
        "#### 2. **Pre-trained Models + Tokenizers = Complete Package**\n",
        "\n",
        "#### 3. **Built ON TOP of PyTorch** (not replacing it)"
      ],
      "metadata": {
        "id": "leeyCuaQjqyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "print(\"\\n--- Basic Sentiment Analysis ---\")\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"I have waited for a HuggingFace tutorial longer than my sourdough starter.\")\n",
        "print(\"Text: 'I have waited for a HuggingFace tutorial longer than my sourdough starter.'\")\n",
        "print(f\"Result: {result}\")\n",
        "print(f\"Label: {result[0]['label']}, Score: {result[0]['score']:.4f}\")"
      ],
      "metadata": {
        "id": "QBV9mhhSj_h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Text Generation Pipeline ---\")\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2\"\n",
        ")\n",
        "generated = generator(\n",
        "    \"By the end of this course, you will finally learn how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2\n",
        ")\n",
        "print(\"Prompt: 'By the end of this course, you will finally learn how to'\")\n",
        "print(\"\\nGenerated sequences:\")\n",
        "for i, seq in enumerate(generated, 1):\n",
        "    print(f\"{i}. {seq['generated_text']}\")"
      ],
      "metadata": {
        "id": "SBFuHSbwkE4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Zero-Shot Classification ---\")\n",
        "zero_shot = pipeline(\"zero-shot-classification\")\n",
        "text = \"This course turns the Transformers library into legible magic for busy humans\"\n",
        "candidate_labels = [\"education\", \"politics\", \"business\"]\n",
        "result = zero_shot(text, candidate_labels)\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"Candidate labels: {candidate_labels}\")\n",
        "print(\"\\nResults:\")\n",
        "for label, score in zip(result['labels'], result['scores']):\n",
        "    print(f\"  {label}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "Xp_IyWvPkKmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Other Available Pipeline Tasks ---\")\n",
        "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "result = unmasker(\"The tallest animal is [MASK].\")\n",
        "print(\"\\nFill-mask example:\")\n",
        "print(\"Input: 'The tallest animal is [MASK].'\")\n",
        "print(f\"Top prediction: {result[0]['token_str']} (score: {result[0]['score']:.4f})\")\n",
        "\n",
        "\n",
        "print(\"\\nTop 5 predictions:\")\n",
        "# Iterate through the list of results\n",
        "for i, item in enumerate(result, 1):\n",
        "    print(f\"{i}. {item['token_str']} (score: {item['score']:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "id": "Cn1ZR-GYkPpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading Tokenizer and Model ---\")\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "print(f\"Loaded tokenizer and model: {model_name}\")\n",
        "print(f\"Model type: {type(model).__name__}\")\n",
        "print(f\"Tokenizer type: {type(tokenizer).__name__}\")\n",
        "classifier_custom = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "result = classifier_custom(\"I have waited for a HuggingFace tutorial longer than my sourdough starter.\")\n",
        "print(f\"\\nUsing custom tokenizer/model: {result}\")\n"
      ],
      "metadata": {
        "id": "YZuH5RYvkUA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Understanding Tokenization ---\")\n",
        "text = \"Greetings from the future, how are the GPUs today?\"\n",
        "encoded = tokenizer(text)\n",
        "print(f\"Text: '{text}'\")\n",
        "print(f\"\\nDirect tokenization:\")\n",
        "print(f\"  Input IDs: {encoded['input_ids']}\")\n",
        "print(f\"  Attention mask: {encoded['attention_mask']}\")\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(f\"\\nStep-by-step tokenization:\")\n",
        "print(f\"  Tokens: {tokens}\")\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"  Token IDs: {ids}\")\n",
        "decoded = tokenizer.decode(ids)\n",
        "print(f\"  Decoded: '{decoded}'\")\n",
        "print(f\"\\nAttention mask explanation:\")\n",
        "print(f\"  1 means: attend to this token\")\n",
        "print(f\"  0 means: ignore this token (padding)\")\n",
        "print(f\"\\nSpecial tokens:\")\n",
        "print(f\"  [CLS] token ID: {tokenizer.cls_token_id}\")\n",
        "print(f\"  [SEP] token ID: {tokenizer.sep_token_id}\")\n",
        "print(f\"  [PAD] token ID: {tokenizer.pad_token_id}\")"
      ],
      "metadata": {
        "id": "vgoEdm0JkX84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Tokenizing Batches ---\")\n",
        "sentences = [\n",
        "    \"This sentence is short and caffeinated.\",\n",
        "    \"This one is dramatically longer because it decided to explain its feelings about gradient descent at 2 AM.\",\n",
        "    \"Tiny.\"\n",
        "]\n",
        "batch = tokenizer(\n",
        "    sentences,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=20,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(\"Batch of sentences:\")\n",
        "for i, sent in enumerate(sentences):\n",
        "    print(f\"  {i+1}. {sent}\")\n",
        "print(f\"\\nTokenized batch:\")\n",
        "print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
        "print(f\"  Attention mask shape: {batch['attention_mask'].shape}\")\n",
        "print(f\"\\nFirst sentence tokens:\")\n",
        "print(f\"  Input IDs: {batch['input_ids'][0]}\")\n",
        "print(f\"  Attention mask: {batch['attention_mask'][0]}\")\n",
        "print(f\"  (Notice the padding tokens at the end)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 3: PYTORCH INTEGRATION\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "WOX3BZJFkcBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Pipeline with Multiple Inputs ---\")\n",
        "x_train = [\n",
        "    \"This gadget sparks joy and eats my chores for breakfast.\",\n",
        "    \"This update is a dumpster fire wearing a bow tie.\",\n",
        "    \"Meh, it works, like decaf coffee.\",\n",
        "    \"Absolute masterpiece; I would frame the receipt.\"\n",
        "]\n",
        "results = classifier(x_train)\n",
        "print(\"Sentiment analysis results:\")\n",
        "for text_item, result in zip(x_train, results):\n",
        "    print(f\"  '{text_item[:50]}...' -> {result['label']} ({result['score']:.3f})\")\n"
      ],
      "metadata": {
        "id": "wTuxX68ekhtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Manual PyTorch Inference ---\")\n",
        "batch = tokenizer(\n",
        "    x_train,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(f\"Tokenized batch:\")\n",
        "print(f\"  Type: {type(batch)}\")\n",
        "print(f\"  Keys: {batch.keys()}\")\n",
        "print(f\"  Input IDs shape: {batch['input_ids'].shape}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(**batch)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    predictions = torch.argmax(probabilities, dim=-1)\n",
        "print(f\"\\nManual inference results:\")\n",
        "print(f\"  Logits shape: {logits.shape}\")\n",
        "print(f\"  Probabilities: {probabilities}\")\n",
        "print(f\"  Predicted labels: {predictions}\")\n",
        "id2label = model.config.id2label\n",
        "for i, (text_item, pred, probs) in enumerate(zip(x_train, predictions, probabilities)):\n",
        "    label = id2label[pred.item()]\n",
        "    confidence = probs.max().item()\n",
        "    print(f\"  {i+1}. {label} ({confidence:.3f})\")"
      ],
      "metadata": {
        "id": "O7wRqywVkluO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Transformers Models ARE PyTorch Models ---\")\n",
        "print(f\"Is model a PyTorch Module? {isinstance(model, torch.nn.Module)}\")\n",
        "print(f\"Model class: {type(model)}\")\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(\"\\nPyTorch operations work:\")\n",
        "print(f\"  model.train() - sets training mode\")\n",
        "print(f\"  model.eval() - sets evaluation mode\")\n",
        "print(f\"  model.parameters() - get parameters for optimizer\")\n",
        "print(f\"  model.to(device) - move to GPU/CPU\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 4: SAVING AND LOADING MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n--- Saving Model and Tokenizer ---\")\n",
        "save_directory = \"./my_saved_model\"\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "print(f\"✓ Tokenizer saved to {save_directory}\")\n",
        "model.save_pretrained(save_directory)\n",
        "print(f\"✓ Model saved to {save_directory}\")\n",
        "print(f\"\\nSaved files include:\")\n",
        "print(f\"  - config.json (model configuration)\")\n",
        "print(f\"  - pytorch_model.bin (model weights)\")\n",
        "print(f\"  - tokenizer.json (tokenizer configuration)\")\n",
        "print(f\"  - vocab.txt (vocabulary)\")\n"
      ],
      "metadata": {
        "id": "dSqNEzJaktYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Loading Model and Tokenizer ---\")\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "print(f\"✓ Tokenizer loaded from {save_directory}\")\n",
        "loaded_model = AutoModelForSequenceClassification.from_pretrained(save_directory)\n",
        "print(f\"✓ Model loaded from {save_directory}\")\n",
        "test_text = \"This reloaded model performs like it had an extra espresso.\"\n",
        "loaded_inputs = loaded_tokenizer(test_text, return_tensors=\"pt\")\n",
        "loaded_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = loaded_model(**loaded_inputs)\n",
        "    prediction = torch.argmax(outputs.logits, dim=-1)\n",
        "    label = loaded_model.config.id2label[prediction.item()]\n",
        "print(f\"\\nTest with loaded model:\")\n",
        "print(f\"  Text: '{test_text}'\")\n",
        "print(f\"  Prediction: {label}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SECTION 5: USING THE MODEL HUB\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "id": "JaK-esvdkwtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Using Models from the Hub ---\")\n",
        "print(\"\\nExample 1: Summarization\")\n",
        "summarizer = pipeline(\n",
        "    \"summarization\",\n",
        "    model=\"facebook/bart-large-cnn\"\n",
        ")\n",
        "article = \"\"\"\n",
        "The HuggingFace Transformers library provides thousands of pretrained models\n",
        "to perform tasks on different modalities such as text, vision, and audio.\n",
        "These models can be applied on text for tasks like classification, information\n",
        "extraction, question answering, summarization, translation, and text generation\n",
        "in over 100 languages. The library also provides easy-to-use APIs that allow\n",
        "you to quickly download and use those pretrained models on a given text,\n",
        "fine-tune them on your own datasets, and share them with the community.\n",
        "\"\"\"\n",
        "summary = summarizer(article, max_length=50, min_length=25, do_sample=False)\n",
        "print(f\"Original length: {len(article)} characters\")\n",
        "print(f\"Summary: {summary[0]['summary_text']}\")\n",
        "\n",
        "print(\"\\nExample 2: German Sentiment Analysis\")\n",
        "german_classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"oliverguhr/german-sentiment-bert\"\n",
        ")\n",
        "german_text = \"Dieses Tutorial ist so gut, dass mein Kaffee Applaus spendet!\"\n",
        "result = german_classifier(german_text)\n",
        "print(f\"Text: '{german_text}'\")\n",
        "print(f\"Result: {result}\")"
      ],
      "metadata": {
        "id": "yyC5ssZAk1S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Finding Models on the Hub ---\")\n",
        "print(\"\"\"\n",
        "To find models:\n",
        "1. Go to huggingface.co/models\n",
        "2. Filter by task, library, language, or dataset\n",
        "3. Use the search bar\n",
        "4. Click on a model to see:\n",
        "   - Model card (documentation)\n",
        "   - Code examples\n",
        "   - Demo widgets\n",
        "5. Copy the model name and use it in your code!\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "n-UIag5olGA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "APIs & Mechanics\n",
        "\n",
        "I'm focusing on contrasting the ease of Hugging Face pipelines with the more granular PyTorch implementations."
      ],
      "metadata": {
        "id": "aV2iOPcdmI4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#------ APIs & Mechanics: contrasting the ease of Hugging Face pipelines with the more granular PyTorch implementations.\n",
        "import torch\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM\n",
        ")\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\\n\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ANALOGY 1: SEQUENCE CLASSIFICATION (Sentiment Analysis)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MODEL_NAME_CLS = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer_cls = AutoTokenizer.from_pretrained(MODEL_NAME_CLS)\n",
        "model_cls = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_CLS).to(DEVICE)\n",
        "\n",
        "text_cls = \"Hugging Face makes NLP incredibly easy!\"\n",
        "\n",
        "print(\"\\n--- 1.1 Pipeline (High-Level Abstraction) ---\")\n",
        "classifier = pipeline(\"sentiment-analysis\", model=model_cls, tokenizer=tokenizer_cls, device=DEVICE)\n",
        "result_pipe = classifier(text_cls)\n",
        "print(f\"Input: '{text_cls}'\")\n",
        "print(f\"Output: {result_pipe}\")\n",
        "\n",
        "print(\"\\n--- 1.2 PyTorch Equivalent (The Mechanics) ---\")\n",
        "model_cls.eval()\n",
        "inputs_cls = tokenizer_cls(text_cls, return_tensors=\"pt\").to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    outputs_cls = model_cls(**inputs_cls)\n",
        "logits_cls = outputs_cls.logits\n",
        "probabilities_cls = torch.nn.functional.softmax(logits_cls, dim=-1)\n",
        "predicted_class_id = torch.argmax(probabilities_cls, dim=-1).item()\n",
        "predicted_label = model_cls.config.id2label[predicted_class_id]\n",
        "confidence = probabilities_cls[0, predicted_class_id].item()\n",
        "print(f\"Logits Shape: {logits_cls.shape}\")\n",
        "print(f\"Predicted Label: {predicted_label}\")\n",
        "print(f\"Confidence: {confidence:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALOGY 2: MASKED LANGUAGE MODELING (Fill-Mask)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MODEL_NAME_MLM = \"bert-base-uncased\"\n",
        "tokenizer_mlm = AutoTokenizer.from_pretrained(MODEL_NAME_MLM)\n",
        "model_mlm = AutoModelForMaskedLM.from_pretrained(MODEL_NAME_MLM).to(DEVICE)\n",
        "model_mlm.eval()\n",
        "\n",
        "text_mlm = \"The capital of France is [MASK].\"\n",
        "\n",
        "print(\"\\n--- 2.1 Pipeline (High-Level Abstraction) ---\")\n",
        "fill_mask = pipeline(\"fill-mask\", model=model_mlm, tokenizer=tokenizer_mlm, device=DEVICE)\n",
        "result_mlm_pipe = fill_mask(text_mlm, top_k=3)\n",
        "print(f\"Input: '{text_mlm}'\")\n",
        "print(f\"Output: {result_mlm_pipe}\")\n",
        "\n",
        "print(\"\\n--- 2.2 PyTorch Equivalent (The Mechanics) ---\")\n",
        "inputs_mlm = tokenizer_mlm(text_mlm, return_tensors=\"pt\").to(DEVICE)\n",
        "mask_token_index = torch.where(inputs_mlm[\"input_ids\"] == tokenizer_mlm.mask_token_id)[1]\n",
        "with torch.no_grad():\n",
        "    outputs_mlm = model_mlm(**inputs_mlm)\n",
        "logits_mlm = outputs_mlm.logits\n",
        "mask_token_logits = logits_mlm[0, mask_token_index, :]\n",
        "top_k = 3\n",
        "top_k_tokens = torch.topk(mask_token_logits, top_k, dim=1)\n",
        "top_k_ids = top_k_tokens.indices[0].tolist()\n",
        "probabilities_mlm = torch.nn.functional.softmax(mask_token_logits, dim=-1)\n",
        "top_k_probs = torch.topk(probabilities_mlm, top_k, dim=1).values[0].tolist()\n",
        "print(f\"Logits Shape (Full Sequence): {logits_mlm.shape}\")\n",
        "print(f\"Logits Shape (Masked Position): {mask_token_logits.shape}\")\n",
        "print(\"\\nTop predictions:\")\n",
        "for token_id, prob in zip(top_k_ids, top_k_probs):\n",
        "    token_str = tokenizer_mlm.decode([token_id])\n",
        "    print(f\"- {token_str} (Probability: {prob:.4f})\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALOGY 3: TOKEN CLASSIFICATION (NER)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MODEL_NAME_NER = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
        "tokenizer_ner = AutoTokenizer.from_pretrained(MODEL_NAME_NER)\n",
        "model_ner = AutoModelForTokenClassification.from_pretrained(MODEL_NAME_NER).to(DEVICE)\n",
        "model_ner.eval()\n",
        "\n",
        "text_ner = \"My name is Wolfgang and I live in Berlin.\"\n",
        "\n",
        "print(\"\\n--- 3.1 Pipeline (High-Level Abstraction) ---\")\n",
        "ner = pipeline(\"ner\", model=model_ner, tokenizer=tokenizer_ner, device=DEVICE, aggregation_strategy=\"simple\")\n",
        "result_ner_pipe = ner(text_ner)\n",
        "print(f\"Input: '{text_ner}'\")\n",
        "print(f\"Output: {result_ner_pipe}\")\n",
        "\n",
        "print(\"\\n--- 3.2 PyTorch Equivalent (The Mechanics) ---\")\n",
        "inputs_ner = tokenizer_ner(text_ner, return_tensors=\"pt\").to(DEVICE)\n",
        "tokens = tokenizer_ner.convert_ids_to_tokens(inputs_ner[\"input_ids\"][0])\n",
        "with torch.no_grad():\n",
        "    outputs_ner = model_ner(**inputs_ner)\n",
        "logits_ner = outputs_ner.logits\n",
        "predictions_ner = torch.argmax(logits_ner, dim=-1)[0]\n",
        "print(f\"Logits Shape: {logits_ner.shape}\")\n",
        "print(\"\\nToken-level predictions:\")\n",
        "for token, prediction_id in zip(tokens, predictions_ner):\n",
        "    if token not in tokenizer_ner.all_special_tokens:\n",
        "        label = model_ner.config.id2label[prediction_id.item()]\n",
        "        print(f\"{token:<15} : {label}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALOGY 4: TEXT GENERATION (Causal LM)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MODEL_NAME_CLM = \"gpt2\"\n",
        "tokenizer_clm = AutoTokenizer.from_pretrained(MODEL_NAME_CLM)\n",
        "model_clm = AutoModelForCausalLM.from_pretrained(MODEL_NAME_CLM).to(DEVICE)\n",
        "model_clm.eval()\n",
        "if tokenizer_clm.pad_token is None:\n",
        "    tokenizer_clm.pad_token = tokenizer_clm.eos_token\n",
        "\n",
        "text_clm = \"Transformers revolutionized NLP by\"\n",
        "\n",
        "print(\"\\n--- 4.1 Hugging Face Generate (High-Level Abstraction) ---\")\n",
        "inputs_clm = tokenizer_clm(text_clm, return_tensors=\"pt\").to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    output_sequences = model_clm.generate(\n",
        "        **inputs_clm,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer_clm.eos_token_id\n",
        "    )\n",
        "generated_text_gen = tokenizer_clm.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(f\"Input: '{text_clm}'\")\n",
        "print(f\"Output:\\n{generated_text_gen}\")\n",
        "\n",
        "print(\"\\n--- 4.2 PyTorch Equivalent (The Autoregressive Loop) ---\")\n",
        "current_input_ids = tokenizer_clm(text_clm, return_tensors=\"pt\").input_ids.to(DEVICE)\n",
        "max_new_tokens = 20\n",
        "print(\"Starting manual autoregressive loop (Greedy Search):\")\n",
        "for _ in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs_clm = model_clm(input_ids=current_input_ids)\n",
        "    next_token_logits = outputs_clm.logits[:, -1, :]\n",
        "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "    current_input_ids = torch.cat([current_input_ids, next_token_id], dim=1)\n",
        "    if next_token_id.item() == tokenizer_clm.eos_token_id:\n",
        "        break\n",
        "generated_text_manual = tokenizer_clm.decode(current_input_ids[0], skip_special_tokens=True)\n",
        "print(f\"\\nGenerated Output:\\n{generated_text_manual}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALOGY 5: TRANSLATION (Seq2Seq)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "MODEL_NAME_S2S = \"t5-small\"\n",
        "tokenizer_s2s = AutoTokenizer.from_pretrained(MODEL_NAME_S2S, model_max_length=512)\n",
        "model_s2s = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_S2S).to(DEVICE)\n",
        "model_s2s.eval()\n",
        "\n",
        "text_s2s = \"translate English to German: The house is wonderful.\"\n",
        "\n",
        "print(\"\\n--- 5.1 Hugging Face Generate (High-Level Abstraction) ---\")\n",
        "inputs_s2s = tokenizer_s2s(text_s2s, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    output_ids = model_s2s.generate(\n",
        "        inputs_s2s.input_ids,\n",
        "        max_length=20,\n",
        "        num_beams=1,\n",
        "    )\n",
        "translation_gen = tokenizer_s2s.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(f\"Input: '{text_s2s}'\")\n",
        "print(f\"Output:\\n{translation_gen}\")\n",
        "\n",
        "print(\"\\n--- 5.2 PyTorch Equivalent (Encoder-Decoder Interaction) ---\")\n",
        "encoder_inputs = tokenizer_s2s(text_s2s, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    encoder_outputs = model_s2s.get_encoder()(**encoder_inputs)\n",
        "encoder_hidden_states = encoder_outputs.last_hidden_state\n",
        "decoder_input_ids = torch.tensor([[model_s2s.config.decoder_start_token_id]], device=DEVICE)\n",
        "max_length = 20\n",
        "print(\"Starting manual Seq2Seq loop:\")\n",
        "for _ in range(max_length):\n",
        "    with torch.no_grad():\n",
        "        decoder_outputs = model_s2s.get_decoder()(\n",
        "            input_ids=decoder_input_ids,\n",
        "            encoder_hidden_states=encoder_hidden_states\n",
        "        )\n",
        "    sequence_output = decoder_outputs[0]\n",
        "    if model_s2s.config.tie_word_embeddings:\n",
        "        sequence_output = sequence_output * (model_s2s.model_dim**-0.5)\n",
        "    lm_logits = model_s2s.lm_head(sequence_output)\n",
        "    next_token_logits = lm_logits[:, -1, :]\n",
        "    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "    decoder_input_ids = torch.cat([decoder_input_ids, next_token_id], dim=1)\n",
        "    if next_token_id.item() == tokenizer_s2s.eos_token_id:\n",
        "        break\n",
        "translation_manual = tokenizer_s2s.decode(decoder_input_ids[0], skip_special_tokens=True)\n",
        "print(f\"\\nEncoder Hidden States Shape: {encoder_hidden_states.shape}\")\n",
        "print(f\"Generated Output:\\n{translation_manual}\")\n"
      ],
      "metadata": {
        "id": "RGb2Vb3eiwbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4aedf13"
      },
      "source": [
        "%pip install evaluate datasets transformers accelerate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import logging\n",
        "import shutil\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# Quiet logs\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "# Devices\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "pipe_device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device for computation: {device}\")\n",
        "\n",
        "# ===== BERT (Encoder-Only) =====\n",
        "MODEL_NAME_BERT = \"bert-base-uncased\"\n",
        "tokenizer_bert = AutoTokenizer.from_pretrained(MODEL_NAME_BERT)\n",
        "model_bert_mlm = AutoModelForMaskedLM.from_pretrained(MODEL_NAME_BERT).to(device)\n",
        "model_bert_mlm.eval()\n",
        "\n",
        "text = \"The quick brown [MASK] jumps over the lazy dog.\"\n",
        "inputs = tokenizer_bert(text, return_tensors=\"pt\").to(device)\n",
        "print(f\"Input text: '{text}'\")\n",
        "print(f\"Tokens: {tokenizer_bert.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
        "\n",
        "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer_bert.mask_token_id)[1]\n",
        "with torch.no_grad():\n",
        "    logits = model_bert_mlm(**inputs).logits\n",
        "\n",
        "mask_token_logits = logits[0, mask_token_index, :]\n",
        "top_5_tokens_ids = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "print(\"\\nBERT's top predictions for [MASK]:\")\n",
        "for token_id in top_5_tokens_ids:\n",
        "    print(f\"- {tokenizer_bert.decode([token_id])}\")\n",
        "\n",
        "fill_mask = pipeline(\"fill-mask\", model=model_bert_mlm, tokenizer=tokenizer_bert, device=pipe_device)\n",
        "text1 = \"We sat by the river [MASK] and enjoyed the view.\"\n",
        "text2 = \"I need to deposit money at the [MASK] before the meeting.\"\n",
        "print(f\"\\nContext 1 (Riverside): '{text1}'\")\n",
        "for pred in fill_mask(text1, top_k=3):\n",
        "    print(f\"- {pred['token_str']} (Score: {pred['score']:.4f})\")\n",
        "print(f\"\\nContext 2 (Financial): '{text2}'\")\n",
        "for pred in fill_mask(text2, top_k=3):\n",
        "    print(f\"- {pred['token_str']} (Score: {pred['score']:.4f})\")\n"
      ],
      "metadata": {
        "id": "WeWMPw9UjfUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== GPT (Decoder-Only) =====\n",
        "MODEL_NAME_GPT = \"gpt2\"\n",
        "tokenizer_gpt = AutoTokenizer.from_pretrained(MODEL_NAME_GPT)\n",
        "model_gpt = AutoModelForCausalLM.from_pretrained(MODEL_NAME_GPT).to(device)\n",
        "model_gpt.eval()\n",
        "\n",
        "prompt = \"The breakthrough of Transformers in NLP revolutionized the field by\"\n",
        "inputs_gpt = tokenizer_gpt(prompt, return_tensors=\"pt\").to(device)\n",
        "print(f\"\\nPrompt: '{prompt}'\")\n",
        "print(\"Generating continuation...\")\n",
        "with torch.no_grad():\n",
        "    output_sequences = model_gpt.generate(\n",
        "        input_ids=inputs_gpt[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer_gpt.eos_token_id,\n",
        "    )\n",
        "generated_text = tokenizer_gpt.decode(output_sequences[0], skip_special_tokens=True)\n",
        "print(\"\\nGenerated Text:\")\n",
        "print(generated_text)\n",
        "\n",
        "print(\"\\n--- Zero-Shot Prompt (Sentiment) ---\")\n",
        "prompt_zero_shot = \"\"\"\n",
        "Analyze the sentiment of the following movie review.\n",
        "\n",
        "Review: The acting was incredible and the story was moving.\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "inputs_zs = tokenizer_gpt(prompt_zero_shot, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    output_zs = model_gpt.generate(\n",
        "        input_ids=inputs_zs[\"input_ids\"],\n",
        "        max_new_tokens=5,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer_gpt.eos_token_id,\n",
        "    )\n",
        "print(tokenizer_gpt.decode(output_zs[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n--- Few-Shot Prompt (Translation) ---\")\n",
        "prompt_few_shot = \"\"\"\n",
        "Translate English to French:\n",
        "\n",
        "English: Hello\n",
        "French: Bonjour\n",
        "\n",
        "English: Goodbye\n",
        "French: Au revoir\n",
        "\n",
        "English: Thank you\n",
        "French:\n",
        "\"\"\"\n",
        "inputs_fs = tokenizer_gpt(prompt_few_shot, return_tensors=\"pt\").to(device)\n",
        "with torch.no_grad():\n",
        "    output_fs = model_gpt.generate(\n",
        "        input_ids=inputs_fs[\"input_ids\"],\n",
        "        max_new_tokens=5,\n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer_gpt.eos_token_id,\n",
        "    )\n",
        "print(tokenizer_gpt.decode(output_fs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "KCoZJKnmm7Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===== T5 (Encoder-Decoder) =====\n",
        "MODEL_NAME_T5 = \"t5-small\"\n",
        "tokenizer_t5 = AutoTokenizer.from_pretrained(MODEL_NAME_T5, model_max_length=512)\n",
        "model_t5 = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME_T5).to(device)\n",
        "model_t5.eval()\n",
        "\n",
        "input_text_translation = \"translate English to German: Hello, welcome to the DS6050.\"\n",
        "print(f\"\\nInput (Translation): {input_text_translation}\")\n",
        "input_ids_t5 = tokenizer_t5(input_text_translation, return_tensors=\"pt\").input_ids.to(device)\n",
        "with torch.no_grad():\n",
        "    outputs_t5 = model_t5.generate(input_ids_t5, max_length=20)\n",
        "decoded_output = tokenizer_t5.decode(outputs_t5[0], skip_special_tokens=True)\n",
        "print(f\"Output: {decoded_output}\")\n",
        "\n",
        "long_text = \"\"\"\n",
        "The Transformer architecture was introduced in 2017.\n",
        "It revolutionized NLP by moving away from RNNs and LSTMs, relying\n",
        "solely on attention mechanisms. This allowed for much better parallelization.\n",
        "In 2018, BERT utilized the Transformer encoder and pre-training to achieve\n",
        "state-of-the-art results on many understanding tasks.\n",
        "\"\"\"\n",
        "input_text_summary = f\"summarize: {long_text.replace('\\n', ' ')}\"\n",
        "print(f\"\\nInput (Summarization): (See long_text variable)\")\n",
        "input_ids_sum = tokenizer_t5(\n",
        "    input_text_summary, return_tensors=\"pt\", max_length=512, truncation=True\n",
        ").input_ids.to(device)\n",
        "with torch.no_grad():\n",
        "    outputs_sum = model_t5.generate(input_ids_sum, max_length=60, num_beams=4, early_stopping=True)\n",
        "decoded_summary = tokenizer_t5.decode(outputs_sum[0], skip_special_tokens=True)\n",
        "print(\"Output:\")\n",
        "print(decoded_summary)\n"
      ],
      "metadata": {
        "id": "ehZKmqR-nCyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Full end-to-end fine-tuning FFT =====\n",
        "FT_MODEL_NAME = \"distilbert-base-uncased\"\n",
        "print(\"\\n--- Loading and Preparing Data (IMDB Sentiment) ---\")\n",
        "dataset = load_dataset(\"imdb\")\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "small_train_dataset = dataset[\"train\"].select(range(300))\n",
        "small_eval_dataset = dataset[\"test\"].select(range(100))\n",
        "print(f\"Loaded IMDB dataset. Training samples: {len(small_train_dataset)}, Eval samples: {len(small_eval_dataset)}\")\n",
        "\n",
        "print(\"\\n--- Tokenization ---\")\n",
        "ft_tokenizer = AutoTokenizer.from_pretrained(FT_MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return ft_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_train = small_train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval = small_eval_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Ensure Trainer sees 'labels' and tensors\n",
        "if \"label\" in tokenized_train.column_names:\n",
        "    tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
        "if \"label\" in tokenized_eval.column_names:\n",
        "    tokenized_eval = tokenized_eval.rename_column(\"label\", \"labels\")\n",
        "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "tokenized_eval.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "print(\"Tokenization complete.\")\n",
        "\n",
        "print(f\"\\n--- Loading Pre-trained Model ({FT_MODEL_NAME}) ---\")\n",
        "ft_model = AutoModelForSequenceClassification.from_pretrained(FT_MODEL_NAME, num_labels=2).to(device)\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "print(\"\\n--- Defining Metrics ---\")\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "print(\"Metrics defined (Accuracy).\")\n",
        "\n",
        "print(\"\\n--- Configuring and Running Trainer ---\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"distilbert_ft_demo_lecture\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=ft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"Starting Fine-Tuning...\")\n",
        "trainer.train()\n",
        "print(\"\\nFine-tuning complete!\")\n",
        "\n",
        "print(\"\\n--- Evaluation and Inference ---\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Final Evaluation Accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
        "\n",
        "finetuned_classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=ft_model,\n",
        "    tokenizer=ft_tokenizer,\n",
        "    device=pipe_device,\n",
        ")\n",
        "\n",
        "test_sentence = \"This lectutorial was incredibly insightful and well-structured!\"\n",
        "print(f\"\\nInference Test: '{test_sentence}'\")\n",
        "prediction = finetuned_classifier(test_sentence)\n",
        "print(f\"Prediction: {prediction}\")\n",
        "\n",
        "# Clean artifacts\n",
        "shutil.rmtree(\"distilbert_ft_demo_lecture\", ignore_errors=True)\n"
      ],
      "metadata": {
        "id": "IQBgbuR_nFVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzR_cGpDopRN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}