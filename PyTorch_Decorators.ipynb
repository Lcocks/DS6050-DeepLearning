{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lcocks/DS6050-DeepLearning/blob/main/PyTorch_Decorators.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential Decorators for PyTorch Deep Learning\n",
        "\n",
        "In the class we discussed `@dataclass` and how it can simplify our life, here I prepared a guide to some other decorators that make your PyTorch code cleaner, faster, and more maintainable.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Python Built-in Decorators](#1-python-built-in-decorators)\n",
        "2. [PyTorch-Specific Decorators](#2-pytorch-specific-decorators)\n",
        "3. [Performance & Caching Decorators](#3-performance--caching-decorators)\n",
        "4. [Custom Utility Decorators](#4-custom-utility-decorators)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Python Built-in Decorators\n",
        "\n",
        "### `@dataclass` – Data Containers\n",
        "\n",
        "Auto-generates `__init__`, `__repr__`, `__eq__` for classes that hold data (configs, hyperparameters).\n",
        "\n",
        "```python\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    batch_size: int = 32\n",
        "    lr: float = 1e-3\n",
        "    epochs: int = 10\n",
        "    device: str = \"cuda\"\n",
        "\n",
        "config = TrainConfig(lr=3e-4)\n",
        "print(config)  # TrainConfig(batch_size=32, lr=0.0003, epochs=10, device='cuda')\n",
        "```\n",
        "\n",
        "**Use case:** Model configs, training hyperparameters, dataset settings.\n",
        "\n",
        "---\n",
        "\n",
        "### `@property` – Computed Attributes\n",
        "\n",
        "Exposes methods as attributes, useful for lazy computation or derived values.\n",
        "\n",
        "```python\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
        "    \n",
        "    @property\n",
        "    def num_parameters(self):\n",
        "        \"\"\"Compute total parameters on-the-fly.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "model = ConvNet()\n",
        "print(model.num_parameters)  # Access like an attribute, not a method call\n",
        "```\n",
        "\n",
        "**Use case:** Model introspection (param count, layer info), dynamic properties.\n",
        "\n",
        "---\n",
        "\n",
        "### `@staticmethod` & `@classmethod` – Non-Instance Methods\n",
        "\n",
        "**`@staticmethod`**: Function that doesn't need `self` or class state.\n",
        "\n",
        "```python\n",
        "class DataProcessor:\n",
        "    @staticmethod\n",
        "    def normalize(x):\n",
        "        \"\"\"Utility function that doesn't need instance state.\"\"\"\n",
        "        return (x - x.mean()) / x.std()\n",
        "\n",
        "# Call without creating an instance\n",
        "normalized = DataProcessor.normalize(tensor)\n",
        "```\n",
        "\n",
        "**`@classmethod`**: Function that receives the class (`cls`) instead of instance (`self`).\n",
        "\n",
        "```python\n",
        "class ResNet(nn.Module):\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_name):\n",
        "        \"\"\"Factory method to load pretrained models.\"\"\"\n",
        "        model = cls()\n",
        "        state_dict = torch.hub.load_state_dict_from_url(MODEL_URLS[model_name])\n",
        "        model.load_state_dict(state_dict)\n",
        "        return model\n",
        "\n",
        "model = ResNet.from_pretrained(\"resnet50\")  # Alternative constructor\n",
        "```\n",
        "\n",
        "**Use case:** Utility functions, factory methods, alternative constructors.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. PyTorch-Specific Decorators\n",
        "\n",
        "### `@torch.no_grad()` – Disable Gradient Computation\n",
        "\n",
        "Disables autograd for inference/validation, reducing memory and speeding up computation.\n",
        "\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader):\n",
        "    \"\"\"Validation loop without gradient tracking.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        outputs = model(batch)\n",
        "        total_loss += loss_fn(outputs, batch.labels)\n",
        "    return total_loss / len(dataloader)\n",
        "```\n",
        "\n",
        "**Why?** Saves memory (~2x), faster forward pass. **Always use for inference.**\n",
        "\n",
        "**Alternative (PyTorch 1.9+):** `@torch.inference_mode()` – even faster, more restrictive.\n",
        "\n",
        "```python\n",
        "@torch.inference_mode()\n",
        "def predict(model, x):\n",
        "    \"\"\"Inference with maximum optimization.\"\"\"\n",
        "    return model(x).argmax(dim=-1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### `@torch.compile()` – JIT Compilation (PyTorch 2.0+)\n",
        "\n",
        "Compiles your model for faster execution using TorchDynamo.\n",
        "\n",
        "```python\n",
        "model = ResNet50()\n",
        "model = torch.compile(model)  # One-line speedup!\n",
        "\n",
        "# Or as a decorator on custom modules\n",
        "@torch.compile\n",
        "class CustomLayer(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return torch.relu(x @ self.weight)\n",
        "```\n",
        "\n",
        "**Benefits:** 30-200% speedup on many models. **Trade-off:** Longer first run (compilation).\n",
        "\n",
        "---\n",
        "\n",
        "### `@torch.jit.script` – TorchScript for Optimization\n",
        "\n",
        "Converts Python code to optimized intermediate representation for production deployment.\n",
        "\n",
        "```python\n",
        "@torch.jit.script\n",
        "def fused_gelu(x):\n",
        "    \"\"\"Custom activation with TorchScript optimization.\"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(0.7978845608 * (x + 0.044715 * x ** 3)))\n",
        "\n",
        "# Can export to C++, mobile, etc.\n",
        "scripted = torch.jit.script(model)\n",
        "scripted.save(\"model.pt\")\n",
        "```\n",
        "\n",
        "**Use case:** Production deployment, mobile, edge devices, C++ inference.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Performance & Caching Decorators\n",
        "\n",
        "### `@lru_cache` – Memoization\n",
        "\n",
        "Caches function results to avoid redundant computation.\n",
        "\n",
        "```python\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_positional_encoding(seq_len, d_model):\n",
        "    \"\"\"Compute once, reuse for same seq_len.\"\"\"\n",
        "    position = torch.arange(seq_len).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "# First call computes, subsequent calls with same args return cached result\n",
        "pe1 = get_positional_encoding(512, 768)  # Computed\n",
        "pe2 = get_positional_encoding(512, 768)  # Cached (instant)\n",
        "```\n",
        "\n",
        "**Use case:** Expensive computations with repeated inputs (positional encodings, lookup tables).\n",
        "\n",
        "**Note:** Use `@cache` (Python 3.9+) for unlimited cache size.\n",
        "\n",
        "---\n",
        "\n",
        "### `@functools.wraps` – Preserve Function Metadata\n",
        "\n",
        "Essential when writing custom decorators to preserve original function's name/docstring.\n",
        "\n",
        "```python\n",
        "from functools import wraps\n",
        "import time\n",
        "\n",
        "def timer(func):\n",
        "    \"\"\"Decorator to time function execution.\"\"\"\n",
        "    @wraps(func)  # Preserves func.__name__, func.__doc__, etc.\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        print(f\"{func.__name__} took {time.time() - start:.4f}s\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@timer\n",
        "def train_epoch(model, dataloader):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    # ... training code ...\n",
        "    pass\n",
        "\n",
        "train_epoch(model, train_loader)  # Output: \"train_epoch took 45.2341s\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Custom Utility Decorators\n",
        "\n",
        "### Timing & Profiling Decorator\n",
        "\n",
        "```python\n",
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "def timeit(func):\n",
        "    \"\"\"Measure execution time of functions.\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.perf_counter()\n",
        "        result = func(*args, **kwargs)\n",
        "        elapsed = time.perf_counter() - start\n",
        "        print(f\"⏱️  {func.__name__}: {elapsed:.4f}s\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@timeit\n",
        "def forward_pass(model, batch):\n",
        "    return model(batch)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### GPU Memory Tracking Decorator\n",
        "\n",
        "```python\n",
        "def track_gpu_memory(func):\n",
        "    \"\"\"Monitor GPU memory usage before/after function call.\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.reset_peak_memory_stats()\n",
        "            start_mem = torch.cuda.memory_allocated() / 1024**2  # MB\n",
        "        \n",
        "        result = func(*args, **kwargs)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            end_mem = torch.cuda.memory_allocated() / 1024**2\n",
        "            peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
        "            print(f\"📊 {func.__name__}: {start_mem:.1f}MB → {end_mem:.1f}MB (peak: {peak_mem:.1f}MB)\")\n",
        "        \n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@track_gpu_memory\n",
        "def train_batch(model, batch):\n",
        "    loss = model(batch).loss\n",
        "    loss.backward()\n",
        "    return loss\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Automatic Mixed Precision (AMP) Decorator\n",
        "\n",
        "```python\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "def mixed_precision(func):\n",
        "    \"\"\"Enable automatic mixed precision for faster training.\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        with autocast():\n",
        "            return func(*args, **kwargs)\n",
        "    return wrapper\n",
        "\n",
        "@mixed_precision\n",
        "def forward_pass(model, x):\n",
        "    return model(x)  # Automatically uses fp16 where beneficial\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Reproducibility Decorator\n",
        "\n",
        "```python\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"Decorator to ensure reproducible results.\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            torch.manual_seed(seed)\n",
        "            torch.cuda.manual_seed_all(seed)\n",
        "            np.random.seed(seed)\n",
        "            random.seed(seed)\n",
        "            torch.backends.cudnn.deterministic = True\n",
        "            torch.backends.cudnn.benchmark = False\n",
        "            return func(*args, **kwargs)\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "@seed_everything(seed=123)\n",
        "def train_model():\n",
        "    # Training will be reproducible\n",
        "    pass\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Gradient Clipping Decorator\n",
        "\n",
        "```python\n",
        "def clip_gradients(max_norm=1.0):\n",
        "    \"\"\"Automatically clip gradients during backward pass.\"\"\"\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(model, *args, **kwargs):\n",
        "            loss = func(model, *args, **kwargs)\n",
        "            if loss.requires_grad:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
        "            return loss\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "@clip_gradients(max_norm=1.0)\n",
        "def compute_loss(model, batch):\n",
        "    return model(batch).loss\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Exception Handling for Training\n",
        "\n",
        "```python\n",
        "def safe_training(func):\n",
        "    \"\"\"Catch and log exceptions without crashing training.\"\"\"\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        try:\n",
        "            return func(*args, **kwargs)\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(f\"⚠️  OOM Error in {func.__name__}. Try reducing batch size.\")\n",
        "                torch.cuda.empty_cache()\n",
        "            else:\n",
        "                print(f\"❌ Error in {func.__name__}: {e}\")\n",
        "            return None\n",
        "    return wrapper\n",
        "\n",
        "@safe_training\n",
        "def train_step(model, batch):\n",
        "    return model(batch).loss.backward()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Reference Table\n",
        "\n",
        "| Decorator | Primary Use | Key Benefit |\n",
        "|:----------|:------------|:------------|\n",
        "| `@dataclass` | Config classes | Auto-generate boilerplate |\n",
        "| `@property` | Computed attributes | Clean API, lazy computation |\n",
        "| `@staticmethod` | Utility functions | No instance needed |\n",
        "| `@classmethod` | Factory methods | Alternative constructors |\n",
        "| `@torch.no_grad()` | Inference/validation | 2x memory savings |\n",
        "| `@torch.inference_mode()` | Pure inference | Maximum speed |\n",
        "| `@torch.compile()` | Model optimization | 30-200% speedup |\n",
        "| `@torch.jit.script` | Production deployment | C++ export, mobile |\n",
        "| `@lru_cache` | Expensive computations | Avoid recomputation |\n",
        "| `@timeit` | Profiling | Track execution time |\n",
        "| `@track_gpu_memory` | Memory debugging | Find memory leaks |\n",
        "\n",
        "---\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "1. **Always use `@torch.no_grad()` for evaluation** – Default for validation/test loops\n",
        "2. **Cache expensive computations** – Use `@lru_cache` for positional encodings, masks\n",
        "3. **Profile before optimizing** – Use `@timeit` to find bottlenecks\n",
        "4. **Prefer `@torch.compile` over manual optimization** – Easier and often faster (PyTorch 2.0+)\n",
        "5. **Use `@property` for model introspection** – Makes debugging easier\n",
        "6. **Write custom decorators for repetitive patterns** – DRY principle for training loops\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- [Python Decorators Guide](https://realpython.com/primer-on-python-decorators/)\n",
        "- [PyTorch Performance Tuning](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
        "- [TorchScript Documentation](https://pytorch.org/docs/stable/jit.html)\n",
        "- [torch.compile Guide](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "wUMI7CU6JXsd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XoMHB60kJZr0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}