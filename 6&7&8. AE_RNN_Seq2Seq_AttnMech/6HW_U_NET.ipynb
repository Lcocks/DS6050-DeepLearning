{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Lcocks/DS6050-DeepLearning/blob/main/6HW_U_NET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdKjwkWX9f2G"
   },
   "source": [
    "\n",
    "\n",
    "## Module 6 HW (to be submitted with Module 7 HW): From Building Blocks to U-Net - Understanding Architectural Innovation\n",
    "(even if you did the homework with your teammates you should submit individually and include the names of your teammates)\n",
    "\n",
    "### Part 1: Understanding U-Net Architecture (15 points)\n",
    "U-Net was introduced by Ronneberger et al. (2015) for biomedical image segmentation. The architecture won the ISBI cell tracking challenge 2015 by a large margin.\n",
    "\n",
    "![U-Net Architecture](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)\n",
    "*Figure 1: Original U-Net architecture (Ronneberger et al., 2015)*\n",
    "\n",
    "### Starter Code: Basic U-Net Implementation\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # Input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "```\n",
    "\n",
    "#### Questions to Answer:\n",
    "\n",
    "**Q1.1 (5 points):** Identify and explain the three major architectural components/concepts from our previous lectures that U-Net combines. For each component:\n",
    "- Name the concept\n",
    "- Explain how it's implemented in U-Net\n",
    "- Describe what problem it solves\n",
    "\n",
    "**Q1.2 (5 points):** Skip connections in U-Net use **concatenation** rather than **addition** (as in ResNet).\n",
    "- What are the implications of this design choice?\n",
    "- How does this affect the number of parameters?\n",
    "- When might addition be preferred over concatenation?\n",
    "\n",
    "**Q1.3 (5 points):** Critical Analysis:\n",
    "- What are the potential limitations of the original U-Net architecture?\n",
    "- How might U-Net struggle with very high-resolution images?\n",
    "- Propose at least two modifications that could address these limitations.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Architectural Variants (15 points)\n",
    "\n",
    "#### Variant 1: U-Net++ (Nested U-Net)\n",
    "\n",
    "U-Net++ introduces nested skip connections to reduce the semantic gap between encoder and decoder features.\n",
    "\n",
    "```python\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net++: A Nested U-Net Architecture\n",
    "    Zhou et al., 2018\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the nested structure\n",
    "        # X^{0,0} -> X^{1,0} -> X^{2,0} -> X^{3,0} -> X^{4,0}\n",
    "        #    |         |         |         |\n",
    "        # X^{0,1} -> X^{1,1} -> X^{2,1} -> X^{3,1}\n",
    "        #    |         |         |\n",
    "        # X^{0,2} -> X^{1,2} -> X^{2,2}\n",
    "        #    |         |\n",
    "        # X^{0,3} -> X^{1,3}\n",
    "        #    |\n",
    "        # X^{0,4}\n",
    "        \n",
    "        nb_filter = [32, 64, 128, 256, 512]\n",
    "        \n",
    "        # Nested skip connections would be implemented here\n",
    "        # This is a simplified structure for illustration\n",
    "        pass\n",
    "```\n",
    "\n",
    "#### Variant 2: V-Net (Volumetric Convolutional Neural Networks)\n",
    "\n",
    "V-Net extends U-Net to 3D volumetric data and adds residual connections within each stage.\n",
    "\n",
    "```python\n",
    "class VNetBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    V-Net block with residual connection\n",
    "    Milletari et al., 2016\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_convs):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv3d(in_channels if i == 0 else out_channels,\n",
    "                     out_channels, kernel_size=3, padding=1)\n",
    "            for i in range(num_convs)\n",
    "        ])\n",
    "        self.activation = nn.PReLU(out_channels)\n",
    "        \n",
    "        # Residual connection\n",
    "        self.residual = nn.Conv3d(in_channels, out_channels, kernel_size=1) \\\n",
    "                        if in_channels != out_channels else nn.Identity()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        \n",
    "        out = x\n",
    "        for conv in self.convs:\n",
    "            out = self.activation(conv(out))\n",
    "        \n",
    "        return out + residual\n",
    "```\n",
    "\n",
    "#### Questions to Answer:\n",
    "\n",
    "**Q3.1 (5 points):** U-Net++ Analysis:\n",
    "- What problem does the nested skip connection architecture solve?\n",
    "- How does it reduce the \"semantic gap\"?\n",
    "- What is the computational cost compared to standard U-Net?\n",
    "\n",
    "**Q3.2 (5 points):** V-Net Contributions:\n",
    "- Besides 3D extension, what are the key innovations in V-Net?\n",
    "- Why are residual connections particularly important for volumetric data?\n",
    "- How does the Dice loss implementation differ for 3D data?\n",
    "\n",
    "**Q3.3 (5 points):** Comparative Analysis:\n",
    "Create a comparison table with at least 5 criteria comparing:\n",
    "- Standard U-Net\n",
    "- U-Net++\n",
    "- V-Net\n",
    "\n",
    "Consider: parameter count, memory usage, suitable applications, training difficulty, and inference speed.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Papers\n",
    "- [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "- [UNet++: A Nested U-Net Architecture](https://arxiv.org/abs/1807.10165)\n",
    "- [V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation](https://arxiv.org/abs/1606.04797)\n",
    "\n",
    "### Implementations\n",
    "- [PyTorch U-Net](https://github.com/milesial/Pytorch-UNet)\n",
    "- [Segmentation Models PyTorch](https://github.com/qubvel/segmentation_models.pytorch)\n",
    "- [MONAI (Medical Imaging)](https://monai.io/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF-ydUH59g8L"
   },
   "source": [
    "Q1.1 (5 points): Identify and explain the three major architectural components/concepts from our previous lectures that U-Net combines. For each component:\n",
    "\n",
    "Name the concept - \n",
    "\n",
    "    The encoder-decoder architecture. The Convolutional blocks following convolutional, batchnorm, activation (ReLU). The copy and crop (skip connection).\n",
    "\n",
    "Explain how it's implemented in U-Net - \n",
    "\n",
    "    The encoder-decoder architecture is a downscaliung and upscaling which can create a bottleneck to reduce noise and help focus on the important features. This downsampling reduces dimensionality and increases feature channels available while the upscaling does the opposite to bring back spatial resolution.\n",
    "    The convolutional block is implemented with a double convbatchrelu to further reduce overfitting (adding regularization), these are repeadetly 3x3 convolutions with padding=1 so we are not losing spatial dimensions but are learning more features.\n",
    "    The copy and crop is checked at the Up() forward pass where we concatenate features channel-wise. These include unsampled and encoded features.  \n",
    "  \n",
    "Describe what problem it solves\n",
    "\n",
    "    The architecture is extracting features while maintaining spatial understanding, which was previously not done. Where the encoder is giving context to the features and the decoder is giving localiztion via spatial dimensions of the pixels.\n",
    "    The ConvBatNormReLU double block is adding regularization and accelarting convergence in fewer runs. Here our block and doubling further increases the allowance of increased learning rate with efficiency.\n",
    "    The copy and crop allows the model to retain information loss regarding spatial structure. This will help the localization of features. \n",
    "\n",
    "Q1.2 (5 points): Skip connections in U-Net use concatenation rather than addition (as in ResNet).\n",
    "\n",
    "What are the implications of this design choice?\n",
    "\n",
    "    U-net can relate the unsampled features with the encoded features rather than adding them together. This implies an information retention whereas the addition would immediately combine the features and may lose information that could otherwise be gained in further passes.\n",
    "\n",
    "How does this affect the number of parameters?\n",
    "\n",
    "    This effectively doubles the number of parameters since we are doubling the channel dimension.\n",
    "\n",
    "When might addition be preferred over concatenation?\n",
    "\n",
    "    Concatentation can lead to overfitting on small data sources so this can be reduced and going hand in hand if you are computational restricted then being half the number of parameters will allows smoother running for the model (more stable with less data).\n",
    "\n",
    "Q1.3 (5 points): Critical Analysis:\n",
    "\n",
    "What are the potential limitations of the original U-Net architecture?\n",
    "\n",
    "    The original architecture still has a semantic gap in high-level semantic features (what/where objects are) while maintaining spatial precision. The functionality also collapses with higher resolution images (extremely inefficient), in effect also quadrupling the memory usage when you double the resolution. \n",
    "\n",
    "How might U-Net struggle with very high-resolution images?\n",
    "\n",
    "    High resolution images would be extremely inefficient and resource intensive to run the basic U-net on. As mentioned doubling the resolution (say from 512 to 1024) would quadruple the memory usage so a ~2GB usage would turn to ~8GB. Global relationships are also not modeled (only local for the most part), so when you have a very large or detailed image there is a heavy loss of relevant information patterns. \n",
    "\n",
    "Propose at least two modifications that could address these limitations.\n",
    "\n",
    "    1 modification would be implementning a depth-wise separable convolutions alongside the concatenation since that would reduce the number of parameters but in doing so you lose unrealized accuracy. \n",
    "    Another would be using attention-gates like in module 8 where we have more specialized feature extraction. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3.1 (5 points): U-Net++ Analysis:\n",
    "\n",
    "What problem does the nested skip connection architecture solve?\n",
    "\n",
    "    It helps reduce the semantic gap between the encoder and decoder meaning that information learned in the encoder is at first simpler patterns like edges and corners but at the same time the decoder can be recognizing high-level patterns like class specific patterns (what a curve of a number is for example).\n",
    "\n",
    "How does it reduce the \"semantic gap\"?\n",
    "\n",
    "    The defined architecture reduced the semantic gap by providing smaller jumps from each semantic layer as to create a more refined understaing. The nested architecture helps bridge this gap by relating more relavent features together.\n",
    "\n",
    "What is the computational cost compared to standard U-Net?\n",
    "\n",
    "    Compared to U-net U-net++ is more resource intensive but not as much as say adding the features at each skip instead of concatenating them. It addes about 1.4x the number of parameters as U-Net (as seen in the U-Net++ paper). Overall though this is not a large leap.\n",
    "\n",
    "Q3.2 (5 points): V-Net Contributions:\n",
    "\n",
    "Besides 3D extension, what are the key innovations in V-Net?\n",
    "\n",
    "    V-net only uses the first layer for a channels transformation then uses the subsequent layers while mainting the same number of channels. Each stage then has a residual connection that is added back to the output. These can help the gradient flow from collapsing. So the each channel is learning its own activation vector.\n",
    "\n",
    "Why are residual connections particularly important for volumetric data?\n",
    "\n",
    "    Residual connections will help preserve the gradient flow and prevent vanishing/exploding (moreso concerned with vanishing here). With volumetric data we are going deeper rather then wider with more channels, this allows us to handle the heavily increased volume of data given our residuals are capturing the necessary information.\n",
    "\n",
    "How does the Dice loss implementation differ for 3D data?\n",
    "\n",
    "    The dice coefficient for loss is used to better handle rare occurences, which is the case with medical imaging where most of the image information is irrelevant to the wanted predictor. It is setup as a comparison between the ground truth and the predicted segmentation one a scale from 0-1. For 3D data this means the it compares the spatial volume of the predicted portion and the ground truth portion of volume.\n",
    "\n",
    "Q3.3 (5 points): Comparative Analysis: Create a comparison table with at least 5 criteria comparing:\n",
    "\n",
    "Standard U-Net\n",
    "U-Net++\n",
    "V-Net\n",
    "Consider: parameter count, memory usage, suitable applications, training difficulty, and inference speed.\n",
    "\n",
    "| Criterion | Standard U-Net | U-Net++ | V-Net |\n",
    "|-----------|---------------|---------|-------|\n",
    "| **Architecture & Skip Connections** | Direct encoder/decoder concatenation | Nested connections with multiple decoder paths | Direct concatenation & residual connections added at each step |\n",
    "| **Dimensionality & Memory** | 2D (~2.3 GB for 512×512) | 2D (~3.8 GB, 1.4-6× more) | 3D (~4.8 GB for 128³, scales cubically as larger base data) |\n",
    "| **Inference Speed** | Fastest: 23 ms (baseline) | Moderate: 35 ms (1.5× slower) | Slowest: 180 ms (7× slower) |\n",
    "| **Application** | General-purpose 2D segmentation, real-time applications, obvious separation of patterns | Complex 2D boundaries, high-accuracy requirements, ambiguous patterns | 3D medical imaging, volumetric data |\n",
    "| **Training Difficulty/Data Needs** | Easier (~1k 2D images, straightforward) | Medium (~2k+ images, prone to overfitting on small data, needs deep supervision) | Hardest (50-200 3D volumes, memory intensive, much longer training) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dsvenv",
   "language": "python",
   "name": "dsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
