{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Lcocks/DS050-DeepLearning/blob/main/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgojiqdMXZ8H"
   },
   "source": [
    "# Problem 1 (10 pts):\n",
    "\n",
    "From Maximum Likelihood to Cross-Entropy Loss\n",
    "Learning Objectives: Connect probability theory to loss functions, understand why cross-entropy emerges naturally.\n",
    "\n",
    "Part A: Binary Classification Loss Derivation\n",
    "\n",
    "1. Setup: We have $n$ data points $\\{ (x_i, y_i) \\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{ 0, 1 \\}$. Assume your model outputs the probability of class 1 as $p_i = p(y_i = 1 \\mid x_i) = \\sigma(w^T x_i + b)$ where $w \\in \\mathbb{R}^d$, $b \\in \\mathbb{R}$, and $\\sigma(z)$ is the sigmoid function $\\sigma(z) = 1 / (1 + e^{-z})$.\n",
    "2. Derive from MLE:\n",
    "    * Write the likelihood function for the dataset\n",
    "    * Take the log-likelihood\n",
    "    * Show that maximizing log-likelihood = minimizing binary cross-entropy\n",
    "    * Bonus (5 pts): Derive the gradient and show it has the nice form: $\\nabla_w = X^T(p - y)$\n",
    "\n",
    "Part B: Extension to Multi-class\n",
    "\n",
    "1. Softmax derivation: Extend to $K$ classes using softmax function\n",
    "2. Implementation: Code both binary and multi-class cross-entropy from scratch\n",
    "3. Verification: Compare your implementation with sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f758aa74"
   },
   "source": [
    "================================================================================\\\n",
    "Let's derive the binary cross-entropy loss from the principle of Maximum Likelihood Estimation (MLE).\n",
    "\n",
    "Given $n$ data points $\\{ (x_i, y_i) \\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\{ 0, 1 \\}$, and our model outputs the probability of class 1 as $p_i = p(y_i = 1 \\mid x_i) = \\sigma(w^T x_i + b)$.\n",
    "\n",
    "Since $y_i$ is a binary variable, the probability of observing $y_i$ given $x_i$ can be written as:\n",
    "$p(y_i \\mid x_i) = p_i^{y_i} (1 - p_i)^{1 - y_i}$\n",
    "\n",
    "This is because if $y_i = 1$, the probability is $p_i^1 (1 - p_i)^0 = p_i$. If $y_i = 0$, the probability is $p_i^0 (1 - p_i)^1 = 1 - p_i$.\n",
    "\n",
    "Assuming the data points are independent and identically distributed (i.i.d.), the likelihood function for the entire dataset is the product of the individual probabilities:\n",
    "$$ L(w, b) = \\prod_{i=1}^n p(y_i \\mid x_i) = \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1 - y_i} $$\n",
    "\n",
    "To simplify calculations, we typically work with the log-likelihood function, which is the natural logarithm of the likelihood function. Maximizing the log-likelihood is equivalent to maximizing the likelihood because the logarithm is a monotonically increasing function.\n",
    "$$ \\log L(w, b) = \\log \\left( \\prod_{i=1}^n p_i^{y_i} (1 - p_i)^{1 - y_i} \\right) $$\n",
    "Using the property of logarithms ($\\log(ab) = \\log a + \\log b$), we can convert the product into a sum:\n",
    "$$ \\log L(w, b) = \\sum_{i=1}^n \\log \\left( p_i^{y_i} (1 - p_i)^{1 - y_i} \\right) $$\n",
    "Using another property of logarithms ($\\log(a^b) = b \\log a$), we get:\n",
    "$$ \\log L(w, b) = \\sum_{i=1}^n \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right) $$\n",
    "\n",
    "Our goal is to maximize the log-likelihood function with respect to the parameters $w$ and $b$.\n",
    "$$ \\max_{w, b} \\sum_{i=1}^n \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right) $$\n",
    "\n",
    "Minimizing a function is equivalent to maximizing the negative of that function. So, maximizing the log-likelihood is equivalent to minimizing the negative log-likelihood:\n",
    "$$ \\min_{w, b} - \\sum_{i=1}^n \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right) $$\n",
    "This expression is the **binary cross-entropy loss** for the entire dataset. For a single data point, the binary cross-entropy loss is:\n",
    "$$ \\mathcal{L}_{BCE}(y_i, p_i) = - \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right) $$\n",
    "\n",
    "Thus, maximizing the log-likelihood of the data under our probabilistic model is equivalent to minimizing the binary cross-entropy loss between the true labels $y_i$ and the predicted probabilities $p_i$. This shows why binary cross-entropy is a natural choice for the loss function in binary classification problems when using models that output probabilities.\n",
    "\n",
    "**Bonus: Gradient Derivation**\n",
    "\n",
    "Let's derive the gradient of the negative log-likelihood (binary cross-entropy loss) with respect to $w$. The loss for a single data point is $\\mathcal{L}_i = - [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]$. We know that $p_i = \\sigma(z_i)$ where $z_i = w^T x_i + b$.\n",
    "\n",
    "Using the chain rule, $\\frac{\\partial \\mathcal{L}_i}{\\partial w} = \\frac{\\partial \\mathcal{L}_i}{\\partial p_i} \\frac{\\partial p_i}{\\partial z_i} \\frac{\\partial z_i}{\\partial w}$.\n",
    "\n",
    "1. $\\frac{\\partial \\mathcal{L}_i}{\\partial p_i}$:\n",
    "   $$ \\frac{\\partial}{\\partial p_i} [-y_i \\log(p_i) - (1 - y_i) \\log(1 - p_i)] = -y_i \\frac{1}{p_i} - (1 - y_i) \\frac{1}{1 - p_i} (-1) = -\\frac{y_i}{p_i} + \\frac{1 - y_i}{1 - p_i} = \\frac{-y_i(1 - p_i) + p_i(1 - y_i)}{p_i(1 - p_i)} = \\frac{-y_i + y_i p_i + p_i - p_i y_i}{p_i(1 - p_i)} = \\frac{p_i - y_i}{p_i(1 - p_i)} $$\n",
    "\n",
    "2. $\\frac{\\partial p_i}{\\partial z_i}$: The derivative of the sigmoid function is $\\frac{d\\sigma}{dz} = \\sigma(z)(1 - \\sigma(z))$. So, $\\frac{\\partial p_i}{\\partial z_i} = p_i(1 - p_i)$.\n",
    "\n",
    "3. $\\frac{\\partial z_i}{\\partial w}$: $z_i = w^T x_i + b$. The gradient with respect to $w$ is $x_i$. So, $\\frac{\\partial z_i}{\\partial w} = x_i$.\n",
    "\n",
    "Now, combining these parts:\n",
    "$$ \\frac{\\partial \\mathcal{L}_i}{\\partial w} = \\left( \\frac{p_i - y_i}{p_i(1 - p_i)} \\right) \\left( p_i(1 - p_i) \\right) (x_i) = (p_i - y_i) x_i $$\n",
    "\n",
    "This is the gradient for a single data point. For the entire dataset, the total loss is the sum of individual losses $\\mathcal{L} = \\sum_{i=1}^n \\mathcal{L}_i$. The gradient of the total loss is the sum of the individual gradients:\n",
    "$$ \\nabla_w \\mathcal{L} = \\sum_{i=1}^n (p_i - y_i) x_i $$\n",
    "If we represent $X$ as the design matrix where each row is $x_i^T$, $p$ as the vector of predicted probabilities $[p_1, p_2, ..., p_n]^T$, and $y$ as the vector of true labels $[y_1, y_2, ..., y_n]^T$, this sum can be written in matrix form:\n",
    "$$ \\nabla_w \\mathcal{L} = X^T (p - y) $$\n",
    "This elegant form shows that the gradient is proportional to the transpose of the input features multiplied by the difference between the predicted probabilities and the true labels. This form is computationally efficient and intuitive, driving the weights in a direction that reduces the error $(p_i - y_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LFyUNeRGqyYM",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f51de500cd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from typing import Tuple, List, Generator\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1B3eyPbQqxRk"
   },
   "source": [
    "Part B:\n",
    "\n",
    "2. Code both binary and multi-class cross-entropy from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-Cross-Entropy loss: 125.058303\n"
     ]
    }
   ],
   "source": [
    "# Binary Cross-Entropy Loss Function ##\n",
    "\n",
    "#np.random.seed(123)  # For reproducible results during lecture\n",
    "n_points, n_features = 100, 3 \n",
    "X = np.random.randn(n_points, n_features)\n",
    "w = np.random.randn(n_features) # weights\n",
    "b = 0.5\n",
    "y = np.random.randint(0, 2, n_points)  # Binary labels\n",
    "    \n",
    "\n",
    "## noise_level is my b\n",
    "def binary_cross_entropy(w, X, b, y):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy loss function\n",
    "    \"\"\"\n",
    "    L = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        z_i = np.dot(w.T, X[i]) + b #for calculating logit to put in sigmoid, this is for the model p_i = σ(np.dot(w.T, X[i]) + b) \n",
    "        \n",
    "        p_i = 1 / (1 + np.exp(-z_i)) #for the sigmopid function of σ(z) = 1 / (1 + e^-z)\n",
    "                   \n",
    "        L += (y[i] * math.log(p_i) + (1 - y[i]) * math.log(1 - p_i)) #calculating the binary-cross-entropy of each sample and summating across all rows of X.\n",
    "    \n",
    "    L_BCE = -L #returning negative total loss for minimization\n",
    "    return L_BCE\n",
    "\n",
    "## Testing ##\n",
    "BCEloss1 = binary_cross_entropy(w, X, b, y)\n",
    "print(f\"Binary-Cross-Entropy loss: {BCEloss1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Class Cross-Entropy loss: 286.419729\n"
     ]
    }
   ],
   "source": [
    "## Multi-class Cross-Entropy Loss Function ##\n",
    "\n",
    "np.random.seed(123)  # For reproducible results during lecture\n",
    "n_points, n_features, K = 100, 4, 4\n",
    "X = np.random.randn(n_points, n_features) # data point matrix\n",
    "W = np.random.randn(n_features, K) # weights as matrix\n",
    "b = np.random.randn(K) # scalar of noise_levels \n",
    "y = np.random.randint(0, K, n_points) # 1D array of labels with that can be either 0 to K  \n",
    "\n",
    "\n",
    "def cross_entropy(W, X, b, y, K):\n",
    "    \"\"\"\n",
    "    Multi-class Cross-Entropy loss function\n",
    "    \n",
    "    Args:\n",
    "        W: weight matrix (shape: features, num_classes)\n",
    "        K: int number of classes\n",
    "        X: input data (shape: n_points, features)\n",
    "        b: bias vector (shape: num_classes,)\n",
    "        y: true labels - can be:\n",
    "            - One-hot encoded (shape: n_points, num_classes)\n",
    "            - Class indices (shape: n_points,) \n",
    "                \n",
    "    Test 1 ###  Currently testing class indices!!! Failed! cannot use L += y[i, k] * math.log(p_i[k])  ###\n",
    "    Test 2 ###  Testing one-hot encoding.\n",
    "    \n",
    "    Returns:\n",
    "        L: Multi-class cross-entropy loss (scalar)\n",
    "    \"\"\"\n",
    "    L = 0\n",
    "    y_one_hot = np.zeros((n_points, K)) # setting up the framework \n",
    "    y_one_hot[np.arange(n_points), y] = 1 # inputting the 1's for each placement of the index provided given the current value of y.\n",
    "\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "\n",
    "        z_i = np.dot(X[i], W) + b # for calculating logit for every class with shape (K,_)\n",
    "        \n",
    "        exp_z = np.exp(z_i) # start applying softmax function\n",
    "        \n",
    "        p_i = exp_z / np.sum(exp_z)  # shape (K,_) \n",
    "        \n",
    "        for k in range(K):\n",
    "            \n",
    "            L += y_one_hot[i, k] * math.log(p_i[k]) # calculating the cross-entropy of each sample of each class. So grabbed all the classes k for each i and calc'd loss.\n",
    "\n",
    "    L = -L #returning negative total loss for minimization\n",
    "    return L\n",
    "\n",
    "\n",
    "## Testing ##\n",
    "cross_loss1 = cross_entropy(W, X, b, y, K)\n",
    "print(f\"Multi-Class Cross-Entropy loss: {cross_loss1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B:\n",
    "\n",
    "3. Verification: Compare your implementation with sklearn.linear_model.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn weights: [[ 0.21661035  0.0414862   0.01722476 -0.27532131]\n",
      " [-0.20309598 -0.0087771   0.17750731  0.03436578]\n",
      " [ 0.18307943 -0.34736288  0.07182578  0.09245767]\n",
      " [ 0.18000169  0.061904   -0.25958034  0.01767464]]\n",
      "Sklearn bias: [-0.08708353  0.11206366 -0.25938927  0.23440914]\n",
      "\n",
      "Our cross-entropy loss from the linear model: 131.60434898543508\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "\n",
    "logreg_model = linear_model.LogisticRegression(C=1e6, random_state=123) # building the model\n",
    "logreg_model.fit(X, y) # making a model fit\n",
    "\n",
    "\n",
    "# Extract learned weights and bias\n",
    "W_sklearn = logreg_model.coef_.T  # sklearn coef is (n_classes, n_features), we need (n_features, n_classes)\n",
    "b_sklearn = logreg_model.intercept_  # bias terms\n",
    "print(f\"Sklearn weights: {W_sklearn}\")\n",
    "print(f\"Sklearn bias: {b_sklearn}\")\n",
    "\n",
    "scratch_cross_entropy_loss = cross_entropy(W_sklearn, X, b_sklearn, y, K)\n",
    "print(f\"\\nOur cross-entropy loss from the linear model: {scratch_cross_entropy_loss}\")\n",
    "\n",
    "logits = logreg_model.decision_function(X) # getting logits from sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvdT93egXiKT"
   },
   "source": [
    "# Problem 2 (10 pts): Normal Equations vs. Gradient Descent - A Computational Study\n",
    "Learning Objectives: Understand trade-offs between analytical and iterative solutions.\n",
    "\n",
    "Analysis Tasks:\n",
    "\n",
    "1. Complexity Analysis: Plot runtime vs. feature dimension (10 to 1000 features)\n",
    "2. Accuracy Comparison: How close are the solutions? Plot error vs. iterations for GD\n",
    "3. Memory Usage: When does the normal equation become impractical?\n",
    "4. Conditioning: What happens when $X^TX$ is nearly singular? Add ridge regularization.\n",
    "5. Report: When would you choose each method in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods(n_points, n_features, noise_level=0.1): # n_features is d, n_points is num_samples = 1000, noise_level is noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1115],\n",
       "        [ 0.1204]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "0ilWPDnXXjN0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "# Generate synthetic regression data - from video https://www.youtube.com/watch?v=WOFb8EKAy7I&t=1s\n",
    "def generate_synthetic_data(\n",
    "    weights: torch.tensor,\n",
    "    bias: float,\n",
    "    num_samples: int,\n",
    "    noise_std: float = 0.01\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]: #This -> means returns and the type that should be returned.\n",
    "\n",
    "    \"\"\"\n",
    "    Model: y = X @ w + b + ε, with ε ~ N(Θ, noise_std^2)\n",
    "    Args:\n",
    "        weights: (d,_) true weight vector\n",
    "        bias: scaler true intercept\n",
    "        num_samples: number of samples n\n",
    "        noise_std: standard deviation of additive Gaussian noise\n",
    "\n",
    "    Returns:\n",
    "        X: (n, d) feature matrix\n",
    "        y: (n, 1) target vector\n",
    "    \"\"\"\n",
    "\n",
    "    d = weights.shape[0]\n",
    "    X = torch.randn(num_samples, d)\n",
    "    noise = torch.randn(num_samples, 1) * noise_std\n",
    "    y = X @ weights.view(-1, 1) + bias + noise\n",
    "    return X, y\n",
    "    \n",
    "\n",
    "class LinearRegressionScratch:\n",
    "    # Linear regression \"neural network\" with parameters θ = (w, b),\n",
    "    # trained by vanilla gradient descent with manually derived gradients.\n",
    "\n",
    "    # Model:\n",
    "    # y_hat = X @ w + b\n",
    "\n",
    "    # Loss (MSE):\n",
    "    # L = (1/n) * Σ_i (y_hat_i - y_i)^2\n",
    "\n",
    "    # Gradients:\n",
    "    # ∂L/∂w = (2/n) * X^T (y_hat - y)\n",
    "    # ∂L/∂b = (2/n) * Σ_i (y_hat_i - y_i)\n",
    "\n",
    "    def __init__(self, input_size: int, learning_rate: float = 0.01):\n",
    "        \n",
    "        self.w = torch.randn(input_size, 1) * 0.01\n",
    "        self.b = torch.randn(1)\n",
    "        self.lr = learning_rate\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # No autograd - we will compute gradients manually\n",
    "        self.w.requires_grad = False\n",
    "        self.b.requires_grad = False\n",
    "    \n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes predictions: y_hat = X @ w + b\n",
    "        \"\"\"\n",
    "        return X @ self.w + self.b\n",
    "    \n",
    "    def mse_loss(self, y_hat: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the Mean Squared Error (MSE) loss.\n",
    "        \"\"\"\n",
    "        return ((y_hat - y)**2).mean()\n",
    "    \n",
    "    ##  Added this to use X, y and still get mse from global  ## \n",
    "    def compute_mse_loss(self, X, y):\n",
    "        y_pred = self.forward(X)  # Calculate prediction\n",
    "        return torch.mean((y_pred - y)**2)    # Return loss directly\n",
    "\n",
    "    def compute_gradients(self, X: torch.Tensor, y_hat: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch]:\n",
    "        \"\"\"\n",
    "        Compute gradients of loss with respect to parameters w and b\n",
    "        Derive ∂L/∂w and ∂L/∂b by calculus.\n",
    "\n",
    "        With L = (1/n) Σ (y_hat - y)^2, the derivatives are:\n",
    "        ∂L/∂w = (2/n) X^T (y_hat - y)\n",
    "        ∂L/∂b = (2/n) Σ (y_hat - y)\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        error = y_hat - y\n",
    "        dw = (2.0/n) * X.T @ error\n",
    "        db = (2.0/n) * error.sum() # gradient of bias is basically average error, so moving the bias to 0\n",
    "        return dw, db   \n",
    "    \n",
    "    def step(self, X: torch.Tensor, y: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        One gradient descent update:\n",
    "            1) forward\n",
    "            2) loss\n",
    "            3) gradients (manual)\n",
    "            4) parameter update\n",
    "        \"\"\"\n",
    "        # 1 forward\n",
    "        y_hat = self.forward(X)\n",
    "        # 2 loss\n",
    "        loss = self.mse_loss(y_hat, y)\n",
    "        # 3 gradients\n",
    "        dw, db = self.compute_gradients(X, y_hat, y)\n",
    "        # update\n",
    "        self.w -= self.lr * dw\n",
    "        self.b -= self.lr * db\n",
    "        return loss.item()\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "\n",
    "##  Mini-batching & Training Loop  ##\n",
    "def create_mini_batches(\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True\n",
    ") -> Iterator[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    n = X.shape[0]\n",
    "    if shuffle:\n",
    "        idx = torch.randperm(n)\n",
    "    else:\n",
    "        idx = torch.arange(n)\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        b = idx[start:end]\n",
    "        yield X[b], y[b]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from re import X\n",
    "##  Training the model  ##\n",
    "def train_model(\n",
    "    model: LinearRegressionScratch,\n",
    "    X: torch.Tensor,\n",
    "    y: torch.Tensor,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 32,\n",
    "    verbose: bool = True\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Basic mini batch GD training loop.\n",
    "    We report avg loss per epoch.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "            print(\"/n==== Training Model ====\")\n",
    "    total_loss = 0.0\n",
    "    epoch_avgs: List[float] = []\n",
    "    for epoch in range(num_epochs):\n",
    "        num_batches = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in create_mini_batches(X, y, batch_size):\n",
    "            loss = model.step(X_batch, y_batch)\n",
    "            total_loss += loss\n",
    "            num_batches += 1\n",
    "        epoch_loss = total_loss / max(num_batches, 1)\n",
    "        epoch_avgs.append(epoch_loss)\n",
    "        if verbose:\n",
    "            w, b = model.w, model.b\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {epoch_loss:.4f} | w={[f'{wi:.3f}' for wi in w]} | b = {b: .3f}\")\n",
    "    return epoch_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] True w=[[0.29044950008392334], [0.5593220591545105], [0.38406169414520264], [0.2044498324394226], [0.025076627731323242], [0.4780524969100952], [0.1957935094833374], [0.8467287421226501], [0.8912301063537598], [0.41954493522644043]], b=4.200\n",
      "[Model] Learned params: w=[[0.2996874451637268], [0.5645212531089783], [0.3905162513256073], [0.1882598102092743], [0.027697401121258736], [0.467103511095047], [0.20247218012809753], [0.8501148819923401], [0.8945286870002747], [0.4245956838130951]], b=4.187\n"
     ]
    }
   ],
   "source": [
    "##  Testing  ##\n",
    "\n",
    "# 1) Ground truth pars and data\n",
    "TRUE_W = torch.rand(10).reshape(-1, 1) # here we have 2 dimensions\n",
    "TRUE_B = 4.2\n",
    "N = 100\n",
    "\n",
    "X, y = generate_synthetic_data(TRUE_W, TRUE_B, N)\n",
    "print(f\"[Data] True w={TRUE_W.tolist()}, b={TRUE_B:.3f}\")\n",
    "\n",
    "# 2) Initialize the model\n",
    "model = LinearRegressionScratch(input_size=10)\n",
    "\n",
    "# 3) Train\n",
    "losses = train_model(model, X, y, num_epochs=100, verbose=False)\n",
    "w_learned, b_learned = model.w, model.b\n",
    "print(f\"[Model] Learned params: w={w_learned.tolist()}, b={b_learned.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Analytical solution...\n",
      "X shape: torch.Size([100, 10]) -> X_augmented shape: torch.Size([100, 11])\n",
      "Analytical solution: w=[[0.2885613441467285], [0.5581826567649841], [0.38367295265197754], [0.20408132672309875], [0.025549769401550293], [0.4775561988353729], [0.19512273371219635], [0.8473958969116211], [0.8923746347427368], [0.4197760820388794]], b=4.1993\n",
      "Optimal loss: 0.0004\n",
      "\n",
      "3. Gradient descent...\n",
      "\n",
      "Gradient descent result: w=[[0.29929375648498535], [0.5642908811569214], [0.39026957750320435], [0.1890319138765335], [0.02759561501443386], [0.4676114022731781], [0.2021772265434265], [0.8501036763191223], [0.894440770149231], [0.42440250515937805]], b=4.1879\n",
      "Difference of Gradient Descent from Analytical: Δw=tensor([[ 0.0107],\n",
      "        [ 0.0061],\n",
      "        [ 0.0066],\n",
      "        [-0.0150],\n",
      "        [ 0.0020],\n",
      "        [-0.0099],\n",
      "        [ 0.0071],\n",
      "        [ 0.0027],\n",
      "        [ 0.0021],\n",
      "        [ 0.0046]]), Δb=0.011333\n",
      "Analytical(Normal) Time: 0.001353 seconds\n",
      "Gradient Descent(Iterative) Time: 0.000190 seconds\n"
     ]
    }
   ],
   "source": [
    "##  Analytical solution (Normal Equations)  ##\n",
    "print(\"\\n2. Analytical solution...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Add bias column to X (augment the matrix)\n",
    "ones_column = torch.ones(X.shape[0], 1, dtype=X.dtype, device=X.device)\n",
    "X_augmented = torch.cat([X, ones_column], dim=1)\n",
    "print(f\"X shape: {X.shape} -> X_augmented shape: {X_augmented.shape}\")\n",
    "\n",
    "# Normal equations: w = (X^T X)^(-1) X^T y\n",
    "XTX = X_augmented.T @ X_augmented # Using @ for matrix multiplication\n",
    "XTy = X_augmented.T @ y\n",
    "w_analytical = torch.linalg.solve(XTX, XTy)  # More stable than inverse\n",
    "\n",
    "w_opt, b_opt = w_analytical[:-1], w_analytical[-1].item() # must drop last value since that value is b so 10x1 length tensor.\n",
    "analytic_loss_optimal = model.compute_mse_loss(X, y)\n",
    "\n",
    "analytical_time = time.time() - start_time\n",
    "\n",
    "print(f\"Analytical solution: w={w_opt.tolist()}, b={b_opt:.4f}\") # to show tensor make it a list\n",
    "print(f\"Optimal loss: {analytic_loss_optimal:.4f}\")\n",
    "\n",
    "\n",
    "##  Iterative solution (gradient descent)  ##\n",
    "print(\"\\n3. Gradient descent...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "w_gd, b_gd, gd_loss = model.w, model.b, model.step(X, y)\n",
    "\n",
    "\n",
    "gd_time = time.time() - start_time\n",
    "    \n",
    "print(f\"\\nGradient descent result: w={w_gd.tolist()}, b={b_gd.item():.4f}\")\n",
    "\n",
    "# Compare solutions and convergence\n",
    "print(f\"Difference of Gradient Descent from Analytical: Δw={w_gd-w_opt}, Δb={abs(b_gd.item()-b_opt):.6f}\")\n",
    "\n",
    "# Time both approaches\n",
    "    ##  Timing setup in the comparison portion, timing printed here  ##\n",
    "print(f\"Analytical(Normal) Time: {analytical_time:.6f} seconds\")\n",
    "print(f\"Gradient Descent(Iterative) Time: {gd_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] True w=[[0.7441354393959045], [0.054113149642944336], [0.1547689437866211], [0.7791526913642883], [0.38427531719207764], [0.9896624088287354], [0.9186819195747375], [0.29841381311416626], [0.9562726616859436], [0.48421043157577515], [0.5419871807098389], [0.24367845058441162], [0.4409177303314209], [0.7986906170845032], [0.8778460025787354], [0.09786063432693481], [0.8654876351356506], [0.7407096028327942], [0.4664528965950012], [0.5897785425186157], [0.8027569651603699], [0.8979907631874084], [0.48103922605514526], [0.9589821696281433], [0.4925660490989685], [0.39514386653900146], [0.789908766746521], [0.05588775873184204], [0.7109460234642029], [0.036553919315338135], [0.0861133337020874], [0.8983678221702576], [0.08828234672546387], [0.26970958709716797], [0.7772068381309509], [0.24374842643737793], [0.4795992970466614], [0.32398712635040283], [0.9536418318748474], [0.984103798866272], [0.998329758644104], [0.7033678293228149], [0.02704530954360962], [0.38615840673446655], [0.6640356779098511], [0.7808841466903687], [0.29421550035476685], [0.443966805934906], [0.7414703369140625], [0.6585979461669922], [0.4761713147163391], [0.5749318599700928], [0.11298280954360962], [0.3221213221549988], [0.33163201808929443], [0.28417980670928955], [0.8093039393424988], [0.2742997407913208], [0.07734936475753784], [0.37865424156188965], [0.0002631545066833496], [0.1303531527519226], [0.6764214038848877], [0.4288039803504944], [0.5751336216926575], [0.9526054263114929], [0.0030771493911743164], [0.8214331269264221], [0.08852607011795044], [0.14611351490020752], [0.40216171741485596], [0.778359591960907], [0.6659727096557617], [0.9400798678398132], [0.9021090865135193], [0.17710912227630615], [0.9490530490875244], [0.6391969919204712], [0.8345440626144409], [0.9870768189430237], [0.4941544532775879], [0.26383453607559204], [0.018253087997436523], [0.9016939997673035], [0.09024316072463989], [0.7251312136650085], [0.0603979229927063], [0.20115786790847778], [0.2700135111808777], [0.5660396814346313], [0.24178677797317505], [0.8918067216873169], [0.641304075717926], [0.618392825126648], [0.013304412364959717], [0.3261961340904236], [0.7198324203491211], [0.23360979557037354], [0.6994728446006775], [0.37613892555236816]], b=4.200\n",
      "[Model] Learned params: w=[[0.3712535500526428], [0.18477240204811096], [0.3570455312728882], [0.7779456973075867], [0.346937358379364], [0.6348859667778015], [0.6363379955291748], [0.5792990922927856], [0.863119900226593], [0.4151732325553894], [0.31051161885261536], [0.1624058336019516], [0.2165997475385666], [0.7836136221885681], [0.2806555926799774], [-0.13103757798671722], [0.6066216230392456], [0.2942449748516083], [0.5698766708374023], [0.16850504279136658], [0.8174733519554138], [0.866686999797821], [0.4903360903263092], [0.8374513387680054], [0.3245853781700134], [0.38689541816711426], [0.8697713613510132], [0.061026930809020996], [0.6329835057258606], [0.3825989067554474], [-0.10407140105962753], [1.0588828325271606], [0.27526357769966125], [0.3659738302230835], [0.5957071781158447], [0.12030796706676483], [0.5701174736022949], [-0.10163792222738266], [0.8397262096405029], [0.8850103616714478], [0.94452965259552], [0.7817646861076355], [-0.1322430670261383], [0.18159756064414978], [0.1380288451910019], [1.0307468175888062], [0.4300459325313568], [0.5917776823043823], [0.2555590867996216], [0.6463830471038818], [0.5505664348602295], [0.42472177743911743], [0.3351373076438904], [0.2724023759365082], [0.08639562875032425], [0.2023380696773529], [0.4275122582912445], [0.46646764874458313], [-0.1016615554690361], [0.10659652948379517], [-0.048167839646339417], [0.0716904029250145], [0.5964690446853638], [0.40181607007980347], [0.5417823195457458], [0.6917173266410828], [0.260074645280838], [0.7984521985054016], [-0.07312989234924316], [0.02474389411509037], [0.3918970823287964], [0.6614492535591125], [0.640625], [0.7420822978019714], [0.5607829689979553], [0.2501080334186554], [0.7069942355155945], [0.3937321901321411], [0.8056118488311768], [0.6311051249504089], [0.5079157948493958], [0.3529060482978821], [-0.33604076504707336], [0.8752352595329285], [0.35604527592658997], [0.5998798608779907], [0.25825732946395874], [0.1954755187034607], [0.5033972859382629], [0.5156001448631287], [0.46711549162864685], [0.7858266234397888], [0.6438718438148499], [0.49741002917289734], [-0.04928134009242058], [0.12929221987724304], [0.12771616876125336], [0.05105922743678093], [0.9958888292312622], [0.09223959594964981]], b=3.922\n",
      "\n",
      "2. Analytical solution...\n",
      "X shape: torch.Size([100, 100]) -> X_augmented shape: torch.Size([100, 101])\n",
      "Analytical solution: w=[[0.6389780044555664], [0.4459768235683441], [-0.9018895626068115], [0.6963189244270325], [0.6089380979537964], [0.7814652323722839], [0.4419623017311096], [0.323095440864563], [1.3071320056915283], [-0.08724579215049744], [0.8743491172790527], [0.6017180681228638], [0.3601830005645752], [0.6212468147277832], [1.3933510780334473], [0.0956893339753151], [1.6443296670913696], [-0.9632776975631714], [1.0339514017105103], [0.5549828410148621], [0.5600781440734863], [0.9988410472869873], [1.0290086269378662], [0.46583589911460876], [-0.20521575212478638], [-0.817237377166748], [1.5549746751785278], [0.0014236060669645667], [2.3027031421661377], [0.3001135587692261], [0.3273768723011017], [0.9646861553192139], [0.6982850432395935], [-0.203750878572464], [1.2337965965270996], [-0.41977885365486145], [0.9330030679702759], [0.39460617303848267], [-0.0010873079299926758], [1.496954321861267], [1.7884438037872314], [0.23575784265995026], [0.315781831741333], [-0.5481002926826477], [0.16942524909973145], [1.6562011241912842], [0.16151362657546997], [1.2129883766174316], [0.316215842962265], [0.7954675555229187], [0.8102220892906189], [0.6032218933105469], [1.0794111490249634], [-0.8508757948875427], [0.5779798030853271], [1.2645564079284668], [0.057365626096725464], [0.5097203254699707], [0.46520376205444336], [0.20332223176956177], [0.027632951736450195], [-0.3576180338859558], [0.684597909450531], [1.4743279218673706], [1.0279858112335205], [0.11892329901456833], [-0.04088181257247925], [1.407151222229004], [0.7008348703384399], [-0.660737156867981], [1.1337648630142212], [0.2649405300617218], [0.25809574127197266], [0.7568503022193909], [0.9015474319458008], [0.3516616225242615], [0.7480392456054688], [0.6737459897994995], [0.40113747119903564], [1.1172499656677246], [-0.10423922538757324], [-0.12450557947158813], [0.3623352646827698], [0.9939712882041931], [-0.007544994354248047], [1.009426236152649], [-0.5554829239845276], [-0.44582241773605347], [0.04874640703201294], [0.1816815733909607], [0.04001903533935547], [2.040095329284668], [-0.7587166428565979], [1.1031354665756226], [-0.25723862648010254], [0.1026797816157341], [-0.08324257284402847], [0.17618021368980408], [-0.2977721691131592], [-0.04371011257171631]], b=4.2595\n",
      "Optimal loss: 0.1479\n",
      "\n",
      "3. Gradient descent...\n",
      "\n",
      "Gradient descent result: w=[[0.3714756369590759], [0.18471850454807281], [0.35676130652427673], [0.778009295463562], [0.34693631529808044], [0.6351480484008789], [0.6360636353492737], [0.5794438123703003], [0.8634669184684753], [0.4153524339199066], [0.3108319342136383], [0.16213183104991913], [0.21672745048999786], [0.7841200828552246], [0.2813297510147095], [-0.13126540184020996], [0.6071776747703552], [0.2947997748851776], [0.5697963833808899], [0.16931523382663727], [0.8176970481872559], [0.8669207096099854], [0.49025964736938477], [0.8377661108970642], [0.3248438239097595], [0.3863977789878845], [0.8697875142097473], [0.06115920469164848], [0.6336943507194519], [0.3822505474090576], [-0.10408563911914825], [1.0591386556625366], [0.2757432460784912], [0.36671119928359985], [0.5952935218811035], [0.12045959383249283], [0.5698474645614624], [-0.10139619559049606], [0.8392109870910645], [0.8854401707649231], [0.945289671421051], [0.7821180820465088], [-0.1321524977684021], [0.18177446722984314], [0.1382417231798172], [1.0306830406188965], [0.42961981892585754], [0.5914016366004944], [0.2559741735458374], [0.6468557715415955], [0.550865650177002], [0.4251662492752075], [0.33490273356437683], [0.273253470659256], [0.08692529797554016], [0.20255938172340393], [0.4276348948478699], [0.46667346358299255], [-0.10192278027534485], [0.10664334893226624], [-0.04881490767002106], [0.07145076245069504], [0.5972602367401123], [0.40225106477737427], [0.5418481230735779], [0.6922303438186646], [0.2602215111255646], [0.798825204372406], [-0.07325148582458496], [0.024585481733083725], [0.39171403646469116], [0.6616508364677429], [0.6409830451011658], [0.7425821423530579], [0.5605847239494324], [0.2501925826072693], [0.7073339223861694], [0.39444735646247864], [0.8062623143196106], [0.6318179965019226], [0.5078851580619812], [0.35214751958847046], [-0.3360356390476227], [0.8754293918609619], [0.355898916721344], [0.5997841954231262], [0.2589133679866791], [0.19509224593639374], [0.5038041472434998], [0.5157421827316284], [0.46725764870643616], [0.7859038710594177], [0.6442399621009827], [0.4975246489048004], [-0.04905153065919876], [0.12869545817375183], [0.12781865894794464], [0.05103765055537224], [0.9955852627754211], [0.09241249412298203]], b=3.9226\n",
      "Difference of Gradient Descent from Analytical: Δw=tensor([[-0.2675],\n",
      "        [-0.2613],\n",
      "        [ 1.2587],\n",
      "        [ 0.0817],\n",
      "        [-0.2620],\n",
      "        [-0.1463],\n",
      "        [ 0.1941],\n",
      "        [ 0.2563],\n",
      "        [-0.4437],\n",
      "        [ 0.5026],\n",
      "        [-0.5635],\n",
      "        [-0.4396],\n",
      "        [-0.1435],\n",
      "        [ 0.1629],\n",
      "        [-1.1120],\n",
      "        [-0.2270],\n",
      "        [-1.0372],\n",
      "        [ 1.2581],\n",
      "        [-0.4642],\n",
      "        [-0.3857],\n",
      "        [ 0.2576],\n",
      "        [-0.1319],\n",
      "        [-0.5387],\n",
      "        [ 0.3719],\n",
      "        [ 0.5301],\n",
      "        [ 1.2036],\n",
      "        [-0.6852],\n",
      "        [ 0.0597],\n",
      "        [-1.6690],\n",
      "        [ 0.0821],\n",
      "        [-0.4315],\n",
      "        [ 0.0945],\n",
      "        [-0.4225],\n",
      "        [ 0.5705],\n",
      "        [-0.6385],\n",
      "        [ 0.5402],\n",
      "        [-0.3632],\n",
      "        [-0.4960],\n",
      "        [ 0.8403],\n",
      "        [-0.6115],\n",
      "        [-0.8432],\n",
      "        [ 0.5464],\n",
      "        [-0.4479],\n",
      "        [ 0.7299],\n",
      "        [-0.0312],\n",
      "        [-0.6255],\n",
      "        [ 0.2681],\n",
      "        [-0.6216],\n",
      "        [-0.0602],\n",
      "        [-0.1486],\n",
      "        [-0.2594],\n",
      "        [-0.1781],\n",
      "        [-0.7445],\n",
      "        [ 1.1241],\n",
      "        [-0.4911],\n",
      "        [-1.0620],\n",
      "        [ 0.3703],\n",
      "        [-0.0430],\n",
      "        [-0.5671],\n",
      "        [-0.0967],\n",
      "        [-0.0764],\n",
      "        [ 0.4291],\n",
      "        [-0.0873],\n",
      "        [-1.0721],\n",
      "        [-0.4861],\n",
      "        [ 0.5733],\n",
      "        [ 0.3011],\n",
      "        [-0.6083],\n",
      "        [-0.7741],\n",
      "        [ 0.6853],\n",
      "        [-0.7421],\n",
      "        [ 0.3967],\n",
      "        [ 0.3829],\n",
      "        [-0.0143],\n",
      "        [-0.3410],\n",
      "        [-0.1015],\n",
      "        [-0.0407],\n",
      "        [-0.2793],\n",
      "        [ 0.4051],\n",
      "        [-0.4854],\n",
      "        [ 0.6121],\n",
      "        [ 0.4767],\n",
      "        [-0.6984],\n",
      "        [-0.1185],\n",
      "        [ 0.3634],\n",
      "        [-0.4096],\n",
      "        [ 0.8144],\n",
      "        [ 0.6409],\n",
      "        [ 0.4551],\n",
      "        [ 0.3341],\n",
      "        [ 0.4272],\n",
      "        [-1.2542],\n",
      "        [ 1.4030],\n",
      "        [-0.6056],\n",
      "        [ 0.2082],\n",
      "        [ 0.0260],\n",
      "        [ 0.2111],\n",
      "        [-0.1251],\n",
      "        [ 1.2934],\n",
      "        [ 0.1361]]), Δb=0.336917\n",
      "Analytical(Normal) Time: 0.141477 seconds\n",
      "Gradient Descent(Iterative) Time: 0.000216 seconds\n"
     ]
    }
   ],
   "source": [
    "##  Testing 2  ##\n",
    "\n",
    "# 1) Ground truth pars and data\n",
    "TRUE_W = torch.rand(100).reshape(-1, 1) # here we have 2 dimensions\n",
    "TRUE_B = 4.2\n",
    "N = 100\n",
    "\n",
    "X, y = generate_synthetic_data(TRUE_W, TRUE_B, N)\n",
    "print(f\"[Data] True w={TRUE_W.tolist()}, b={TRUE_B:.3f}\")\n",
    "\n",
    "# 2) Initialize the model\n",
    "model = LinearRegressionScratch(input_size=100)\n",
    "\n",
    "# 3) Train\n",
    "losses = train_model(model, X, y, num_epochs=100, verbose=False)\n",
    "w_learned, b_learned = model.w, model.b\n",
    "print(f\"[Model] Learned params: w={w_learned.tolist()}, b={b_learned.item():.3f}\")\n",
    "\n",
    "##  Analytical solution (Normal Equations)  ##\n",
    "print(\"\\n2. Analytical solution...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Add bias column to X (augment the matrix)\n",
    "ones_column = torch.ones(X.shape[0], 1, dtype=X.dtype, device=X.device)\n",
    "X_augmented = torch.cat([X, ones_column], dim=1)\n",
    "print(f\"X shape: {X.shape} -> X_augmented shape: {X_augmented.shape}\")\n",
    "\n",
    "# Normal equations: w = (X^T X)^(-1) X^T y\n",
    "XTX = X_augmented.T @ X_augmented # Using @ for matrix multiplication\n",
    "XTy = X_augmented.T @ y\n",
    "w_analytical = torch.linalg.solve(XTX, XTy)  # More stable than inverse\n",
    "\n",
    "w_opt, b_opt = w_analytical[:-1], w_analytical[-1].item() # must drop last value since that value is b so 10x1 length tensor.\n",
    "analytic_loss_optimal = model.compute_mse_loss(X, y)\n",
    "\n",
    "analytical_time = time.time() - start_time\n",
    "\n",
    "print(f\"Analytical solution: w={w_opt.tolist()}, b={b_opt:.4f}\") # to show tensor make it a list\n",
    "print(f\"Optimal loss: {analytic_loss_optimal:.4f}\")\n",
    "\n",
    "\n",
    "##  Iterative solution (gradient descent)  ##\n",
    "print(\"\\n3. Gradient descent...\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "w_gd, b_gd, gd_loss = model.w, model.b, model.step(X, y)\n",
    "\n",
    "\n",
    "gd_time = time.time() - start_time\n",
    "    \n",
    "print(f\"\\nGradient descent result: w={w_gd.tolist()}, b={b_gd.item():.4f}\")\n",
    "\n",
    "# Compare solutions and convergence\n",
    "print(f\"Difference of Gradient Descent from Analytical: Δw={w_gd-w_opt}, Δb={abs(b_gd.item()-b_opt):.6f}\")\n",
    "\n",
    "# Time both approaches\n",
    "    ##  Timing setup in the comparison portion, timing printed here  ##\n",
    "print(f\"Analytical(Normal) Time: {analytical_time:.6f} seconds\")\n",
    "print(f\"Gradient Descent(Iterative) Time: {gd_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## My iterative solution is finding the GD and my analytical solution is using linear regression.\n",
    "\n",
    "def compare_methods(n_points, n_features, noise_level=0.1):\n",
    "    \n",
    "    \n",
    "    # Generate synthetic regression data\n",
    "        # \"What should our true relationship be?\"\n",
    "    print(\"\\n1. Generating data...\")\n",
    "    np.random.seed(123)\n",
    "    X = np.linspace(0, 10, n_points)\n",
    "\n",
    "        # True relationship: y = 2x + 1 + noise\n",
    "    true_w, true_b = 2.0, 1.0\n",
    "    noise_level = 2.0 \n",
    "    y = true_w * X + true_b + np.random.randn(n_points) * noise_level\n",
    "\n",
    "    print(f\"True parameters: w={true_w}, b={true_b}\")\n",
    "\n",
    "        #Define loss function - here using MSE\n",
    "             # \"Recall, why squared error? What are alternatives?\"\n",
    "    def compute_MSE_loss(w, b, X, y):\n",
    "        \"\"\"\n",
    "        Mean Squared Error loss function\n",
    "        \"\"\"\n",
    "        predictions = w * X + b\n",
    "        errors = y - predictions\n",
    "        mse = np.mean(errors**2)\n",
    "        return mse\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Implement both methods\n",
    "        ##  Analytical solution (Normal Equations)  ##\n",
    "    print(\"\\n2. Analytical solution...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_augmented = np.column_stack([X, np.ones(len(X))]) # Add bias column to X (augment the matrix)\n",
    "    print(f\"X shape: {X.shape} -> X_augmented shape: {X_augmented.shape}\")\n",
    "\n",
    "        # Normal equations: w = (X^T X)^(-1) X^T y\n",
    "    XTX = X_augmented.T @ X_augmented # Using @ for matrix multiplication\n",
    "    XTy = X_augmented.T @ y\n",
    "    w_analytical = np.linalg.solve(XTX, XTy)  # More stable than inverse\n",
    "\n",
    "    w_opt, b_opt = w_analytical[0], w_analytical[1]\n",
    "    analytic_loss_optimal = compute_MSE_loss(w_opt, b_opt, X, y)\n",
    "\n",
    "    print(f\"Analytical solution: w={w_opt:.3f}, b={b_opt:.3f}\")\n",
    "    print(f\"Optimal loss: {analytic_loss_optimal:.3f}\")\n",
    "\n",
    "    analytical_time = time.time() - start_time\n",
    "    \n",
    "        ##  Iterative solution (gradient descent)  ##\n",
    "    print(\"\\n3. Gradient descent...\")\n",
    "    \n",
    "    def gradient_descent(X, y, learning_rate=0.01, max_epochs=1000, verbose=True):\n",
    "        \"\"\"\n",
    "        Gradient descent for linear regression\n",
    "        \"\"\"\n",
    "            # Initialize parameters\n",
    "        w, b = 0.0, 0.0  # Start from zero (ask: \"Why not random?\")\n",
    "\n",
    "            # Storage for plotting convergence\n",
    "        loss_history = []\n",
    "        w_history, b_history = [], []\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "                # Forward pass: compute predictions\n",
    "            predictions = w * X + b\n",
    "\n",
    "                # Compute loss\n",
    "            current_MSE_loss = compute_MSE_loss(w, b, X, y) # Using MSE from above \n",
    "            loss_history.append(current_MSE_loss)\n",
    "            w_history.append(w)\n",
    "            b_history.append(b)\n",
    "\n",
    "                # Backward pass: compute gradients\n",
    "            errors = predictions - y\n",
    "            dw = np.mean(errors * X)  # Derivative w.r.t. w\n",
    "            db = np.mean(errors)      # Derivative w.r.t. b\n",
    "\n",
    "                # Update parameters\n",
    "            w = w - learning_rate * dw\n",
    "            b = b - learning_rate * db\n",
    "\n",
    "                # Print progress\n",
    "            if verbose and epoch % 200 == 0:\n",
    "                print(f\"Epoch {epoch:4d}: Loss = {current_MSE_loss:.4f}, w = {w:.3f}, b = {b:.3f}\")\n",
    "\n",
    "        return w, b, loss_history, w_history, b_history\n",
    "\n",
    "    # Run gradient descent\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    w_gd, b_gd, losses, w_hist, b_hist = gradient_descent(X, y, learning_rate)\n",
    "\n",
    "    gd_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nGradient descent result: w={w_gd:.3f}, b={b_gd:.3f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Time both approaches\n",
    "        ##  Timing setup in the comparison portion, timing printed here  ##\n",
    "    print(f\"Analytical(Normal) Time: {analytical_time:.6f} seconds\")\n",
    "    print(f\"Gradient Descent(Iterative) Time: {gd_time:.6f} seconds\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Compare solutions and convergence\n",
    "    print(f\"Difference of Gradient Descent from Analytical: Δw={abs(w_gd-w_opt):.6f}, Δb={abs(b_gd-b_opt):.6f}\")\n",
    "    \n",
    "    return {\n",
    "    'normal_solution': {\n",
    "        'parameters': (w_opt, b_opt),\n",
    "        'time': analytical_time,\n",
    "        'mse': analytic_loss_optimal\n",
    "        },\n",
    "    'gradient_descent': {\n",
    "        'parameters': (w_gd, b_gd),\n",
    "        'time': gd_time,\n",
    "        'mse': losses[-1],\n",
    "        'loss_history': losses\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Generating data...\n",
      "True parameters: w=2.0, b=1.0\n",
      "\n",
      "2. Analytical solution...\n",
      "X shape: (50,) -> X_augmented shape: (50, 2)\n",
      "Analytical solution: w=2.076, b=0.645\n",
      "Optimal loss: 5.610\n",
      "\n",
      "3. Gradient descent...\n",
      "Epoch    0: Loss = 164.5876, w = 0.731, b = 0.110\n",
      "Epoch  200: Loss = 5.6203, w = 2.106, b = 0.448\n",
      "Epoch  400: Loss = 5.6139, w = 2.094, b = 0.526\n",
      "Epoch  600: Loss = 5.6116, w = 2.087, b = 0.573\n",
      "Epoch  800: Loss = 5.6108, w = 2.083, b = 0.602\n",
      "\n",
      "Gradient descent result: w=2.080, b=0.619\n",
      "Analytical(Normal) Time: 0.000162 seconds\n",
      "Gradient Descent(Iterative) Time: 0.017723 seconds\n",
      "Difference of Gradient Descent from Analytical: Δw=0.003924, Δb=0.026228\n"
     ]
    }
   ],
   "source": [
    "solutions_10feat = compare_methods(n_points = 100, n_features = 10, noise_level = 0.1)\n",
    "#solutions_100feat = compare_methods(n_points = 100, n_features = 100, noise_level = 0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing branch feature commit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6eGQF_5XeNY"
   },
   "source": [
    "# Problem 3 (10 pts): SGD Exploration - Escaping Local Minima (Extended)\n",
    "Learning Objectives: Understand SGD's stochastic nature and hyperparameter effects.\n",
    "\n",
    "Part A: In the Google Colab code for Module 1 ([link](https://colab.research.google.com/drive/1dXLaMnIOQtcBqCZ9LnPiSEpVYmtviRxB?usp=sharing)), we code the Two-Hole Landscape, now extend it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJLAHSyRXc_F"
   },
   "outputs": [],
   "source": [
    "## Part B: Systematic Hyperparameter Study\n",
    "\n",
    "def sgd_parameter_study():\n",
    "    learning_rates = [0.01, 0.05, 0.1, 0.2]\n",
    "    batch_sizes = [1, 4, 16, 64]  # Simulate different batch sizes\n",
    "    noise_scales = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "    # For each combination:\n",
    "    # - Run 20 trials from the same starting point\n",
    "    # - Record: final loss, success rate (reaching global min), convergence time\n",
    "    # - Create heatmaps showing success rate vs. (lr, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_r7_p7bWYBaX"
   },
   "outputs": [],
   "source": [
    "## Part C: Design Your Own Landscape\n",
    "## Create a more complex loss function with 3+ minima:\n",
    "\n",
    "def multi_modal_loss(w):\n",
    "    # Design a function with multiple local minima of different qualities\n",
    "    # Include saddle points for extra challenge\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC10Ee_jX2jl"
   },
   "source": [
    "Part D: Analysis Questions\n",
    "\n",
    "1. What batch size gives the best exploration vs. exploitation trade-off?\n",
    "2. How does the \"escape probability\" change with learning rate?\n",
    "3. Create an \"optimization phase diagram\" showing which parameter combinations succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-9EOXzfSrMv"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9_VboSURl6f"
   },
   "source": [
    "# Problem 4 (10 pts): The Perceptron Problem - Understanding Linear Separability Limitations\n",
    "\n",
    "## What is a Perceptron?\n",
    "\n",
    "Based on our lecture, a **perceptron** is a binary classifier that makes predictions using a linear decision boundary. It consists of:\n",
    "\n",
    "- **Inputs**: A feature vector $x \\in \\mathbb{R}^d$\n",
    "- **Weights**: A weight vector $w \\in \\mathbb{R}^d$\n",
    "- **Bias**: A scalar bias term $b \\in \\mathbb{R}$\n",
    "- **Activation**: A step function (threshold function)\n",
    "\n",
    "The perceptron computes:\n",
    "$$ f(x) = \\text{step} (w^T x + b) $$\n",
    "\n",
    "Where the step function outputs:\n",
    "$$ \\text{step}(w^T x + b) = \\begin{cases}\n",
    "1 & \\text{if} \\quad w^T x + b \\geq 0 \\\\\n",
    "0 & \\text{if} \\quad w^T x + b < 0\n",
    "\\end{cases} $$\n",
    "\n",
    "The decision boundary is the hyperplane defined by $w^T x + b = 0$, which divides the input space into two regions.\n",
    "\n",
    "## The Fundamental Problem\n",
    "\n",
    "The perceptron suffers from a **critical limitation**: it can only solve **linearly separable** problems. This means it can only correctly classify data where the two classes can be perfectly separated by a single straight line (in 2D) or hyperplane (in higher dimensions).\n",
    "\n",
    "### The XOR Problem: A Classic Example\n",
    "\n",
    "The most famous demonstration of this limitation is the **XOR (Exclusive OR) problem**:\n",
    "\n",
    "| x₁ | x₂ | XOR Output |\n",
    "|----|----|------------|\n",
    "| 0  | 0  | 0          |\n",
    "| 0  | 1  | 1          |\n",
    "| 1  | 0  | 1          |\n",
    "| 1  | 1  | 0          |\n",
    "\n",
    "If you plot these four points:\n",
    "- Points (0,1) and (1,0) should be classified as class 1 (XOR = 1)\n",
    "- Points (0,0) and (1,1) should be classified as class 0 (XOR = 0)\n",
    "\n",
    "**No single straight line can separate these classes!** The pattern requires a non-linear decision boundary.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "This limitation reveals why:\n",
    "\n",
    "1. **Single perceptrons are insufficient** for many real-world problems\n",
    "2. **We need non-linearity** in our models (like ReLU activation functions)\n",
    "3. **Multiple layers are essential** to create complex, non-linear decision boundaries\n",
    "4. **The XOR problem motivated** the development of multi-layer neural networks\n",
    "\n",
    "As we learned in our previous lecture, when we combine multiple ReLU neurons and stack them in layers, we can create complex, bent decision boundaries that can solve non-linearly separable problems like XOR.\n",
    "\n",
    "This historical limitation of the perceptron was so significant that it contributed to the \"AI winter\" of the 1970s, until researchers developed multi-layer networks with backpropagation in the 1980s.\n",
    "\n",
    "\n",
    "To Do: Fill in the blanks marked by **TODO**.\n",
    "\n",
    "Learning Objectives\n",
    "\n",
    "1. Implement a perceptron from scratch to understand its mechanics\n",
    "2. Demonstrate why linear models fail on non-linearly separable data\n",
    "3. Visualize decision boundaries and their limitations\n",
    "4. Show how adding non-linear features can solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 998
    },
    "id": "I0R3JuqvSYr0",
    "outputId": "4c7f3656-511d-46cd-b43d-35b86a8f50ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR Dataset - The Classic Non-Linearly Separable Problem:\n",
      "Inputs (X):\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Outputs (y):\n",
      "[0 1 1 0]\n",
      "\n",
      "Notice: Points (0,1) and (1,0) have output 1, while (0,0) and (1,1) have output 0\n",
      "No single straight line can separate these two classes!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Part 1: Creating the XOR Dataset\n",
    "\n",
    "def create_xor_dataset():\n",
    "    \"\"\"\n",
    "    Create the classic XOR dataset that demonstrates linear non-separability.\n",
    "    This is the problem that historically showed perceptron limitations.\n",
    "    \"\"\"\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Create the XOR dataset\n",
    "X_xor, y_xor = create_xor_dataset()\n",
    "print(\"XOR Dataset - The Classic Non-Linearly Separable Problem:\")\n",
    "print(\"Inputs (X):\")\n",
    "print(X_xor)\n",
    "print(\"Outputs (y):\")\n",
    "print(y_xor)\n",
    "print(\"\\nNotice: Points (0,1) and (1,0) have output 1, while (0,0) and (1,1) have output 0\")\n",
    "print(\"No single straight line can separate these two classes!\")\n",
    "\n",
    "\n",
    "## Part 2: Visualizing the Non-Linear Separability Problem\n",
    "\n",
    "def plot_xor_data(X, y):\n",
    "    \"\"\"\n",
    "    Visualize the XOR problem to show why it's non-linearly separable.\n",
    "    This plot clearly shows the geometric impossibility for a perceptron.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    class_0_mask = (y == 0)\n",
    "    class_1_mask = (y == 1)\n",
    "    plt.scatter(X[class_0_mask, 0], X[class_0_mask, 1], c='red', marker='o', s=200, label='XOR = 0', edgecolor='black', linewidth=2)\n",
    "    plt.scatter(X[class_1_mask, 0], X[class_1_mask, 1], c='blue', marker='s', s=200, label='XOR = 1', edgecolor='black', linewidth=2)\n",
    "\n",
    "    plt.xlabel('Input x₁', fontsize=14)\n",
    "    plt.ylabel('Input x₂', fontsize=14)\n",
    "    plt.title('XOR Problem: Why Perceptrons Fail\\nCan you draw a single straight line to separate red and blue points?', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "\n",
    "    # Add text annotations for each point showing the XOR computation\n",
    "    for i in range(len(X)):\n",
    "        plt.annotate(f'({X[i,0]},{X[i,1]}) → {y[i]}',\n",
    "                    (X[i,0], X[i,1]),\n",
    "                    xytext=(10, 10), textcoords='offset points',\n",
    "                    fontsize=11, fontweight='bold',\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_xor_data(X_xor, y_xor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGBklwgPSaGc"
   },
   "outputs": [],
   "source": [
    "## Part 3: Implementing a Perceptron from Scratch\n",
    "\n",
    "class SimplePerceptron:\n",
    "    \"\"\"\n",
    "    A simple perceptron implementation to demonstrate the linear threshold mechanism.\n",
    "    This follows the classic perceptron learning algorithm from our lectures.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=0.1, max_epochs=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_epochs = max_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.training_errors = []\n",
    "\n",
    "    def _activation_function(self, z):\n",
    "        \"\"\"Step function: returns 1 if z >= 0, else 0\"\"\"\n",
    "        return np.where(z >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the perceptron using the classic perceptron learning rule:\n",
    "        w = w + η(target - prediction) * input\n",
    "        \"\"\"\n",
    "        n_points, n_features = X.shape\n",
    "\n",
    "        # TODO: Initialize weights randomly and bias to zero\n",
    "        self.weights = ?\n",
    "        self.bias = ?\n",
    "\n",
    "        # Training loop - implement the perceptron learning algorithm\n",
    "        for epoch in range(self.max_epochs):\n",
    "            # Number of wrong predictions in current epoch\n",
    "            errors = 0\n",
    "\n",
    "            for i in range(n_points):\n",
    "                # TODO: Compute the linear combination (net input)\n",
    "                linear_output = ?\n",
    "\n",
    "                # Apply step function to get prediction\n",
    "                prediction = self._activation_function(linear_output)\n",
    "\n",
    "                # TODO: Calculate the error and update rule\n",
    "                error = ?\n",
    "\n",
    "                # Only update weights if there's an error (classic perceptron rule)\n",
    "                if error != 0:\n",
    "                    errors += 1\n",
    "\n",
    "                    # TODO: Apply perceptron update rule\n",
    "                    self.weights += ?\n",
    "                    self.bias += ?\n",
    "\n",
    "            self.training_errors.append(errors)\n",
    "\n",
    "            # If no errors in this epoch, we've converged (for linearly separable data)\n",
    "            if errors == 0:\n",
    "                print(f\"Converged after {epoch + 1} epochs!\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Did not converge after {self.max_epochs} epochs - likely not linearly separable!\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using the learned decision boundary\"\"\"\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self._activation_function(linear_output)\n",
    "\n",
    "    def get_decision_boundary_params(self):\n",
    "        \"\"\"\n",
    "        Get parameters for plotting the decision boundary line.\n",
    "        Decision boundary: w₁x₁ + w₂x₂ + b = 0\n",
    "        Rearranged as: x₂ = -(w₁x₁ + b) / w₂\n",
    "        \"\"\"\n",
    "        if len(self.weights) == 2 and self.weights[1] != 0:\n",
    "            slope = -self.weights[0] / self.weights[1]\n",
    "            intercept = -self.bias / self.weights[1]\n",
    "            return slope, intercept\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWdyxB9USaJQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "## Part 4: Training the Perceptron on XOR and Observing the Failure\n",
    "\n",
    "print(\"Training a perceptron on the XOR problem...\")\n",
    "print(\"Expected result: The perceptron will fail to learn the correct pattern!\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# TODO: Create and train the perceptron\n",
    "perceptron = ?\n",
    "perceptron.fit(?, ?)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "predictions = ?\n",
    "accuracy = ?\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2f} (Perfect would be 1.00)\")\n",
    "print(f\"Final weights: {perceptron.weights}\")\n",
    "print(f\"Final bias: {perceptron.bias:.3f}\")\n",
    "\n",
    "print(\"\\nPredictions vs True values:\")\n",
    "print(\"Input  | True | Predicted | Correct?\")\n",
    "print(\"-\" * 36)\n",
    "for i in range(len(X_xor)):\n",
    "    correct = \"✓\" if predictions[i] == y_xor[i] else \"✗\"\n",
    "    print(f\"{X_xor[i]}  |  {y_xor[i]}   |     {predictions[i]}     |    {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7g6LIheSaMn"
   },
   "outputs": [],
   "source": [
    "## Part 5: Visualizing the Decision Boundary and the Failure\n",
    "\n",
    "def plot_perceptron_decision_boundary(X, y, perceptron, title=\"Perceptron Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot the data points and the linear decision boundary learned by the perceptron.\n",
    "    This visualization clearly shows why the linear boundary fails on XOR.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Plot the data points\n",
    "    class_0_mask = (y == 0)\n",
    "    class_1_mask = (y == 1)\n",
    "    plt.scatter(X[class_0_mask, 0], X[class_0_mask, 1], c='red', marker='o', s=200,\n",
    "                label='XOR = 0', edgecolor='black', linewidth=2)\n",
    "    plt.scatter(X[class_1_mask, 0], X[class_1_mask, 1], c='blue', marker='s', s=200,\n",
    "                label='XOR = 1', edgecolor='black', linewidth=2)\n",
    "\n",
    "    # TODO: Plot the decision boundary\n",
    "    slope, intercept = ?\n",
    "\n",
    "    if slope is not None:\n",
    "        x_line = np.linspace(-0.5, 1.5, 100)\n",
    "        y_line = slope * x_line + intercept\n",
    "        plt.plot(x_line, y_line, 'k--', linewidth=3, label='Decision Boundary', alpha=0.8)\n",
    "\n",
    "        # Show which side is which\n",
    "        plt.fill_between(x_line, y_line, 2, alpha=0.1, color='blue', label='Predicted: Class 1')\n",
    "        plt.fill_between(x_line, y_line, -1, alpha=0.1, color='red', label='Predicted: Class 0')\n",
    "\n",
    "    # Show predictions as text annotations\n",
    "    predictions = perceptron.predict(X)\n",
    "    for i in range(len(X)):\n",
    "        color = 'green' if predictions[i] == y[i] else 'red'\n",
    "        marker = '✓' if predictions[i] == y[i] else '✗'\n",
    "        plt.annotate(f'Pred: {predictions[i]} {marker}',\n",
    "                    (X[i,0], X[i,1]),\n",
    "                    xytext=(5, -25), textcoords='offset points',\n",
    "                    color=color, fontweight='bold', fontsize=10,\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.xlabel('Input x₁', fontsize=14)\n",
    "    plt.ylabel('Input x₂', fontsize=14)\n",
    "    plt.title(title + '\\nNotice: The straight line cannot separate the classes correctly!', fontsize=16)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(-0.5, 1.5)\n",
    "    plt.ylim(-0.5, 1.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_perceptron_decision_boundary(X_xor, y_xor, perceptron,\n",
    "                                 \"Perceptron Failure on XOR Problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K2x273tyS2w_"
   },
   "outputs": [],
   "source": [
    "## Part 6: The Solution - Non-Linear Feature Engineering\n",
    "\n",
    "def create_nonlinear_features(X):\n",
    "    \"\"\"\n",
    "    Transform the XOR problem into a linearly separable one by adding non-linear features.\n",
    "    This is the key insight: we need to transform the problem space!\n",
    "    \"\"\"\n",
    "    # TODO: Add non-linear features to make XOR linearly separable\n",
    "    # One approach: add x₁ XOR x₂ directly (but that's cheating!)\n",
    "    # Another approach: add the product x₁ * x₂ as a new feature\n",
    "    # Let's add the product feature: X_enhanced = [x₁, x₂, x₁*x₂]\n",
    "\n",
    "    X_enhanced = np.column_stack([X, ?])\n",
    "\n",
    "    return X_enhanced\n",
    "\n",
    "# Create enhanced features\n",
    "X_enhanced = create_nonlinear_features(X_xor)\n",
    "\n",
    "print(\"Original XOR problem:\")\n",
    "print(\"Inputs (x₁, x₂):\")\n",
    "print(X_xor)\n",
    "print(\"\\nEnhanced with non-linear features:\")\n",
    "print(\"Inputs (x₁, x₂, x₁*x₂):\")\n",
    "print(X_enhanced)\n",
    "print(\"\\nOutputs:\", y_xor)\n",
    "\n",
    "print(\"\\nKey insight: In the enhanced space, the problem becomes linearly separable!\")\n",
    "print(\"Notice how the x₁*x₂ feature helps distinguish the classes:\")\n",
    "for i in range(len(X_enhanced)):\n",
    "    print(f\"Point {X_enhanced[i]} → class {y_xor[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVa0X6FrS8jj"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "# Train a perceptron on the enhanced features\n",
    "print(\"\\nTraining perceptron on enhanced features...\")\n",
    "enhanced_perceptron = Perceptron(random_state=42, max_iter=1000)\n",
    "enhanced_perceptron.fit(X_enhanced, y_xor)\n",
    "\n",
    "enhanced_predictions = enhanced_perceptron.predict(X_enhanced)\n",
    "enhanced_accuracy = accuracy_score(y_xor, enhanced_predictions)\n",
    "\n",
    "print(f\"\\nResults with non-linear features:\")\n",
    "print(f\"Accuracy: {enhanced_accuracy:.2f} (Perfect!)\")\n",
    "print(f\"Weights: {enhanced_perceptron.coef_[0]}\")\n",
    "print(f\"Bias: {enhanced_perceptron.intercept_[0]:.3f}\")\n",
    "\n",
    "print(\"\\nPredictions vs True values:\")\n",
    "print(\"Enhanced Input | True | Predicted | Correct?\")\n",
    "print(\"-\" * 44)\n",
    "for i in range(len(X_enhanced)):\n",
    "    correct = \"✓\" if enhanced_predictions[i] == y_xor[i] else \"✗\"\n",
    "    print(f\"{X_enhanced[i]}        |  {y_xor[i]}   |     {enhanced_predictions[i]}     |    {correct}\")\n",
    "\n",
    "print(f\"\\nConclusion: By adding non-linear features, we made XOR linearly separable!\")\n",
    "print(\"This is exactly what hidden layers in neural networks do automatically!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dsvenv",
   "language": "python",
   "name": "dsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
