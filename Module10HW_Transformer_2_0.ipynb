{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lcocks/DS6050-DeepLearning/blob/main/Module10HW_Transformer_2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULVIrHFRMr39"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Modern Transformer Architecture - Homework\n",
        "To be submitted as part II of HW4 (last homework)\n",
        "In this assignment, you'll implement key innovations in modern Transformers:\n",
        "- **RMSNorm**: Simpler and faster normalization\n",
        "- **RoPE**: Rotary positional embeddings for better length extrapolation\n",
        "- **Simplified MHLA**: Efficient attention with reduced KV cache\n",
        "\n",
        "**What you'll learn:**\n",
        "- Why modern LLMs use these techniques\n",
        "- How they improve efficiency and performance\n",
        "- Trade-offs between different approaches\n",
        "\n",
        "**Submission:** Submit this completed notebook with all cells executed.\n",
        "\n",
        "**Grading:**\n",
        "- Part 1 (RMSNorm): 20 points\n",
        "- Part 2 (RoPE): 30 points\n",
        "- Part 3 (MHLA): 35 points\n",
        "- Part 4 (Integration + Questions): 15 points\n",
        "Total: 100 points\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# Setup\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "## Setup and Installation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "\n",
        "# ============================================================================\n",
        "# Part 1: RMSNorm (20 points)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "## Part 1: RMSNorm - Root Mean Square Normalization\n",
        "\n",
        "**Background:** LayerNorm centers (zero mean) and scales (unit variance).\n",
        "RMSNorm only scales, which is simpler and faster while being equally effective in Pre-LN architectures.\n",
        "**Your task:** Complete the RMSNorm implementation below.\n",
        "\"\"\"\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: Feature dimension\n",
        "            eps: Small constant for numerical stability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "        # TODO: Initialize learnable scale parameter (gamma)\n",
        "        # Hint: Use nn.Parameter(torch.ones(dim))\n",
        "        self.gamma = None  # REPLACE THIS LINE\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len, dim)\n",
        "        Returns:\n",
        "            Normalized tensor of same shape\n",
        "        \"\"\"\n",
        "        # TODO: Implement RMSNorm\n",
        "        # Step 1: Compute mean of squares along last dimension\n",
        "        # Hint: mean_sq = (x ** 2).mean(dim=-1, keepdim=True)\n",
        "\n",
        "        # Step 2: Compute RMS (root mean square)\n",
        "        # Hint: rms = torch.sqrt(mean_sq + self.eps)\n",
        "\n",
        "        # Step 3: Normalize by dividing by RMS\n",
        "        # Hint: x_normalized = x / rms\n",
        "\n",
        "        # Step 4: Apply learnable scale\n",
        "        # Hint: return self.gamma * x_normalized\n",
        "\n",
        "        pass  # REPLACE WITH YOUR IMPLEMENTATION\n",
        "\n",
        "# Test your implementation\n",
        "def test_rmsnorm():\n",
        "    \"\"\"Test that RMSNorm produces correct scale\"\"\"\n",
        "    print(\"Testing RMSNorm...\")\n",
        "    norm = RMSNorm(dim=64)\n",
        "    x = torch.randn(2, 10, 64)\n",
        "    out = norm(x)\n",
        "\n",
        "    # Check 1: Output should have RMS â‰ˆ 1\n",
        "    rms = torch.sqrt((out ** 2).mean(dim=-1))\n",
        "    assert torch.allclose(rms, torch.ones_like(rms), atol=1e-5), \"RMS should be ~1\"\n",
        "    print(f\"  âœ“ Output RMS: {rms.mean().item():.6f} (should be ~1.0)\")\n",
        "\n",
        "    # Check 2: Compare with LayerNorm - RMSNorm mean is NOT forced to zero\n",
        "    layer_norm = nn.LayerNorm(64)\n",
        "    out_ln = layer_norm(x)\n",
        "\n",
        "    mean_rms = out.mean(dim=-1).abs().mean()\n",
        "    mean_ln = out_ln.mean(dim=-1).abs().mean()\n",
        "\n",
        "    print(f\"  âœ“ RMSNorm output mean: {mean_rms.item():.4f}\")\n",
        "    print(f\"  âœ“ LayerNorm output mean: {mean_ln.item():.6f} (near zero)\")\n",
        "    print(f\"  âœ“ RMSNorm does NOT center data (unlike LayerNorm)\")\n",
        "\n",
        "    print(\"âœ“ RMSNorm test passed!\")\n",
        "    return out\n",
        "\n",
        "# Uncomment to test after implementing\n",
        "# test_rmsnorm()\n",
        "\n",
        "\"\"\"\n",
        "### Comparison: RMSNorm vs LayerNorm\n",
        "\n",
        "Let's compare the two normalization methods.\n",
        "\"\"\"\n",
        "\n",
        "def compare_normalizations():\n",
        "    \"\"\"Compare RMSNorm and LayerNorm\"\"\"\n",
        "    dim = 512\n",
        "    batch, seq_len = 4, 128\n",
        "\n",
        "    # Create input\n",
        "    x = torch.randn(batch, seq_len, dim).to(device)\n",
        "\n",
        "    # Initialize both norms\n",
        "    rms_norm = RMSNorm(dim).to(device)\n",
        "    layer_norm = nn.LayerNorm(dim).to(device)\n",
        "\n",
        "    # Apply both\n",
        "    out_rms = rms_norm(x)\n",
        "    out_ln = layer_norm(x)\n",
        "\n",
        "    # Compare statistics\n",
        "    print(\"RMSNorm output - Mean: {:.4f}, Std: {:.4f}\".format(\n",
        "        out_rms.mean().item(), out_rms.std().item()))\n",
        "    print(\"LayerNorm output - Mean: {:.4f}, Std: {:.4f}\".format(\n",
        "        out_ln.mean().item(), out_ln.std().item()))\n",
        "\n",
        "    # Speed comparison\n",
        "    n_iters = 1000\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = rms_norm(x)\n",
        "        _ = layer_norm(x)\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # RMSNorm timing\n",
        "    start = time.time()\n",
        "    for _ in range(n_iters):\n",
        "        _ = rms_norm(x)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    rms_time = time.time() - start\n",
        "\n",
        "    # LayerNorm timing\n",
        "    start = time.time()\n",
        "    for _ in range(n_iters):\n",
        "        _ = layer_norm(x)\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    ln_time = time.time() - start\n",
        "\n",
        "    print(f\"\\nSpeed comparison ({n_iters} iterations):\")\n",
        "    print(f\"RMSNorm: {rms_time:.4f}s\")\n",
        "    print(f\"LayerNorm: {ln_time:.4f}s\")\n",
        "    if ln_time > rms_time:\n",
        "        print(f\"Speedup: {ln_time/rms_time:.2f}x\")\n",
        "    else:\n",
        "        print(f\"Note: On CPU, PyTorch's optimized LayerNorm may be faster.\")\n",
        "        print(f\"      RMSNorm shows speedups on GPU or with optimized kernels.\")\n",
        "\n",
        "    # Memory comparison\n",
        "    print(f\"\\nParameter count:\")\n",
        "    print(f\"RMSNorm: {sum(p.numel() for p in rms_norm.parameters())} (only gamma)\")\n",
        "    print(f\"LayerNorm: {sum(p.numel() for p in layer_norm.parameters())} (gamma + beta)\")\n",
        "\n",
        "# Uncomment to run comparison after implementing RMSNorm\n",
        "# compare_normalizations()\n",
        "\n",
        "\"\"\"\n",
        "**Question 1.1 (5 points - Written):** Why doesn't RMSNorm need to compute the mean?\n",
        "In what way is \"controlling scale\" sufficient for gradient stability?\n",
        "\n",
        "**Your answer here:**\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# Part 2: Rotary Positional Embeddings (30 points)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "## Part 2: RoPE - Rotary Positional Embeddings\n",
        "\n",
        "**Background:** Instead of adding position information to embeddings, RoPE rotates\n",
        "the query and key vectors based on their position. This encodes *relative* position\n",
        "in the attention scores.\n",
        "\n",
        "**Key insight:** After rotation, the dot product between $q_m$ and $k_n$ depends\n",
        "only on the distance $(m-n)$, not absolute positions.\n",
        "\n",
        "**Your task:** Implement RoPE following the steps below.\n",
        "\"\"\"\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: Embedding dimension (must be even)\n",
        "            max_seq_len: Maximum sequence length\n",
        "            base: Base for frequency computation\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert dim % 2 == 0, \"Dimension must be even\"\n",
        "\n",
        "        # TODO: Compute frequencies: theta_i = base^(-2i/dim) for i in [0, dim/2)\n",
        "        # Hint: inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        inv_freq = None  # REPLACE THIS LINE\n",
        "\n",
        "        # TODO: Precompute position indices [0, 1, 2, ..., max_seq_len-1]\n",
        "        # Hint: position = torch.arange(max_seq_len).float()\n",
        "        position = None  # REPLACE THIS LINE\n",
        "\n",
        "        # TODO: Compute all rotation angles (outer product)\n",
        "        # Hint: freqs = torch.outer(position, inv_freq)\n",
        "        # Shape should be (max_seq_len, dim/2)\n",
        "        freqs = None  # REPLACE THIS LINE\n",
        "\n",
        "        # Store as buffer (not a parameter, but part of state)\n",
        "        self.register_buffer('freqs', freqs)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply rotary embeddings to input.\n",
        "\n",
        "        Args:\n",
        "            x: Input of shape (batch, seq_len, dim)\n",
        "        Returns:\n",
        "            Rotated input of same shape\n",
        "        \"\"\"\n",
        "        batch, seq_len, dim = x.shape\n",
        "\n",
        "        # Get frequencies for this sequence length\n",
        "        freqs = self.freqs[:seq_len]  # (seq_len, dim/2)\n",
        "\n",
        "        # TODO: Compute cos and sin of frequencies\n",
        "        # Hint: cos_freqs = torch.cos(freqs), sin_freqs = torch.sin(freqs)\n",
        "        cos_freqs = None  # REPLACE THIS LINE\n",
        "        sin_freqs = None  # REPLACE THIS LINE\n",
        "\n",
        "        # Reshape x into pairs: (batch, seq_len, dim/2, 2)\n",
        "        x_reshaped = x.reshape(batch, seq_len, -1, 2)\n",
        "\n",
        "        # Split into even and odd indices\n",
        "        x_even = x_reshaped[..., 0]  # (batch, seq_len, dim/2)\n",
        "        x_odd = x_reshaped[..., 1]   # (batch, seq_len, dim/2)\n",
        "\n",
        "        # TODO: Apply rotation using rotation matrix:\n",
        "        # [cos, -sin]\n",
        "        # [sin,  cos]\n",
        "        # Hint: x_even_rot = x_even * cos_freqs - x_odd * sin_freqs\n",
        "        # Hint: x_odd_rot = x_even * sin_freqs + x_odd * cos_freqs\n",
        "        x_even_rot = None  # REPLACE THIS LINE\n",
        "        x_odd_rot = None   # REPLACE THIS LINE\n",
        "\n",
        "        # Stack back together\n",
        "        x_rotated = torch.stack([x_even_rot, x_odd_rot], dim=-1)\n",
        "\n",
        "        # Reshape back to original shape\n",
        "        return x_rotated.reshape(batch, seq_len, dim)\n",
        "\n",
        "# Test RoPE\n",
        "def test_rope():\n",
        "    \"\"\"Test that RoPE encodes relative positions\"\"\"\n",
        "    print(\"Testing RoPE...\")\n",
        "    rope = RotaryEmbedding(dim=64)\n",
        "\n",
        "    # Create separate Q and K vectors (same content, will be rotated differently)\n",
        "    torch.manual_seed(123)  # For reproducibility\n",
        "    base_vec = torch.randn(1, 1, 64)  # Single base vector\n",
        "\n",
        "    # Create Q and K at different positions by repeating the base vector\n",
        "    q_positions = torch.tensor([0, 10])  # Query at positions 0 and 10\n",
        "    k_positions = torch.tensor([5, 15])  # Key at positions 5 and 15\n",
        "\n",
        "    # Create sequences where we place our base vector at specific positions\n",
        "    seq_len = 20\n",
        "    Q = torch.zeros(1, seq_len, 64)\n",
        "    K = torch.zeros(1, seq_len, 64)\n",
        "\n",
        "    # Place the same base vector at different positions\n",
        "    Q[0, q_positions[0]] = base_vec[0, 0]\n",
        "    Q[0, q_positions[1]] = base_vec[0, 0]\n",
        "    K[0, k_positions[0]] = base_vec[0, 0]\n",
        "    K[0, k_positions[1]] = base_vec[0, 0]\n",
        "\n",
        "    # Apply RoPE\n",
        "    Q_rot = rope(Q)\n",
        "    K_rot = rope(K)\n",
        "\n",
        "    # Test relative position property\n",
        "    # dot(q@pos0, k@pos5) should equal dot(q@pos10, k@pos15) (both have distance 5)\n",
        "    dot1 = (Q_rot[0, q_positions[0]] * K_rot[0, k_positions[0]]).sum()\n",
        "    dot2 = (Q_rot[0, q_positions[1]] * K_rot[0, k_positions[1]]).sum()\n",
        "\n",
        "    print(f\"  Dot product at distance 5 (positions 0â†’5): {dot1:.4f}\")\n",
        "    print(f\"  Dot product at distance 5 (positions 10â†’15): {dot2:.4f}\")\n",
        "    print(f\"  Difference: {(dot1 - dot2).abs():.6f}\")\n",
        "\n",
        "    assert torch.allclose(dot1, dot2, atol=1e-3), \"Should encode relative position!\"\n",
        "    print(\"âœ“ RoPE relative position test passed!\")\n",
        "\n",
        "    # Test length extrapolation\n",
        "    print(\"\\nTesting length extrapolation...\")\n",
        "    x_long = torch.randn(1, 100, 64)\n",
        "    x_long_rotated = rope(x_long)\n",
        "    print(f\"  âœ“ Can process sequences longer than some typical lengths ({x_long.shape[1]} tokens)\")\n",
        "\n",
        "# Uncomment to test after implementing\n",
        "# test_rope()\n",
        "\n",
        "\"\"\"\n",
        "### Visualize RoPE\n",
        "\n",
        "Let's visualize how RoPE encodes positions.\n",
        "\"\"\"\n",
        "\n",
        "def visualize_rope():\n",
        "    \"\"\"Visualize RoPE attention patterns\"\"\"\n",
        "    rope = RotaryEmbedding(dim=64)\n",
        "\n",
        "    # Create queries and keys at different positions\n",
        "    seq_len = 50\n",
        "    x = torch.randn(1, seq_len, 64)\n",
        "    x_rotated = rope(x)\n",
        "\n",
        "    # Compute attention scores (without softmax)\n",
        "    scores = torch.matmul(x_rotated, x_rotated.transpose(-2, -1))\n",
        "    scores = scores[0].detach().numpy()  # (seq_len, seq_len)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Full attention matrix\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(scores, cmap='viridis', aspect='auto')\n",
        "    plt.colorbar(label='Attention Score')\n",
        "    plt.xlabel('Key Position')\n",
        "    plt.ylabel('Query Position')\n",
        "    plt.title('Attention Scores with RoPE')\n",
        "\n",
        "    # Attention as function of relative distance\n",
        "    plt.subplot(1, 3, 2)\n",
        "    distances = []\n",
        "    avg_scores = []\n",
        "    for d in range(25):\n",
        "        # Get all pairs with distance d\n",
        "        mask = torch.zeros(seq_len, seq_len)\n",
        "        for i in range(seq_len - d):\n",
        "            mask[i, i + d] = 1\n",
        "\n",
        "        if mask.sum() > 0:\n",
        "            avg_score = (torch.tensor(scores) * mask).sum() / mask.sum()\n",
        "            distances.append(d)\n",
        "            avg_scores.append(avg_score.item())\n",
        "\n",
        "    plt.plot(distances, avg_scores, marker='o', linewidth=2)\n",
        "    plt.xlabel('Relative Distance')\n",
        "    plt.ylabel('Average Attention Score')\n",
        "    plt.title('Attention vs Relative Position')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Show that position 0 has same pattern as position 20\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(scores[0, :], label='Query at position 0', linewidth=2)\n",
        "    plt.plot(scores[20, :], label='Query at position 20', linewidth=2, linestyle='--')\n",
        "    plt.xlabel('Key Position')\n",
        "    plt.ylabel('Attention Score')\n",
        "    plt.title('RoPE Relative Position Property')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Notice: The patterns are shifted but have the same shape!\")\n",
        "    print(\"This shows that RoPE encodes relative position, not absolute position.\")\n",
        "\n",
        "# Uncomment to visualize after implementing\n",
        "# visualize_rope()\n",
        "\n",
        "\"\"\"\n",
        "**Question 2.1 (5 points - Written):** Why does RoPE generalize better to longer sequences\n",
        "than learned absolute positional embeddings?\n",
        "\n",
        "**Your answer here:**\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# Part 3: Simplified Multi-Head Latent Attention (35 points)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "## Part 3: Simplified MHLA\n",
        "\n",
        "**Background:** Standard attention caches both K and V for all heads, which uses\n",
        "a lot of memory. MHLA compresses K and V into a lower-dimensional \"latent\" space.\n",
        "\n",
        "**Standard attention cache per token:** $2 \\times n_{\\text{heads}} \\times d_{\\text{head}}$\n",
        "\n",
        "**MHLA cache per token:** $d_{\\text{latent}}$ (much smaller!)\n",
        "\n",
        "**Your task:** Implement a simplified single-head version of MHLA.\n",
        "\"\"\"\n",
        "\n",
        "class SimplifiedLatentAttention(nn.Module):\n",
        "    def __init__(self, d_model: int = 256, d_latent: int = 64):\n",
        "        \"\"\"\n",
        "        Simplified Multi-Head Latent Attention (single head version)\n",
        "\n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            d_latent: Latent dimension (compressed, this is what gets cached)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_latent = d_latent\n",
        "        self.scale = math.sqrt(d_latent)\n",
        "\n",
        "        # TODO: Initialize projection matrices (all with bias=False)\n",
        "        # W_DKV: projects input to latent KV space (d_model -> d_latent)\n",
        "        # W_Q: projects input to query space (d_model -> d_model)\n",
        "        # W_UK: projects queries to latent space (d_model -> d_latent)\n",
        "        # W_O: projects latent output back to model space (d_latent -> d_model)\n",
        "        # Hint: Use nn.Linear(in_features, out_features, bias=False)\n",
        "\n",
        "        self.W_DKV = None   # REPLACE THIS LINE\n",
        "        self.W_Q = None     # REPLACE THIS LINE\n",
        "        self.W_UK = None    # REPLACE THIS LINE\n",
        "        self.W_O = None     # REPLACE THIS LINE\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input of shape (batch, seq_len, d_model)\n",
        "            cache: Optional cached L_KV from previous steps\n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            L_KV: (batch, total_seq_len, d_latent) for caching\n",
        "        \"\"\"\n",
        "        batch, seq_len, _ = x.shape\n",
        "\n",
        "        # TODO: Step 1 - Create KV latent (THIS is what gets cached!)\n",
        "        # Hint: L_KV_new = self.W_DKV(x)\n",
        "        # Shape: (batch, seq_len, d_latent)\n",
        "        L_KV_new = None  # REPLACE THIS LINE\n",
        "\n",
        "        # TODO: Step 2 - Create queries and project to latent space\n",
        "        # Hint: Q = self.W_Q(x), then QK_T = self.W_UK(Q)\n",
        "        # QK_T shape: (batch, seq_len, d_latent)\n",
        "        Q = None       # REPLACE THIS LINE\n",
        "        QK_T = None    # REPLACE THIS LINE\n",
        "\n",
        "        # Step 3: Handle cache (for autoregressive generation)\n",
        "        if cache is not None:\n",
        "            # Concatenate with previous L_KV\n",
        "            L_KV = torch.cat([cache, L_KV_new], dim=1)\n",
        "        else:\n",
        "            L_KV = L_KV_new\n",
        "\n",
        "        # TODO: Step 4 - Compute attention scores in latent space\n",
        "        # Hint: scores = (QK_T @ L_KV.transpose(-2, -1)) / self.scale\n",
        "        # Shape: (batch, seq_len, total_seq_len)\n",
        "        scores = None  # REPLACE THIS LINE\n",
        "\n",
        "        # TODO: Step 5 - Apply softmax\n",
        "        # Hint: attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = None  # REPLACE THIS LINE\n",
        "\n",
        "        # TODO: Step 6 - Weighted sum of latent values\n",
        "        # Hint: weighted_latents = attn_weights @ L_KV\n",
        "        # Shape: (batch, seq_len, d_latent)\n",
        "        weighted_latents = None  # REPLACE THIS LINE\n",
        "\n",
        "        # TODO: Step 7 - Project back to model dimension\n",
        "        # Hint: output = self.W_O(weighted_latents)\n",
        "        # Shape: (batch, seq_len, d_model)\n",
        "        output = None  # REPLACE THIS LINE\n",
        "\n",
        "        return output, L_KV\n",
        "\n",
        "# Test MHLA\n",
        "def test_mhla():\n",
        "    \"\"\"Test SimplifiedLatentAttention\"\"\"\n",
        "    print(\"Testing MHLA...\")\n",
        "\n",
        "    d_model, d_latent = 256, 64\n",
        "    mhla = SimplifiedLatentAttention(d_model, d_latent)\n",
        "\n",
        "    # Test forward pass\n",
        "    x = torch.randn(2, 10, d_model)\n",
        "    output, L_KV = mhla(x)\n",
        "\n",
        "    # Check shapes\n",
        "    assert output.shape == x.shape, f\"Output shape mismatch: {output.shape} vs {x.shape}\"\n",
        "    assert L_KV.shape == (2, 10, d_latent), f\"L_KV shape mismatch: {L_KV.shape}\"\n",
        "    print(f\"  âœ“ Forward pass: input {x.shape} -> output {output.shape}\")\n",
        "    print(f\"  âœ“ Cache shape: {L_KV.shape}\")\n",
        "\n",
        "    # Test with cache\n",
        "    x_next = torch.randn(2, 1, d_model)\n",
        "    output_next, L_KV_next = mhla(x_next, cache=L_KV)\n",
        "\n",
        "    assert output_next.shape == (2, 1, d_model), \"Cached output shape wrong\"\n",
        "    assert L_KV_next.shape == (2, 11, d_latent), \"Cached L_KV shape wrong\"\n",
        "    print(f\"  âœ“ With cache: new input {x_next.shape} -> cache {L_KV_next.shape}\")\n",
        "\n",
        "    print(\"âœ“ MHLA test passed!\")\n",
        "\n",
        "# Uncomment to test after implementing\n",
        "# test_mhla()\n",
        "\n",
        "\"\"\"\n",
        "### Standard Attention for Comparison\n",
        "\n",
        "Here's standard attention implemented for comparison (already complete).\n",
        "\"\"\"\n",
        "\n",
        "class StandardAttention(nn.Module):\n",
        "    def __init__(self, d_model: int = 256):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.scale = math.sqrt(d_model)\n",
        "\n",
        "        self.W_Q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_O = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cache: Optional[Tuple[torch.Tensor, torch.Tensor]] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input of shape (batch, seq_len, d_model)\n",
        "            cache: Optional tuple of (cached_K, cached_V)\n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            (K, V): Tuple for caching\n",
        "        \"\"\"\n",
        "        Q = self.W_Q(x)\n",
        "        K_new = self.W_K(x)\n",
        "        V_new = self.W_V(x)\n",
        "\n",
        "        # Handle cache\n",
        "        if cache is not None:\n",
        "            K_cache, V_cache = cache\n",
        "            K = torch.cat([K_cache, K_new], dim=1)\n",
        "            V = torch.cat([V_cache, V_new], dim=1)\n",
        "        else:\n",
        "            K, V = K_new, V_new\n",
        "\n",
        "        # Attention\n",
        "        scores = (Q @ K.transpose(-2, -1)) / self.scale\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        output = attn_weights @ V\n",
        "        output = self.W_O(output)\n",
        "\n",
        "        return output, (K, V)\n",
        "\n",
        "\"\"\"\n",
        "### Compare MHLA vs Standard Attention\n",
        "\"\"\"\n",
        "\n",
        "def compare_attention_mechanisms():\n",
        "    \"\"\"Compare cache sizes and efficiency\"\"\"\n",
        "    d_model = 256\n",
        "    d_latent = 64\n",
        "    seq_lengths = [50, 100, 200, 500, 1000, 2000]\n",
        "\n",
        "    mhla = SimplifiedLatentAttention(d_model, d_latent)\n",
        "    std_attn = StandardAttention(d_model)\n",
        "\n",
        "    mhla_cache_sizes = []\n",
        "    std_cache_sizes = []\n",
        "\n",
        "    for seq_len in seq_lengths:\n",
        "        # MHLA cache: only L_KV\n",
        "        mhla_cache = seq_len * d_latent\n",
        "        mhla_cache_sizes.append(mhla_cache)\n",
        "\n",
        "        # Standard attention cache: both K and V\n",
        "        std_cache = seq_len * d_model * 2\n",
        "        std_cache_sizes.append(std_cache)\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Cache size comparison\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(seq_lengths, np.array(std_cache_sizes)/1000, 'o-', label='Standard Attention', linewidth=2, markersize=8)\n",
        "    plt.plot(seq_lengths, np.array(mhla_cache_sizes)/1000, 's-', label='MHLA', linewidth=2, markersize=8)\n",
        "    plt.xlabel('Sequence Length', fontsize=12)\n",
        "    plt.ylabel('Cache Size (thousands of values)', fontsize=12)\n",
        "    plt.title('KV Cache Size vs Sequence Length', fontsize=14)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Compression ratio\n",
        "    plt.subplot(1, 2, 2)\n",
        "    ratios = [std / mhla for std, mhla in zip(std_cache_sizes, mhla_cache_sizes)]\n",
        "    plt.plot(seq_lengths, ratios, 'o-', linewidth=2, markersize=8, color='green')\n",
        "    plt.axhline(y=ratios[0], color='gray', linestyle='--', alpha=0.5, label=f'{ratios[0]:.1f}x (constant)')\n",
        "    plt.xlabel('Sequence Length', fontsize=12)\n",
        "    plt.ylabel('Compression Ratio', fontsize=12)\n",
        "    plt.title('Cache Compression: Standard / MHLA', fontsize=14)\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nFor d_model={d_model}, d_latent={d_latent}:\")\n",
        "    print(f\"Compression ratio: {ratios[0]:.1f}x\")\n",
        "    print(f\"At seq_len=2000: Standard={std_cache_sizes[-1]:,} vs MHLA={mhla_cache_sizes[-1]:,}\")\n",
        "    print(f\"Memory savings: {(1 - mhla_cache_sizes[-1]/std_cache_sizes[-1])*100:.1f}%\")\n",
        "\n",
        "# Uncomment to run comparison after implementing MHLA\n",
        "# compare_attention_mechanisms()\n",
        "\n",
        "\"\"\"\n",
        "**Question 3.1 (5 points - Written):** The compression ratio is constant regardless of sequence\n",
        "length. Why? What does this tell you about where the memory savings come from?\n",
        "\n",
        "**Your answer here:**\n",
        "\n",
        "\n",
        "**Question 3.2 (5 points - Written):** MHLA compresses K and V significantly. What information\n",
        "might be lost? In what scenarios might this hurt model quality?\n",
        "\n",
        "**Your answer here:**\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# Part 4: Putting It All Together (15 points)\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "## Part 4: Complete Transformer Block\n",
        "\n",
        "Now let's combine everything into a modern Transformer block.\n",
        "\"\"\"\n",
        "\n",
        "class ModernTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int = 256, d_latent: int = 64, mlp_ratio: int = 4):\n",
        "        \"\"\"\n",
        "        Modern Transformer block with:\n",
        "        - Pre-LN architecture\n",
        "        - RMSNorm\n",
        "        - Simplified MHLA\n",
        "        - Standard MLP\n",
        "\n",
        "        Args:\n",
        "            d_model: Model dimension\n",
        "            d_latent: Latent dimension for MHLA\n",
        "            mlp_ratio: MLP expansion ratio\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Normalization (Pre-LN style)\n",
        "        self.norm1 = RMSNorm(d_model)\n",
        "        self.norm2 = RMSNorm(d_model)\n",
        "\n",
        "        # Attention\n",
        "        self.attn = SimplifiedLatentAttention(d_model, d_latent)\n",
        "\n",
        "        # MLP\n",
        "        d_mlp = d_model * mlp_ratio\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, d_mlp),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_mlp, d_model)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, cache: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input of shape (batch, seq_len, d_model)\n",
        "            cache: Optional cached L_KV\n",
        "        Returns:\n",
        "            output: (batch, seq_len, d_model)\n",
        "            L_KV: Updated cache\n",
        "        \"\"\"\n",
        "        # TODO: Implement Pre-LN attention block\n",
        "        # Hint: attn_out, L_KV = self.attn(self.norm1(x), cache)\n",
        "        # Hint: x = x + attn_out\n",
        "\n",
        "        # TODO: Implement Pre-LN MLP block\n",
        "        # Hint: mlp_out = self.mlp(self.norm2(x))\n",
        "        # Hint: x = x + mlp_out\n",
        "\n",
        "        pass  # REPLACE WITH YOUR IMPLEMENTATION\n",
        "\n",
        "# Test the complete block\n",
        "def test_transformer_block():\n",
        "    \"\"\"Test the complete modern transformer block\"\"\"\n",
        "    print(\"Testing Modern Transformer Block...\")\n",
        "\n",
        "    block = ModernTransformerBlock(d_model=256, d_latent=64)\n",
        "\n",
        "    # Forward pass\n",
        "    x = torch.randn(2, 10, 256)\n",
        "    output, cache = block(x)\n",
        "\n",
        "    assert output.shape == x.shape, \"Output shape mismatch\"\n",
        "    assert cache.shape == (2, 10, 64), \"Cache shape mismatch\"\n",
        "    print(f\"  âœ“ Forward pass: {x.shape} -> {output.shape}\")\n",
        "\n",
        "    # Test with cache (autoregressive)\n",
        "    x_next = torch.randn(2, 1, 256)\n",
        "    output_next, cache_next = block(x_next, cache=cache)\n",
        "\n",
        "    assert output_next.shape == (2, 1, 256), \"Cached output shape wrong\"\n",
        "    assert cache_next.shape == (2, 11, 64), \"Updated cache shape wrong\"\n",
        "    print(f\"  âœ“ With cache: {x_next.shape} -> cache {cache_next.shape}\")\n",
        "\n",
        "    print(\"âœ“ Transformer block test passed!\")\n",
        "    print(f\"\\nCache compression: Standard would cache {10 * 256 * 2} values,\")\n",
        "    print(f\"MHLA only caches {10 * 64} values = {(10*256*2)/(10*64):.1f}x smaller!\")\n",
        "\n",
        "# Uncomment to test after implementing all components\n",
        "# test_transformer_block()\n",
        "\n",
        "\"\"\"\n",
        "### Simple Next-Token Prediction Task\n",
        "\n",
        "Let's test our modern transformer on a simple task.\n",
        "\"\"\"\n",
        "\n",
        "def create_toy_dataset(vocab_size: int = 100, seq_len: int = 32, n_samples: int = 1000):\n",
        "    \"\"\"Create a simple synthetic dataset for next-token prediction\"\"\"\n",
        "    # Simple pattern: next token = (current token + 1) mod vocab_size\n",
        "    data = torch.randint(0, vocab_size, (n_samples, seq_len))\n",
        "    targets = (data + 1) % vocab_size\n",
        "    return data, targets\n",
        "\n",
        "def train_model(model, n_epochs: int = 100, vocab_size: int = 100):\n",
        "    \"\"\"Simple training loop\"\"\"\n",
        "    d_model = 256\n",
        "\n",
        "    # Dataset\n",
        "    train_data, train_targets = create_toy_dataset(vocab_size=vocab_size, n_samples=800, seq_len=32)\n",
        "    val_data, val_targets = create_toy_dataset(vocab_size=vocab_size, n_samples=200, seq_len=32)\n",
        "\n",
        "    # Move to device\n",
        "    train_data = train_data.to(device)\n",
        "    train_targets = train_targets.to(device)\n",
        "    val_data = val_data.to(device)\n",
        "    val_targets = val_targets.to(device)\n",
        "\n",
        "    # Embedding and output layers\n",
        "    embedding = nn.Embedding(vocab_size, d_model).to(device)\n",
        "    output_proj = nn.Linear(d_model, vocab_size).to(device)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Combine into simple model\n",
        "    params = list(model.parameters()) + list(embedding.parameters()) + list(output_proj.parameters())\n",
        "    optimizer = torch.optim.Adam(params, lr=1e-3)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    print(\"Training model...\")\n",
        "    for epoch in range(n_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        x_embed = embedding(train_data)  # (batch, seq, d_model)\n",
        "        output, _ = model(x_embed)\n",
        "        logits = output_proj(output)  # (batch, seq, vocab)\n",
        "\n",
        "        # Loss\n",
        "        loss = F.cross_entropy(\n",
        "            logits.reshape(-1, vocab_size),\n",
        "            train_targets.reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Validation\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                x_embed_val = embedding(val_data)\n",
        "                output_val, _ = model(x_embed_val)\n",
        "                logits_val = output_proj(output_val)\n",
        "                val_loss = F.cross_entropy(\n",
        "                    logits_val.reshape(-1, vocab_size),\n",
        "                    val_targets.reshape(-1)\n",
        "                )\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "                print(f\"Epoch {epoch+1}/{n_epochs} - Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Uncomment to train after implementing all components\n",
        "\"\"\"\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training modern transformer on toy task...\")\n",
        "print(\"=\"*60)\n",
        "modern_model = ModernTransformerBlock(d_model=256, d_latent=64)\n",
        "train_losses, val_losses = train_model(modern_model, n_epochs=100)\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Train Loss', alpha=0.7, linewidth=1)\n",
        "plt.plot([i*10-1 for i in range(1, len(val_losses)+1)], val_losses, 'o-', label='Val Loss', markersize=6, linewidth=2)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Training Progress: Next-Token Prediction', fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nFinal training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
        "\"\"\"\n",
        "\n",
        "# ============================================================================\n",
        "# Final Questions\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "## Final Questions\n",
        "\n",
        "**Question 4.1 (3 points - Written):** Imagine you're deploying a chatbot that needs to handle\n",
        "conversations of 50,000 tokens. Would you use standard attention or MHLA?\n",
        "Why? What would be the memory savings?\n",
        "\n",
        "**Your answer here:**\n",
        "\n",
        "\n",
        "**Question 4.2 (3 points - Written):** What's the main trade-off when using a smaller latent dimension\n",
        "$d_{\\text{latent}}$ in MHLA? How would you choose this hyperparameter?\n",
        "\n",
        "**Your answer here:**\n",
        "\n",
        "\n",
        "**Question 4.3 (4 points - Written):** We used Pre-LN (normalize before the block) instead of Post-LN\n",
        "(normalize after the residual). Why is Pre-LN better for deep networks? Hint:\n",
        "think about gradient flow.\n",
        "\n",
        "**Your answer here:**\n",
        "\n",
        "\n",
        "## Submission Checklist\n",
        "\n",
        "Before submitting, make sure you have:\n",
        "- [ ] Implemented RMSNorm correctly (test passes)\n",
        "- [ ] Implemented RoPE correctly (test passes)\n",
        "- [ ] Implemented SimplifiedLatentAttention correctly (test passes)\n",
        "- [ ] Implemented ModernTransformerBlock (test passes)\n",
        "- [ ] Run all comparison visualizations\n",
        "- [ ] Answered all written questions\n",
        "\n",
        "Good luck! ðŸš€\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ljtHbb8POYQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}