{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lcocks/DS6050-DeepLearning/blob/main/7HW_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework: The Dawn of Neural Machine Translation with Seq2Seq\n",
        "\n",
        "## Part 1: Historical Context and Motivation\n",
        "\n",
        "Before the rise of deep learning, machine translation (MT) was dominated by **Statistical Machine Translation (SMT)**. SMT systems were complex engineering feats, relying on statistical models to translate phrases piece-by-piece and then reassembling them using intricate rules.\n",
        "\n",
        "In 2014, a seminal paper changed the landscape: **\"Sequence to Sequence Learning with Neural Networks\"** by Sutskever, Vinyals, and Le. They proposed an elegant, end-to-end neural architecture.\n",
        "\n",
        "### The Core Idea\n",
        "\n",
        "The core idea is remarkably simple:\n",
        "\n",
        "1. **The Encoder**: An RNN reads the input sentence (e.g., English) one word at a time, compressing the entire meaning into a single, fixed-size vector. This is often called the **context vector** or, more poetically, a **\"thought vector.\"**\n",
        "\n",
        "2. **The Decoder**: Another RNN takes this \"thought vector\" as its starting point and generates the output sentence (e.g., French) one word at a time.\n",
        "\n",
        "This architecture marked the beginning of **Neural Machine Translation (NMT)**. In 2016, Google Translate switched from its older SMT system to NMT. The improvement was dramatic.\n",
        "\n",
        "> **\"With this update, Google Translate is improving more in a single leap than we've seen in the last ten years combined.\"** – [Google Blog, 2016 ](https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/)\n",
        "\n",
        "The original 2014 paper used LSTMs. However, we will use **Gated Recurrent Units (GRUs)** for this assignment just to mix it up! GRUs are similar to LSTMs in that they use gates to control information flow, but their architecture is simpler (two gates vs. three, and no separate cell state). They often perform similarly to LSTMs but are slightly faster to train and easier to implement.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 2: Key Concepts\n",
        "\n",
        "### 2.1 Backpropagation Through Time (BPTT)\n",
        "\n",
        "When training RNNs, we must backpropagate gradients through all time steps of the sequence. This is called **Backpropagation Through Time (BPTT)**. The gradients flow backwards through the unrolled RNN, allowing the model to learn long-term dependencies.\n",
        "\n",
        "### 2.2 BPTT and Truncated BPTT (TBPTT)\n",
        "\n",
        "If a sequence is very long (e.g., modeling an entire document), full BPTT consumes excessive memory because we must store the activations for every time step.\n",
        "\n",
        "**Truncated BPTT (TBPTT)** solves this by breaking the sequence into chunks. We process a chunk, backpropagate gradients only within that chunk, and then pass the hidden state forward to the next chunk, stopping the gradient flow at the chunk boundary.\n",
        "\n",
        "In this assignment, our sentences are short, so we will use standard BPTT.\n",
        "\n",
        "### 2.3 The \"Reversal Trick\"\n",
        "\n",
        "The 2014 paper discovered a surprising trick that significantly boosted performance: **Reverse the source sentence.**\n",
        "\n",
        "- **Original**: [I, love, AI] → [J'aime, l'IA]\n",
        "- **Reversed**: [AI, love, I] → [J'aime, l'IA]\n",
        "\n",
        "By doing this, the first words of the output (J'aime) are very close to the corresponding words in the reversed input (I). This creates short-term dependencies, making it much easier for the optimizer to \"establish communication\" between the input and the output early in training.\n",
        "\n",
        "### 2.4 Teacher Forcing\n",
        "\n",
        "When training the decoder, if we use the model's prediction as the input for the subsequent step, an early mistake can cascade, making training unstable.\n",
        "\n",
        "**Teacher Forcing** is a strategy where we sometimes use the actual ground truth token from the training data as the input for the next step, rather than the model's own prediction.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 3: Setup and Data Preprocessing\n",
        "\n",
        "We will use a dataset of English-French sentence pairs.\n",
        "\n",
        "### 3.0 Download the Data\n",
        "\n",
        "Run this cell in Colab to download and unzip the data:\n",
        "\n",
        "```bash\n",
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "!unzip -o data.zip\n",
        "```\n",
        "\n",
        "### 3.1 Imports and Utilities (Provided)\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "# Utilities for handling variable length sequences\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Define special tokens\n",
        "PAD_IDX = 0\n",
        "SOS_IDX = 1\n",
        "EOS_IDX = 2\n",
        "UNK_IDX = 3\n",
        "```\n",
        "\n",
        "### 3.2 Vocabulary and Data Loading (Provided)\n",
        "\n",
        "We provide the utilities to load, normalize, and filter the data. We limit the dataset size and sentence length for faster training.\n",
        "\n",
        "```python\n",
        "class Lang:\n",
        "    \"\"\"A class to hold the vocabulary of a language.\"\"\"\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"<PAD>\": PAD_IDX, \"<SOS>\": SOS_IDX, \"<EOS>\": EOS_IDX, \"<UNK>\": UNK_IDX}\n",
        "        self.index2word = {PAD_IDX: \"<PAD>\", SOS_IDX: \"<SOS>\", EOS_IDX: \"<EOS>\", UNK_IDX: \"<UNK>\"}\n",
        "        self.n_words = 4\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    # Normalize Unicode characters (e.g., remove accents)\n",
        "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "# We filter for relatively short sentences\n",
        "MAX_LENGTH = 15\n",
        "NUM_EXAMPLES = 15000\n",
        "\n",
        "def prepareData(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(f'data/{lang1}-{lang2}.txt', encoding='utf-8').read().strip().split('\\n')\n",
        "    \n",
        "    # Limit the number of examples and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines[:NUM_EXAMPLES]]\n",
        "\n",
        "    # Filter pairs by length\n",
        "    pairs = [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]\n",
        "    \n",
        "    input_lang = Lang(lang1)\n",
        "    output_lang = Lang(lang2)\n",
        "\n",
        "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(f\"Vocabularies: {input_lang.name} ({input_lang.n_words}), {output_lang.name} ({output_lang.n_words})\")\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra')\n",
        "```\n",
        "\n",
        "### 3.3 Dataset and DataLoader (Provided)\n",
        "\n",
        "We implement the PyTorch Dataset. This is where we apply the **Input Reversal Trick**.\n",
        "\n",
        "We also implement a `collate_fn`. This function handles padding sequences in a batch to the same length. Crucially, it also returns the original lengths of the sequences, which we need for **Packing**.\n",
        "\n",
        "```python\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, input_lang, output_lang, reverse_source=True):\n",
        "        self.pairs = pairs\n",
        "        self.input_lang = input_lang\n",
        "        self.output_lang = output_lang\n",
        "        self.reverse_source = reverse_source\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def indexesFromSentence(self, lang, sentence):\n",
        "        return [lang.word2index.get(word, UNK_IDX) for word in sentence.split(' ')]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "        src_text = pair[0]\n",
        "        tgt_text = pair[1]\n",
        "\n",
        "        src_indices = self.indexesFromSentence(self.input_lang, src_text)\n",
        "        tgt_indices = self.indexesFromSentence(self.output_lang, tgt_text)\n",
        "\n",
        "        # Apply the Reversal Trick to the source sentence\n",
        "        if self.reverse_source:\n",
        "            src_indices.reverse()\n",
        "\n",
        "        # Add EOS token to both\n",
        "        src_indices.append(EOS_IDX)\n",
        "        tgt_indices.append(EOS_IDX)\n",
        "\n",
        "        return torch.tensor(src_indices, dtype=torch.long), \\\n",
        "               torch.tensor(tgt_indices, dtype=torch.long)\n",
        "\n",
        "# Collate function to handle padding and return lengths\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_item, tgt_item in batch:\n",
        "        src_batch.append(src_item)\n",
        "        tgt_batch.append(tgt_item)\n",
        "    \n",
        "    # Get the lengths of the source sequences BEFORE padding\n",
        "    src_lengths = torch.tensor([len(s) for s in src_batch])\n",
        "    \n",
        "    # Pad the sequences\n",
        "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
        "\n",
        "    # We return the lengths as well for packing later\n",
        "    return src_batch.to(device), src_lengths, tgt_batch.to(device)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "BATCH_SIZE = 64\n",
        "dataset = TranslationDataset(pairs, input_lang, output_lang, reverse_source=True)\n",
        "\n",
        "# Split into train and validation (90/10)\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Part 4: The Seq2Seq Architecture (Implementation Tasks)\n",
        "\n",
        "Now we implement the core components.\n",
        "\n",
        "### Task 1: The Encoder (20 Points)\n",
        "\n",
        "The Encoder processes the input sequence and compresses it into the context vector.\n",
        "\n",
        "**Important: Packing Padded Sequences.** When training RNNs on batches, we must use `pack_padded_sequence`. This tells the GRU/LSTM to ignore PAD tokens. If we don't pack, the RNN processes the padding, which wastes computation and can negatively affect the final hidden state (the context vector).\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Initialize the `nn.Embedding` and `nn.GRU` layers. Use `batch_first=True`.\n",
        "2. In the forward pass, embed the input.\n",
        "3. Pack the embedded sequence using `pack_padded_sequence`.\n",
        "4. Pass the packed sequence through the GRU.\n",
        "5. Return the final hidden state.\n",
        "\n",
        "```python\n",
        "class EncoderGRU(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # TODO: 1. Initialize the Embedding layer (input_dim -> emb_dim)\n",
        "        self.embedding = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 2. Initialize the GRU layer (emb_dim -> hid_dim)\n",
        "        # Set batch_first=True. Set dropout only if n_layers > 1.\n",
        "        self.rnn = None # <<< YOUR CODE HERE\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src shape: (batch_size, src_len)\n",
        "        # src_lengths shape: (batch_size)\n",
        "\n",
        "        # TODO: 3. Pass the source through the embedding layer and apply dropout\n",
        "        # embedded shape: (batch_size, src_len, emb_dim)\n",
        "        embedded = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 4. Pack the embedded sequences.\n",
        "        # This ensures the RNN ignores the padding.\n",
        "        # Remember to move src_lengths to CPU and set enforce_sorted=False.\n",
        "        packed_embedded = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 5. Pass the packed sequence through the RNN\n",
        "        # hidden shape: (n_layers, batch_size, hid_dim)\n",
        "        packed_outputs, hidden = None, None # <<< YOUR CODE HERE\n",
        "\n",
        "        # In vanilla Seq2Seq, we only need the final hidden state (the context vector).\n",
        "        return hidden\n",
        "```\n",
        "\n",
        "### Task 2: The Decoder (20 Points)\n",
        "\n",
        "The Decoder takes the context vector as its initial hidden state and generates the output sequence one token at a time.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Initialize the Embedding, GRU, and output Linear (`fc_out`) layers.\n",
        "2. The forward pass accepts one token (`input`) and the previous hidden state.\n",
        "3. Embed the input token (remembering to add a sequence dimension).\n",
        "4. Pass the embedding and hidden state to the GRU.\n",
        "5. Pass the GRU output through the linear layer to get the prediction logits.\n",
        "6. Return the prediction and the new hidden state.\n",
        "\n",
        "```python\n",
        "class DecoderGRU(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        # TODO: 1. Initialize the Embedding layer (output_dim -> emb_dim)\n",
        "        self.embedding = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 2. Initialize the GRU layer (emb_dim -> hid_dim). Must match encoder's hid_dim.\n",
        "        # Set batch_first=True.\n",
        "        self.rnn = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 3. Initialize the output linear layer (hid_dim -> output_dim)\n",
        "        self.fc_out = None # <<< YOUR CODE HERE\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        # input shape: (batch_size) -> We are decoding one token at a time!\n",
        "        # hidden shape: (n_layers, batch_size, hid_dim)\n",
        "\n",
        "        # We need to add a sequence dimension: (batch_size) -> (batch_size, 1)\n",
        "        input = input.unsqueeze(1)\n",
        "\n",
        "        # TODO: 4. Pass the input token through the embedding layer and apply dropout\n",
        "        # embedded shape: (batch_size, 1, emb_dim)\n",
        "        embedded = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 5. Pass the embedded input and the hidden state to the RNN\n",
        "        # output shape: (batch_size, 1, hid_dim)\n",
        "        # hidden shape: (n_layers, batch_size, hid_dim)\n",
        "        output, hidden = None, None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 6. Generate the prediction logits.\n",
        "        # Remove the sequence dimension (squeeze) before passing to the linear layer\n",
        "        # (batch_size, 1, hid_dim) -> (batch_size, hid_dim) -> (batch_size, output_dim)\n",
        "        prediction = None # <<< YOUR CODE HERE\n",
        "\n",
        "        return prediction, hidden\n",
        "```\n",
        "\n",
        "### Task 3: The Seq2Seq Wrapper (30 Points)\n",
        "\n",
        "This class combines the Encoder and Decoder and manages the overall process, including the decoding loop and Teacher Forcing.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. Run the encoder on the source sequence and lengths to get the context vector (hidden).\n",
        "2. Initialize the decoder input with the `<SOS>` token.\n",
        "3. Iterate over the length of the target sequence:\n",
        "    - Run the decoder one step.\n",
        "    - Store the output.\n",
        "    - Decide whether to use teacher forcing or the model's own prediction as the next input.\n",
        "\n",
        "```python\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
        "        # src shape: (batch_size, src_len)\n",
        "        # tgt shape: (batch_size, tgt_len)\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        tgt_len = tgt.shape[1]\n",
        "        tgt_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        # Tensor to store decoder outputs\n",
        "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
        "\n",
        "        # TODO: 1. Encode the source sentence (passing src and src_lengths).\n",
        "        # The final hidden state of the encoder is the initial hidden state of the decoder.\n",
        "        hidden = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 2. Initialize the first input to the decoder with the <SOS> token.\n",
        "        # input shape: (batch_size)\n",
        "        input = torch.full((batch_size,), SOS_IDX, dtype=torch.long, device=self.device)\n",
        "\n",
        "        # Iterate over the target sequence length\n",
        "        for t in range(0, tgt_len):\n",
        "            # TODO: 3. Decode one step (pass input and hidden state to decoder)\n",
        "            output, hidden = None, None # <<< YOUR CODE HERE\n",
        "\n",
        "            # 4. Store the output\n",
        "            outputs[:, t, :] = output\n",
        "\n",
        "            # 5. Decide whether to use teacher forcing\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # Get the highest predicted token\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            # TODO: 6. Prepare the next input.\n",
        "            # If teacher forcing, use the actual next token from the target sequence (tgt[:, t]).\n",
        "            # Otherwise, use the predicted token (top1).\n",
        "            if teacher_force:\n",
        "                 input = tgt[:, t]\n",
        "            else:\n",
        "                 input = top1\n",
        "\n",
        "        return outputs\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Part 5: Training the Model\n",
        "\n",
        "### 5.1 Initialization (Provided)\n",
        "\n",
        "We initialize the model with sensible hyperparameters. We use a relatively small model (2 layers, 512 hidden units) which provides a good balance of capacity and training speed for this dataset.\n",
        "\n",
        "```python\n",
        "# Hyperparameters\n",
        "INPUT_DIM = input_lang.n_words\n",
        "OUTPUT_DIM = output_lang.n_words\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2 # Using 2 layers\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "# Initialize models (Ensure Tasks 1-3 are completed first!)\n",
        "enc = EncoderGRU(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n",
        "dec = DecoderGRU(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "# Initialize weights (common practice for RNNs)\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "# Loss function: CrossEntropyLoss, ignoring the padding index\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "```\n",
        "\n",
        "### Task 4: The Training and Evaluation Loops (20 Points)\n",
        "\n",
        "Implement the training and evaluation functions.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. In `train`, implement the forward pass, loss calculation, backpropagation (BPTT), gradient clipping, and optimizer step.\n",
        "2. In `evaluate`, implement the forward pass (with `teacher_forcing_ratio=0`).\n",
        "3. **Crucial:** Reshape the output and tgt tensors correctly for the loss function. CrossEntropyLoss expects predictions of shape `(N, C)` and targets of shape `(N)`, where N is the total number of tokens.\n",
        "\n",
        "```python\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i, batch in enumerate(iterator):\n",
        "        # Unpack the batch (including lengths from the collate_fn)\n",
        "        src, src_lengths, tgt = batch\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # TODO: 1. Forward pass (use default teacher forcing ratio)\n",
        "        # Remember to pass src_lengths to the model\n",
        "        output = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # output shape: (batch_size, tgt_len, output_dim)\n",
        "        # tgt shape: (batch_size, tgt_len)\n",
        "\n",
        "        # TODO: 2. Reshape for loss calculation.\n",
        "        # Flatten the outputs and targets.\n",
        "        output_dim = output.shape[-1]\n",
        "        # Reshape output to (batch_size * tgt_len, output_dim)\n",
        "        output = output.reshape(-1, output_dim)\n",
        "        # Reshape tgt to (batch_size * tgt_len)\n",
        "        tgt = tgt.reshape(-1)\n",
        "\n",
        "        # TODO: 3. Calculate the loss\n",
        "        loss = None # <<< YOUR CODE HERE\n",
        "\n",
        "        # TODO: 4. Backward pass (BPTT)\n",
        "        # <<< YOUR CODE HERE\n",
        "\n",
        "        # 5. Clip gradients to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        # TODO: 6. Update parameters\n",
        "        # <<< YOUR CODE HERE\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            src, src_lengths, tgt = batch\n",
        "\n",
        "            # TODO: 1. Forward pass (Set teacher_forcing_ratio=0 for evaluation)\n",
        "            output = None # <<< YOUR CODE HERE\n",
        "\n",
        "            # TODO: 2. Reshape for loss calculation (same as in train)\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output.reshape(-1, output_dim)\n",
        "            tgt = tgt.reshape(-1)\n",
        "\n",
        "            # TODO: 3. Calculate the loss\n",
        "            loss = None # <<< YOUR CODE HERE\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "```\n",
        "\n",
        "### 5.3 Running the Training (Provided)\n",
        "\n",
        "```python\n",
        "N_EPOCHS = 30  # Note: 15 epochs may not be sufficient for good translations!\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "# NOTE: Uncomment the loop content after completing the tasks above.\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
        "    # valid_loss = evaluate(model, val_loader, criterion)\n",
        "    train_loss = 0 # Placeholder\n",
        "    valid_loss = 0 # Placeholder\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    # if valid_loss < best_valid_loss:\n",
        "    #     best_valid_loss = valid_loss\n",
        "    #     torch.save(model.state_dict(), 'seq2seq-gru-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {int(end_time - start_time)}s')\n",
        "    # PPL (Perplexity) is exp(loss), a common metric for language models.\n",
        "    # print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "```\n",
        "\n",
        "**Important Note on Training Duration:**\n",
        "\n",
        "Vanilla Seq2Seq models typically require 30-50+ epochs to produce reasonable translations. If you train for only 15 epochs, you will likely see:\n",
        "- Decreasing loss (showing the model is learning)\n",
        "- But poor actual translations (the model hasn't converged yet)\n",
        "\n",
        "This is normal! The model needs more time to learn the complex mapping between languages.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 6: Inference and Analysis (10 Points)\n",
        "\n",
        "### 6.1 Inference (Greedy Decoding)\n",
        "\n",
        "During inference, we don't have the target sentence, so teacher forcing is impossible. We use the model's own predictions at each step. The simplest method is **Greedy Decoding**: always choose the word with the highest probability.\n",
        "\n",
        "```python\n",
        "def translate_sentence(sentence, src_lang, tgt_lang, model, device, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Preprocess the input sentence (normalize and reverse!)\n",
        "    normalized_sentence = normalizeString(sentence)\n",
        "    reversed_sentence = ' '.join(normalized_sentence.split(' ')[::-1])\n",
        "\n",
        "    # 2. Convert to indices and tensor\n",
        "    indices = [src_lang.word2index.get(word, UNK_IDX) for word in reversed_sentence.split(' ')] + [EOS_IDX]\n",
        "    src_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device) # (1, T)\n",
        "    src_len = torch.tensor([len(indices)])\n",
        "\n",
        "    # 3. Encode the sentence\n",
        "    with torch.no_grad():\n",
        "        hidden = model.encoder(src_tensor, src_len)\n",
        "\n",
        "    # 4. Start decoding\n",
        "    trg_indices = [SOS_IDX]\n",
        "    input_tensor = torch.tensor([SOS_IDX], dtype=torch.long).to(device) # (1)\n",
        "\n",
        "    for i in range(max_len):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model.decoder(input_tensor, hidden)\n",
        "\n",
        "        # 5. Greedy Decoding\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indices.append(pred_token)\n",
        "\n",
        "        # Check for <EOS>\n",
        "        if pred_token == EOS_IDX:\n",
        "            break\n",
        "\n",
        "        # Prepare the next input\n",
        "        input_tensor = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
        "\n",
        "    # 6. Convert indices back to words\n",
        "    trg_tokens = [tgt_lang.index2word[i] for i in trg_indices]\n",
        "    return trg_tokens[1:-1] # Exclude <SOS> and <EOS>\n",
        "\n",
        "# Qualitative Analysis (Uncomment after training)\n",
        "# model.load_state_dict(torch.load('seq2seq-gru-model.pt'))\n",
        "# examples = [\"i am cold\", \"she is happy\", \"he is running\", \"we are ready\"]\n",
        "# for example in examples:\n",
        "#     translation = translate_sentence(example, input_lang, output_lang, model, device)\n",
        "#     print(f\"EN: {example}\")\n",
        "#     print(f\"FR: {' '.join(translation)}\\n\")\n",
        "```\n",
        "\n",
        "### 6.2 Understanding Your Results\n",
        "\n",
        "**Expected Performance:**\n",
        "\n",
        "After training, you may notice that your translations are not perfect - and that's completely normal! Here's what you should expect:\n",
        "\n",
        "**What Good Results Look Like:**\n",
        "- Training loss decreasing from ~5.0 to ~1.0-1.5\n",
        "- Validation loss around 2.5-3.5\n",
        "- Some simple phrases translating correctly (e.g., \"how are you\" → \"comment vas tu\")\n",
        "- Shorter sentences working better than longer ones\n",
        "\n",
        "**Why Translations May Be Poor:**\n",
        "\n",
        "1. **The Information Bottleneck**: This is the fundamental limitation we've been discussing. The entire English sentence must be compressed into a single fixed-size vector (512 numbers). For complex sentences, critical information gets lost.\n",
        "\n",
        "2. **Insufficient Training**: 30 epochs on this small dataset is barely enough. Production NMT systems train for much longer on millions of examples.\n",
        "\n",
        "3. **Overfitting**: If your validation loss is significantly higher than training loss (e.g., 2.8 vs 1.3), the model is memorizing training patterns rather than learning to translate.\n",
        "\n",
        "4. **Common Phrase Bias**: The model often outputs frequent French phrases (like \"je suis...\") regardless of the actual input, because these patterns were common in training data.\n",
        "\n",
        "5. **Greedy Decoding**: We always pick the highest probability word. Beam search (which considers multiple possibilities) would improve results.\n",
        "\n",
        "**What Your Model Is Actually Learning:**\n",
        "\n",
        "Look at a translation like:\n",
        "```\n",
        "\"i am cold\" → \"je suis serieux\"\n",
        "```\n",
        "\n",
        "The model correctly learned:\n",
        "- \"i am\" → \"je suis\" ✓\n",
        "- But outputs a common word \"serieux\" instead of \"froid\"\n",
        "\n",
        "This shows the model IS learning French grammar and common patterns, just not the specific vocabulary mapping yet.\n",
        "\n",
        "**This Is Why Attention Was Invented!**\n",
        "\n",
        "The poor performance of vanilla Seq2Seq on longer sentences directly motivated the invention of attention mechanisms (covered in the next module). Attention allows the decoder to \"look back\" at different parts of the input instead of relying on a single compressed vector.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.3 Bonus: Diagnostic Function (Optional)\n",
        "\n",
        "To better understand what your model has learned, implement this diagnostic function that checks if the model can at least memorize some training examples:\n",
        "\n",
        "```python\n",
        "def diagnose_model(model, src_lang, tgt_lang, pairs, device, num_examples=5):\n",
        "    \"\"\"\n",
        "    Check if model can translate training examples (memorization test)\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"MODEL DIAGNOSIS - Testing on Training Examples\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    for i in range(num_examples):\n",
        "        en_sentence = pairs[i][0]\n",
        "        fr_actual = pairs[i][1]\n",
        "        fr_predicted = translate_sentence(en_sentence, src_lang, tgt_lang, model, device)\n",
        "        \n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"  EN (input):     {en_sentence}\")\n",
        "        print(f\"  FR (expected):  {fr_actual}\")\n",
        "        print(f\"  FR (predicted): {' '.join(fr_predicted)}\")\n",
        "        \n",
        "        # Calculate word overlap\n",
        "        expected_words = set(fr_actual.split())\n",
        "        predicted_words = set(fr_predicted)\n",
        "        overlap = expected_words.intersection(predicted_words)\n",
        "        if len(expected_words) > 0:\n",
        "            accuracy = len(overlap) / len(expected_words) * 100\n",
        "            print(f\"  Word overlap:   {len(overlap)}/{len(expected_words)} ({accuracy:.1f}%)\")\n",
        "\n",
        "# Run after loading best model\n",
        "diagnose_model(model, input_lang, output_lang, pairs, device)\n",
        "```\n",
        "\n",
        "If the model can't even memorize training examples with >50% word overlap, it needs more training epochs or there may be a bug.\n",
        "\n",
        "---\n",
        "\n",
        "### 6.4 Conceptual Questions\n",
        "\n",
        "Answer the following questions in a separate text cell or document:\n",
        "\n",
        "1. **The Information Bottleneck**: The core limitation of this architecture is that the encoder must compress the entire input sentence into a single fixed-size context vector (hidden). Why is this a significant problem when translating very long or complex sentences?\n",
        "\n",
        "2. **Input Reversal**: Explain again, in your own words, why reversing the input (the \"Reversal Trick\") helped the model learn more effectively. Relate your answer to the concept of gradient flow in BPTT.\n",
        "\n",
        "3. **TBPTT Application**: While we used standard BPTT here, describe a different NLP task where Truncated BPTT (TBPTT) would be essential, and explain why standard BPTT would be unsuitable in that scenario.\n",
        "\n",
        "4. **Packing**: Why is it important to use `pack_padded_sequence` in the encoder when dealing with batched inputs? What might happen if we didn't use it?\n"
      ],
      "metadata": {
        "id": "k8kEIsDB-PlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For those that are interested to improve the performance, try to add:[optional]\n",
        "- Beam Search for better decoding (instead of greedy)\n",
        "- Better evaluation metrics (BLEU score)"
      ],
      "metadata": {
        "id": "VRz38vQV1kVw"
      }
    }
  ]
}