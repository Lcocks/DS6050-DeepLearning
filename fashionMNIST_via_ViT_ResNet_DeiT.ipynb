{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lcocks/DS6050-DeepLearning/blob/main/fashionMNIST_via_ViT_ResNet_DeiT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can skip to the code if you want to run the code on Google COLAB.\n",
        "# Fashion-MNIST Vision Transformer Training on Rivanna\n",
        "\n",
        "This guide walks you through training a Vision Transformer (ViT) model on Rivanna's GPU cluster.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## One-Time Virtual Environment Setup\n",
        "\n",
        "You only need to do this once. The virtual environment will contain all Python dependencies.\n",
        "\n",
        "1. **Request an interactive session** (optional, but recommended for testing):\n",
        "   ```bash\n",
        "   ijob -c 4 -p standard -A your_allocation\n",
        "   ```\n",
        "\n",
        "2. **Create and activate a virtual environment:**\n",
        "   ```bash\n",
        "   mkdir -p ~/venvs\n",
        "   python -m venv ~/venvs/dl-course\n",
        "   source ~/venvs/dl-course/bin/activate\n",
        "   ```\n",
        "\n",
        "3. **Install dependencies:**\n",
        "   ```bash\n",
        "   python -m pip install --upgrade pip wheel\n",
        "   python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "   python -m pip install numpy pillow tqdm matplotlib\n",
        "   ```\n",
        "\n",
        "4. **Verify installation (optional):**\n",
        "   ```bash\n",
        "   python -c \"import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}')\"\n",
        "   ```\n",
        "\n",
        "5. **Deactivate when done:**\n",
        "   ```bash\n",
        "   deactivate\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "## SLURM Submission Script\n",
        "\n",
        "Create or edit `~/DL_course/ViT/run_training.sh` with the following content:\n",
        "\n",
        "```bash\n",
        "#!/bin/bash\n",
        "#SBATCH --job-name=fmnist-vit\n",
        "#SBATCH --partition=gpu\n",
        "#SBATCH --gres=gpu:1\n",
        "#SBATCH --time=12:00:00\n",
        "#SBATCH --nodes=1\n",
        "#SBATCH --ntasks=1\n",
        "#SBATCH --cpus-per-task=8\n",
        "#SBATCH --mem=32G\n",
        "#SBATCH --output=/home/%u/DL_course/ViT/logs/fmnist-vit-%j.out\n",
        "#SBATCH --account=YOUR_ALLOCATION_HERE\n",
        "\n",
        "# Exit on error\n",
        "set -euo pipefail\n",
        "\n",
        "# Load CUDA modules\n",
        "module purge\n",
        "module load cuda/12.8.0\n",
        "module load cudnn/9.8.0-CUDA-12.8.0\n",
        "\n",
        "# Determine project directory\n",
        "if [[ -n \"${SLURM_SUBMIT_DIR:-}\" ]]; then\n",
        "    cd \"${SLURM_SUBMIT_DIR}\" || exit 1\n",
        "else\n",
        "    cd \"$(dirname \"${BASH_SOURCE[0]}\")/..\" || exit 1\n",
        "fi\n",
        "\n",
        "PROJECT_ROOT=$(pwd)\n",
        "\n",
        "# Locate training script\n",
        "if [[ -f \"${PROJECT_ROOT}/ViT/train_fashionmnist.py\" ]]; then\n",
        "    SCRIPT_DIR=\"${PROJECT_ROOT}/ViT\"\n",
        "elif [[ -f \"${PROJECT_ROOT}/train_fashionmnist.py\" ]]; then\n",
        "    SCRIPT_DIR=\"${PROJECT_ROOT}\"\n",
        "else\n",
        "    echo \"ERROR: Could not locate train_fashionmnist.py\"\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "PYTHON_SCRIPT=\"${SCRIPT_DIR}/train_fashionmnist.py\"\n",
        "\n",
        "# Activate virtual environment\n",
        "VENV_PATH=${VENV_PATH:-$HOME/venvs/dl-course}\n",
        "if [[ ! -d \"${VENV_PATH}\" ]]; then\n",
        "    echo \"ERROR: Virtual environment not found at ${VENV_PATH}\"\n",
        "    echo \"Please create it following the setup instructions.\"\n",
        "    exit 1\n",
        "fi\n",
        "source \"${VENV_PATH}/bin/activate\"\n",
        "\n",
        "echo \"Starting training...\"\n",
        "echo \"Python script: ${PYTHON_SCRIPT}\"\n",
        "echo \"Virtual environment: ${VENV_PATH}\"\n",
        "\n",
        "# Run training script with arguments\n",
        "python \"${PYTHON_SCRIPT}\" \"$@\"\n",
        "\n",
        "echo \"Training complete!\"\n",
        "```\n",
        "\n",
        "**Important:** Replace `YOUR_ALLOCATION_HERE` with your actual allocation name (e.g., `hpc_build`).\n",
        "\n",
        "---\n",
        "\n",
        "## Running Your Training Job\n",
        "\n",
        "1. **Navigate to project directory:**\n",
        "   ```bash\n",
        "   cd ~/DL_course\n",
        "   ```\n",
        "\n",
        "2. **Make the script executable:**\n",
        "   ```bash\n",
        "   chmod +x ViT/run_training.sh\n",
        "   ```\n",
        "\n",
        "3. **Submit a training job:**\n",
        "   ```bash\n",
        "   sbatch ViT/run_training.sh --plot-results --epochs 30\n",
        "   ```\n",
        "\n",
        "4. **Submit with custom virtual environment path (if different):**\n",
        "   ```bash\n",
        "   sbatch --export=ALL,VENV_PATH=$HOME/my-custom-venv ViT/run_training.sh --plot-results --epochs 30\n",
        "   ```\n",
        "\n",
        "---\n",
        "\n",
        "## Monitoring Your Job\n",
        "\n",
        "- **Check job status:**\n",
        "  ```bash\n",
        "  squeue -u $USER\n",
        "  ```\n",
        "\n",
        "- **View job details:**\n",
        "  ```bash\n",
        "  scontrol show job <JOBID>\n",
        "  ```\n",
        "\n",
        "- **Monitor output in real-time:**\n",
        "  ```bash\n",
        "  tail -f ~/DL_course/ViT/logs/fmnist-vit-<JOBID>.out\n",
        "  ```\n",
        "\n",
        "- **Check all your recent jobs:**\n",
        "  ```bash\n",
        "  sacct -u $USER --format=JobID,JobName,Partition,State,Elapsed,ExitCode\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## Understanding Outputs\n",
        "\n",
        "After training completes, you'll find:\n",
        "\n",
        "- **Logs:** `~/DL_course/ViT/logs/fmnist-vit-<JOBID>.out`\n",
        "- **Model checkpoints:** `~/DL_course/ViT/saved_models/*.pth`\n",
        "- **Training metrics:** `~/DL_course/ViT/metrics/*.json`\n",
        "- **Plots:** `~/DL_course/ViT/plots/*.png` (if `--plot-results` was used)\n",
        "\n",
        "---\n",
        "\n",
        "## Command-Line Arguments\n",
        "\n",
        "You can customize training by passing arguments to the Python script:\n",
        "\n",
        "```bash\n",
        "sbatch ViT/run_training.sh \\\n",
        "  --epochs 50 \\\n",
        "  --batch-size 128 \\\n",
        "  --learning-rate 0.001 \\\n",
        "  --plot-results \\\n",
        "  --phases 1 2 3\n",
        "```\n",
        "\n",
        "Common options:\n",
        "- `--epochs N`: Number of training epochs (default: 30)\n",
        "- `--batch-size N`: Batch size (default: 64)\n",
        "- `--learning-rate X`: Learning rate (default: 0.0001)\n",
        "- `--plot-results`: Generate plots after training\n",
        "- `--phases 1 2 3`: Run specific training phases only\n",
        "\n",
        "---\n",
        "\n",
        "## Rerunning or Resuming Training\n",
        "\n",
        "The training script automatically detects existing checkpoints and skips completed phases.\n",
        "\n",
        "**To retrain a specific model:**\n",
        "```bash\n",
        "rm ~/DL_course/ViT/saved_models/DeiT_Distilled_*.pth\n",
        "sbatch ViT/run_training.sh --plot-results --epochs 30\n",
        "```\n",
        "\n",
        "**To start completely fresh:**\n",
        "```bash\n",
        "rm -rf ~/DL_course/ViT/saved_models/*\n",
        "rm -rf ~/DL_course/ViT/metrics/*\n",
        "sbatch ViT/run_training.sh --plot-results --epochs 30\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "### Job fails with \"Virtual environment not found\"\n",
        "Make sure you created the venv and the path matches:\n",
        "```bash\n",
        "ls -la ~/venvs/dl-course/bin/activate\n",
        "```\n",
        "\n",
        "### Job fails with \"module not found\"\n",
        "Check available CUDA versions and update the script:\n",
        "```bash\n",
        "module avail cuda\n",
        "```\n",
        "\n",
        "### Out of memory errors\n",
        "Reduce batch size or request more memory in the SLURM script:\n",
        "```bash\n",
        "#SBATCH --mem=64G\n",
        "```\n",
        "\n",
        "### Job pending forever\n",
        "Check your allocation and available resources:\n",
        "```bash\n",
        "squeue -u $USER\n",
        "sacctmgr show associations user=$USER\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Cleanup\n",
        "\n",
        "**Cancel a running job:**\n",
        "```bash\n",
        "scancel <JOBID>\n",
        "```\n",
        "\n",
        "**Remove virtual environment** (only if you're completely done):\n",
        "```bash\n",
        "rm -rf ~/venvs/dl-course\n",
        "```\n",
        "\n",
        "**Clean up old logs:**\n",
        "```bash\n",
        "rm ~/DL_course/ViT/logs/*.out\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Reference\n",
        "\n",
        "| Task | Command |\n",
        "|------|---------|\n",
        "| Submit job | `sbatch ViT/run_training.sh --plot-results --epochs 30` |\n",
        "| Check status | `squeue -u $USER` |\n",
        "| View output | `tail -f ~/DL_course/ViT/logs/fmnist-vit-<JOBID>.out` |\n",
        "| Cancel job | `scancel <JOBID>` |\n",
        "| Retrain model | `rm saved_models/<model>.pth && sbatch ...` |\n",
        "\n",
        "---\n",
        "\n",
        "**Questions?** Contact your instructor or consult the [Rivanna Documentation](https://www.rc.virginia.edu/userinfo/rivanna/overview/)."
      ],
      "metadata": {
        "id": "jewm3Yzd8B3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\"\"\"\n",
        "Train ResNet18, Vision Transformer, and DeiT models on FashionMNIST with heavy\n",
        "augmentation options. Intended for use on UVA Rivanna or similar HPC clusters.\n",
        "\n",
        "Example:\n",
        "    python train_fashionmnist.py --batch-size 128 --epochs 30 --data-dir ./data\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Tuple\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "# ==================== Utility Helpers ====================\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# ==================== Data Augmentation ====================\n",
        "\n",
        "class RandAugment:\n",
        "    \"\"\"RandAugment for FashionMNIST.\"\"\"\n",
        "\n",
        "    def __init__(self, n: int = 2, m: int = 9) -> None:\n",
        "        self.n = n  # Number of augmentations\n",
        "        self.m = m  # Magnitude (unused placeholder to match API)\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = [\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "            transforms.RandomAffine(0, shear=15),\n",
        "        ]\n",
        "\n",
        "        for _ in range(self.n):\n",
        "            op = np.random.choice(ops)\n",
        "            img = op(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "class RandomErasing:\n",
        "    \"\"\"Random erasing augmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, p: float = 0.5, scale: Tuple[float, float] = (0.02, 0.33)) -> None:\n",
        "        self.p = p\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() > self.p:\n",
        "            return img\n",
        "\n",
        "        h, w = img.shape[-2:]\n",
        "        area = h * w\n",
        "        target_area = np.random.uniform(*self.scale) * area\n",
        "        aspect_ratio = np.random.uniform(0.3, 3.3)\n",
        "\n",
        "        h_erase = int(round(np.sqrt(target_area * aspect_ratio)))\n",
        "        w_erase = int(round(np.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "        if h_erase < h and w_erase < w:\n",
        "            i = np.random.randint(0, h - h_erase)\n",
        "            j = np.random.randint(0, w - w_erase)\n",
        "            img[:, i : i + h_erase, j : j + w_erase] = 0\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "def mixup_data(x, y, alpha: float = 0.2):\n",
        "    \"\"\"Mixup augmentation.\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size, device=x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "def cutmix_data(x, y, alpha: float = 1.0):\n",
        "    \"\"\"CutMix augmentation.\"\"\"\n",
        "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1.0\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    index = torch.randperm(batch_size, device=x.device)\n",
        "\n",
        "    W = x.size(2)\n",
        "    H = x.size(3)\n",
        "    cut_rat = np.sqrt(1.0 - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return x, y_a, y_b, lam\n",
        "\n",
        "\n",
        "# ==================== Vision Transformer Implementation ====================\n",
        "\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them.\"\"\"\n",
        "\n",
        "    def __init__(self, img_size: int = 32, patch_size: int = 4, in_channels: int = 1, embed_dim: int = 384) -> None:\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\"Embedding dimension must be divisible by number of heads.\")\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        scale = self.head_dim ** -0.5\n",
        "        attn = (q @ k.transpose(-2, -1)) * scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP block with GELU activation.\"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, hidden_features: int, dropout: float = 0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block with pre-normalization.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_heads: int, mlp_ratio: float = 4, dropout: float = 0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer for image classification.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 32,\n",
        "        patch_size: int = 4,\n",
        "        in_channels: int = 1,\n",
        "        num_classes: int = 10,\n",
        "        embed_dim: int = 384,\n",
        "        num_heads: int = 6,\n",
        "        num_blocks: int = 6,\n",
        "        mlp_ratio: float = 4,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        n_patches = self.patch_embed.n_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x[:, 0])\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ==================== DeiT Implementation ====================\n",
        "\n",
        "\n",
        "class DeiT(nn.Module):\n",
        "    \"\"\"Data-efficient Image Transformer with distillation token.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: int = 32,\n",
        "        patch_size: int = 4,\n",
        "        in_channels: int = 1,\n",
        "        num_classes: int = 10,\n",
        "        embed_dim: int = 384,\n",
        "        num_heads: int = 6,\n",
        "        num_blocks: int = 6,\n",
        "        mlp_ratio: float = 4,\n",
        "        dropout: float = 0.1,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        n_patches = self.patch_embed.n_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 2, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout) for _ in range(num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        self.head_dist = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        dist_tokens = self.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        x_cls = self.head(x[:, 0])\n",
        "        x_dist = self.head_dist(x[:, 1])\n",
        "\n",
        "        return x_cls, x_dist\n",
        "\n",
        "\n",
        "# ==================== ResNet18 Implementation ====================\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Basic residual block for ResNet.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    \"\"\"ResNet18 architecture.\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int = 1, num_classes: int = 10) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _make_layer(self, in_channels: int, out_channels: int, num_blocks: int, stride: int):\n",
        "        layers = [ResidualBlock(in_channels, out_channels, stride)]\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# ==================== Data Loading ====================\n",
        "\n",
        "\n",
        "def get_dataloaders(\n",
        "    batch_size: int = 128,\n",
        "    data_dir: Path | str = \"./data\",\n",
        "    use_augmentation: bool = False,\n",
        "    num_workers: int = 2,\n",
        ") -> Tuple[DataLoader, DataLoader, DataLoader, DataLoader]:\n",
        "    \"\"\"Create data loaders with optional heavy augmentation.\"\"\"\n",
        "\n",
        "    if use_augmentation:\n",
        "        transform_train = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(32),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                RandAugment(n=2, m=9),\n",
        "                transforms.ToTensor(),\n",
        "                RandomErasing(p=0.25),\n",
        "                transforms.Normalize((0.5,), (0.5,)),\n",
        "            ]\n",
        "        )\n",
        "    else:\n",
        "        transform_train = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(32),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    transform_test = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(32),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    transform_resnet_train = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(96),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    transform_resnet_test = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(96),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    data_dir = Path(data_dir)\n",
        "    data_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataset = datasets.FashionMNIST(root=data_dir, train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.FashionMNIST(root=data_dir, train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_dataset_resnet = datasets.FashionMNIST(\n",
        "        root=data_dir, train=True, download=True, transform=transform_resnet_train\n",
        "    )\n",
        "    test_dataset_resnet = datasets.FashionMNIST(\n",
        "        root=data_dir, train=False, download=True, transform=transform_resnet_test\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    train_loader_resnet = DataLoader(\n",
        "        train_dataset_resnet, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "    test_loader_resnet = DataLoader(\n",
        "        test_dataset_resnet, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader, train_loader_resnet, test_loader_resnet\n",
        "\n",
        "\n",
        "# ==================== Training Functions ====================\n",
        "\n",
        "\n",
        "def count_parameters(model: nn.Module) -> int:\n",
        "    \"\"\"Count the number of trainable parameters.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def save_model(model: nn.Module, path: Path, epoch: int, optimizer: optim.Optimizer, acc: float) -> None:\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(\n",
        "        {\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"accuracy\": acc,\n",
        "        },\n",
        "        path,\n",
        "    )\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "\n",
        "def load_model_if_exists(model: nn.Module, model_name: str, device: torch.device, save_dir: Path):\n",
        "    \"\"\"Load model if checkpoint exists.\"\"\"\n",
        "    final_path = save_dir / f\"{model_name}_final.pth\"\n",
        "    best_path = save_dir / f\"{model_name}_best.pth\"\n",
        "\n",
        "    if final_path.exists():\n",
        "        print(f\"Loading existing model from {final_path}\")\n",
        "        checkpoint = torch.load(final_path, map_location=device, weights_only=False)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "        model = model.to(device)\n",
        "        print(\n",
        "            f\"Loaded model - Epoch: {checkpoint['epoch']}, Accuracy: {checkpoint['accuracy']:.2f}%\"\n",
        "        )\n",
        "        return model, checkpoint[\"accuracy\"], True\n",
        "\n",
        "    if best_path.exists():\n",
        "        print(f\"Best checkpoint found at {best_path} (no final checkpoint)\")\n",
        "\n",
        "    return model, 0.0, False\n",
        "\n",
        "\n",
        "def save_metrics(metrics_dir: Path, model_key: str, history: Dict[str, list], best_acc: float) -> None:\n",
        "    \"\"\"Persist training history and metadata for later analysis.\"\"\"\n",
        "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
        "    payload = {\"history\": history, \"best_acc\": best_acc}\n",
        "    target = metrics_dir / f\"{model_key}.json\"\n",
        "    with target.open(\"w\", encoding=\"utf-8\") as fp:\n",
        "        json.dump(payload, fp, indent=2)\n",
        "    print(f\"Metrics saved to {target}\")\n",
        "\n",
        "\n",
        "def load_metrics(metrics_dir: Path, model_key: str) -> Dict[str, Any] | None:\n",
        "    \"\"\"Load previously saved metrics if available.\"\"\"\n",
        "    target = metrics_dir / f\"{model_key}.json\"\n",
        "    if target.exists():\n",
        "        with target.open(\"r\", encoding=\"utf-8\") as fp:\n",
        "            data = json.load(fp)\n",
        "        return data\n",
        "    return None\n",
        "\n",
        "\n",
        "def train_epoch(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    use_augment: bool = False,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "        if use_augment and np.random.rand() < 0.5:\n",
        "            if np.random.rand() < 0.5:\n",
        "                inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                outputs = model(inputs)\n",
        "                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "            else:\n",
        "                inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets)\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                outputs = model(inputs)\n",
        "                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        else:\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def train_epoch_deit(\n",
        "    model: DeiT,\n",
        "    loader: DataLoader,\n",
        "    teacher: nn.Module,\n",
        "    criterion: nn.Module,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    alpha: float = 0.5,\n",
        "    beta: float = 0.5,\n",
        "    temperature: float = 3.0,\n",
        "    use_augment: bool = True,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Train DeiT for one epoch with distillation.\"\"\"\n",
        "    model.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "        if use_augment and np.random.rand() < 0.5:\n",
        "            if np.random.rand() < 0.5:\n",
        "                inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "            else:\n",
        "                inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets)\n",
        "\n",
        "            inputs_teacher = F.interpolate(inputs, size=(96, 96), mode=\"bilinear\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher(inputs_teacher)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs_cls, outputs_dist = model(inputs)\n",
        "\n",
        "            loss_cls = lam * criterion(outputs_cls, targets_a) + (1 - lam) * criterion(outputs_cls, targets_b)\n",
        "            loss_dist_hard = (\n",
        "                lam * criterion(outputs_dist, targets_a) + (1 - lam) * criterion(outputs_dist, targets_b)\n",
        "            )\n",
        "\n",
        "            loss_kl = (\n",
        "                F.kl_div(\n",
        "                    F.log_softmax(outputs_dist / temperature, dim=1),\n",
        "                    F.softmax(teacher_outputs / temperature, dim=1),\n",
        "                    reduction=\"batchmean\",\n",
        "                )\n",
        "                * (temperature**2)\n",
        "            )\n",
        "\n",
        "            loss = (1 - alpha) * loss_cls + alpha * loss_dist_hard + beta * loss_kl\n",
        "        else:\n",
        "            inputs_teacher = F.interpolate(inputs, size=(96, 96), mode=\"bilinear\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher(inputs_teacher)\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            outputs_cls, outputs_dist = model(inputs)\n",
        "\n",
        "            loss_cls = criterion(outputs_cls, targets)\n",
        "            loss_dist_hard = criterion(outputs_dist, targets)\n",
        "\n",
        "            loss_kl = (\n",
        "                F.kl_div(\n",
        "                    F.log_softmax(outputs_dist / temperature, dim=1),\n",
        "                    F.softmax(teacher_outputs / temperature, dim=1),\n",
        "                    reduction=\"batchmean\",\n",
        "                )\n",
        "                * (temperature**2)\n",
        "            )\n",
        "\n",
        "            loss = (1 - alpha) * loss_cls + alpha * loss_dist_hard + beta * loss_kl\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs_cls.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module, device: torch.device, is_deit: bool = False):\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
        "\n",
        "            if is_deit:\n",
        "                outputs_cls, outputs_dist = model(inputs)\n",
        "                outputs = (outputs_cls + outputs_dist) / 2\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100.0 * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    test_loader: DataLoader,\n",
        "    num_epochs: int,\n",
        "    lr: float,\n",
        "    device: torch.device,\n",
        "    model_name: str,\n",
        "    save_dir: Path,\n",
        "    use_augment: bool = False,\n",
        "    teacher: nn.Module | None = None,\n",
        "    is_deit: bool = False,\n",
        ") -> Tuple[Dict[str, list], float]:\n",
        "    \"\"\"Train and evaluate a model.\"\"\"\n",
        "    separator = \"=\" * 60\n",
        "    print(f\"\\n{separator}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(separator)\n",
        "    print(f\"Parameters: {count_parameters(model):,}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    def lr_lambda(epoch: int) -> float:\n",
        "        if epoch < 5:\n",
        "            return (epoch + 1) / 5\n",
        "        return 0.5 * (1 + np.cos(np.pi * (epoch - 5) / (max(num_epochs - 5, 1))))\n",
        "\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    history: Dict[str, list] = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        if is_deit and teacher is not None:\n",
        "            train_loss, train_acc = train_epoch_deit(\n",
        "                model,\n",
        "                train_loader,\n",
        "                teacher,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                device,\n",
        "                use_augment=use_augment,\n",
        "            )\n",
        "        else:\n",
        "            train_loss, train_acc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer, device, use_augment=use_augment\n",
        "            )\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion, device, is_deit=is_deit)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"test_loss\"].append(test_loss)\n",
        "        history[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch + 1}/{num_epochs}] \"\n",
        "            f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% \"\n",
        "            f\"Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}% \"\n",
        "            f\"Time: {epoch_time:.2f}s LR: {scheduler.get_last_lr()[0]:.6f}\"\n",
        "        )\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            save_path = save_dir / f\"{model_name.replace(' ', '_')}_best.pth\"\n",
        "            save_model(model, save_path, epoch, optimizer, best_acc)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    save_path = save_dir / f\"{model_name.replace(' ', '_')}_final.pth\"\n",
        "    save_model(model, save_path, num_epochs, optimizer, history[\"test_acc\"][-1])\n",
        "\n",
        "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
        "    print(f\"Best test accuracy: {best_acc:.2f}%\")\n",
        "    print(f\"Final test accuracy: {history['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "    return history, best_acc\n",
        "\n",
        "\n",
        "def generate_plots(plots_dir: Path, metrics: Dict[str, Dict[str, Any]]) -> None:\n",
        "    \"\"\"Generate comparison plots for available metrics.\"\"\"\n",
        "    try:\n",
        "        import matplotlib\n",
        "        matplotlib.use(\"Agg\", force=True)\n",
        "        import matplotlib.pyplot as plt\n",
        "    except ImportError:\n",
        "        print(\"Matplotlib not installed; skipping plot generation.\")\n",
        "        return\n",
        "\n",
        "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _plot_series(key: str, ylabel: str, filename: str) -> None:\n",
        "        plt.figure()\n",
        "        has_data = False\n",
        "        for model_name, payload in metrics.items():\n",
        "            history = payload.get(\"history\", {})\n",
        "            series = history.get(key)\n",
        "            if not series:\n",
        "                continue\n",
        "            epochs = range(1, len(series) + 1)\n",
        "            label = model_name.replace(\"_\", \" \")\n",
        "            plt.plot(epochs, series, label=label)\n",
        "            has_data = True\n",
        "        if has_data:\n",
        "            plt.xlabel(\"Epoch\")\n",
        "            plt.ylabel(ylabel)\n",
        "            plt.title(f\"{ylabel} by Epoch\")\n",
        "            plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "            plt.legend()\n",
        "            plt.tight_layout()\n",
        "            outfile = plots_dir / filename\n",
        "            plt.savefig(outfile)\n",
        "            print(f\"Saved plot: {outfile}\")\n",
        "        plt.close()\n",
        "\n",
        "    _plot_series(\"train_loss\", \"Train Loss\", \"train_loss.png\")\n",
        "    _plot_series(\"test_loss\", \"Test Loss\", \"test_loss.png\")\n",
        "    _plot_series(\"train_acc\", \"Train Accuracy (%)\", \"train_accuracy.png\")\n",
        "    _plot_series(\"test_acc\", \"Test Accuracy (%)\", \"test_accuracy.png\")\n",
        "\n",
        "    # Bar chart summarizing best test accuracy\n",
        "    best_entries = [\n",
        "        (model_name.replace(\"_\", \" \"), payload.get(\"best_acc\"))\n",
        "        for model_name, payload in metrics.items()\n",
        "        if payload.get(\"best_acc\") is not None\n",
        "    ]\n",
        "    if best_entries:\n",
        "        labels, values = zip(*best_entries)\n",
        "        plt.figure()\n",
        "        plt.bar(labels, values)\n",
        "        plt.ylabel(\"Best Test Accuracy (%)\")\n",
        "        plt.title(\"Best Test Accuracy Comparison\")\n",
        "        plt.xticks(rotation=20, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        outfile = plots_dir / \"best_accuracy.png\"\n",
        "        plt.savefig(outfile)\n",
        "        plt.close()\n",
        "        print(f\"Saved plot: {outfile}\")\n",
        "\n",
        "\n",
        "# ==================== Main Execution ====================\n",
        "\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    parser = argparse.ArgumentParser(description=\"Train ViT/DeiT models on FashionMNIST.\")\n",
        "    parser.add_argument(\"--batch-size\", type=int, default=128, help=\"Mini-batch size for training.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=30, help=\"Number of training epochs.\")\n",
        "    parser.add_argument(\"--lr-vit\", type=float, default=1e-3, help=\"Learning rate for Vision Transformer.\")\n",
        "    parser.add_argument(\"--lr-resnet\", type=float, default=1e-2, help=\"Learning rate for ResNet18 teacher.\")\n",
        "    parser.add_argument(\"--lr-deit\", type=float, default=1e-3, help=\"Learning rate for DeiT student.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
        "    parser.add_argument(\"--data-dir\", type=Path, default=Path(\"./data\"), help=\"Dataset download root.\")\n",
        "    parser.add_argument(\"--save-dir\", type=Path, default=Path(\"./saved_models\"), help=\"Directory for checkpoints.\")\n",
        "    parser.add_argument(\n",
        "        \"--num-workers\",\n",
        "        type=int,\n",
        "        default=4,\n",
        "        help=\"Number of worker processes for dataloaders.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--skip-resnet\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Skip training the ResNet teacher if a checkpoint is already available.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--skip-vit\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Skip training the ViT baseline if a checkpoint is already available.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--skip-deit\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Skip training the DeiT student if a checkpoint is already available.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--metrics-dir\",\n",
        "        type=Path,\n",
        "        default=Path(\"./metrics\"),\n",
        "        help=\"Directory for saving/loading training metrics.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--plots-dir\",\n",
        "        type=Path,\n",
        "        default=Path(\"./plots\"),\n",
        "        help=\"Directory for saving generated plots.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no-save-metrics\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Disable saving metrics to disk.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--plot-results\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Generate plots comparing training curves (requires matplotlib).\",\n",
        "    )\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = parse_args()\n",
        "\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    args.save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    args.metrics_dir = args.metrics_dir.resolve()\n",
        "    args.plots_dir = args.plots_dir.resolve()\n",
        "    args.save_metrics = not args.no_save_metrics\n",
        "\n",
        "    if args.plot_results and not args.metrics_dir.exists():\n",
        "        print(f\"Metrics directory {args.metrics_dir} does not exist yet; plots may be incomplete.\")\n",
        "\n",
        "    print(\"Loading FashionMNIST dataset...\")\n",
        "    (\n",
        "        train_loader_vit,\n",
        "        test_loader_vit,\n",
        "        train_loader_resnet,\n",
        "        test_loader_resnet,\n",
        "    ) = get_dataloaders(\n",
        "        batch_size=args.batch_size,\n",
        "        data_dir=args.data_dir,\n",
        "        use_augmentation=False,\n",
        "        num_workers=args.num_workers,\n",
        "    )\n",
        "\n",
        "    train_loader_deit, test_loader_deit, _, _ = get_dataloaders(\n",
        "        batch_size=args.batch_size,\n",
        "        data_dir=args.data_dir,\n",
        "        use_augmentation=True,\n",
        "        num_workers=args.num_workers,\n",
        "    )\n",
        "\n",
        "    metrics_data: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "    # ========== Train ResNet18 (Teacher) ==========\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PHASE 1: Training ResNet18 (Teacher)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    resnet_model = ResNet18(in_channels=1, num_classes=10)\n",
        "    resnet_model, resnet_best_acc, resnet_loaded = load_model_if_exists(\n",
        "        resnet_model, \"ResNet18_Teacher\", device, args.save_dir\n",
        "    )\n",
        "\n",
        "    metrics_key_resnet = \"ResNet18_Teacher\"\n",
        "    resnet_metrics = load_metrics(args.metrics_dir, metrics_key_resnet) if args.metrics_dir.exists() else None\n",
        "\n",
        "    if not resnet_loaded and not args.skip_resnet:\n",
        "        resnet_history, resnet_best_acc = train_model(\n",
        "            resnet_model,\n",
        "            train_loader_resnet,\n",
        "            test_loader_resnet,\n",
        "            args.epochs,\n",
        "            args.lr_resnet,\n",
        "            device,\n",
        "            \"ResNet18_Teacher\",\n",
        "            args.save_dir,\n",
        "        )\n",
        "        resnet_metrics = {\"history\": resnet_history, \"best_acc\": resnet_best_acc}\n",
        "    else:\n",
        "        print(\"Skipping training - using loaded model\")\n",
        "        if resnet_metrics is None:\n",
        "            resnet_metrics = {\"history\": {\"test_acc\": [resnet_best_acc]}, \"best_acc\": resnet_best_acc}\n",
        "\n",
        "    if args.save_metrics:\n",
        "        save_metrics(args.metrics_dir, metrics_key_resnet, resnet_metrics[\"history\"], resnet_metrics[\"best_acc\"])\n",
        "\n",
        "    resnet_history = resnet_metrics[\"history\"]\n",
        "    resnet_best_acc = resnet_metrics.get(\"best_acc\", resnet_best_acc)\n",
        "    metrics_data[metrics_key_resnet] = resnet_metrics\n",
        "\n",
        "    # ========== Train ViT (Baseline) ==========\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PHASE 2: Training Vision Transformer (Baseline)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    vit_model = VisionTransformer(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=1,\n",
        "        num_classes=10,\n",
        "        embed_dim=384,\n",
        "        num_heads=6,\n",
        "        num_blocks=6,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1,\n",
        "    )\n",
        "\n",
        "    vit_model, vit_best_acc, vit_loaded = load_model_if_exists(\n",
        "        vit_model, \"Vision_Transformer\", device, args.save_dir\n",
        "    )\n",
        "\n",
        "    metrics_key_vit = \"Vision_Transformer\"\n",
        "    vit_metrics = load_metrics(args.metrics_dir, metrics_key_vit) if args.metrics_dir.exists() else None\n",
        "\n",
        "    if not vit_loaded and not args.skip_vit:\n",
        "        vit_history, vit_best_acc = train_model(\n",
        "            vit_model,\n",
        "            train_loader_vit,\n",
        "            test_loader_vit,\n",
        "            args.epochs,\n",
        "            args.lr_vit,\n",
        "            device,\n",
        "            \"Vision_Transformer\",\n",
        "            args.save_dir,\n",
        "        )\n",
        "        vit_metrics = {\"history\": vit_history, \"best_acc\": vit_best_acc}\n",
        "    else:\n",
        "        print(\"Skipping training - using loaded model\")\n",
        "        if vit_metrics is None:\n",
        "            vit_metrics = {\"history\": {\"test_acc\": [vit_best_acc]}, \"best_acc\": vit_best_acc}\n",
        "\n",
        "    if args.save_metrics:\n",
        "        save_metrics(args.metrics_dir, metrics_key_vit, vit_metrics[\"history\"], vit_metrics[\"best_acc\"])\n",
        "\n",
        "    vit_history = vit_metrics[\"history\"]\n",
        "    vit_best_acc = vit_metrics.get(\"best_acc\", vit_best_acc)\n",
        "    metrics_data[metrics_key_vit] = vit_metrics\n",
        "\n",
        "    # ========== Train DeiT with Distillation ==========\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PHASE 3: Training DeiT with Knowledge Distillation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    deit_model = DeiT(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=1,\n",
        "        num_classes=10,\n",
        "        embed_dim=384,\n",
        "        num_heads=6,\n",
        "        num_blocks=6,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1,\n",
        "    )\n",
        "\n",
        "    deit_model, deit_best_acc, deit_loaded = load_model_if_exists(\n",
        "        deit_model, \"DeiT_Distilled\", device, args.save_dir\n",
        "    )\n",
        "\n",
        "    metrics_key_deit = \"DeiT_Distilled\"\n",
        "    deit_metrics = load_metrics(args.metrics_dir, metrics_key_deit) if args.metrics_dir.exists() else None\n",
        "\n",
        "    if not deit_loaded and not args.skip_deit:\n",
        "        if not resnet_loaded:\n",
        "            teacher_ckpt = args.save_dir / \"ResNet18_Teacher_best.pth\"\n",
        "            if teacher_ckpt.exists():\n",
        "                checkpoint = torch.load(teacher_ckpt, map_location=device, weights_only=False)\n",
        "                resnet_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "                print(f\"Loaded teacher weights from {teacher_ckpt}\")\n",
        "            else:\n",
        "                raise FileNotFoundError(\n",
        "                    f\"Teacher checkpoint {teacher_ckpt} not found. \"\n",
        "                    \"Please train the ResNet teacher first or provide a checkpoint.\"\n",
        "                )\n",
        "\n",
        "        resnet_model = resnet_model.to(device)\n",
        "        resnet_model.eval()\n",
        "\n",
        "        deit_history, deit_best_acc = train_model(\n",
        "            deit_model,\n",
        "            train_loader_deit,\n",
        "            test_loader_deit,\n",
        "            args.epochs,\n",
        "            args.lr_deit,\n",
        "            device,\n",
        "            \"DeiT_Distilled\",\n",
        "            args.save_dir,\n",
        "            use_augment=True,\n",
        "            teacher=resnet_model,\n",
        "            is_deit=True,\n",
        "        )\n",
        "        deit_metrics = {\"history\": deit_history, \"best_acc\": deit_best_acc}\n",
        "    else:\n",
        "        print(\"Skipping training - using loaded model\")\n",
        "        if deit_metrics is None:\n",
        "            deit_metrics = {\"history\": {\"test_acc\": [deit_best_acc]}, \"best_acc\": deit_best_acc}\n",
        "\n",
        "    if args.save_metrics:\n",
        "        save_metrics(args.metrics_dir, metrics_key_deit, deit_metrics[\"history\"], deit_metrics[\"best_acc\"])\n",
        "\n",
        "    deit_history = deit_metrics[\"history\"]\n",
        "    deit_best_acc = deit_metrics.get(\"best_acc\", deit_best_acc)\n",
        "    metrics_data[metrics_key_deit] = deit_metrics\n",
        "\n",
        "    # ========== Final Comparison ==========\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(\"FINAL COMPARISON\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    print(f\"\\nResNet18 (Teacher):\")\n",
        "    print(f\"  Parameters: {count_parameters(resnet_model):,}\")\n",
        "    print(f\"  Best Test Acc: {resnet_best_acc:.2f}%\")\n",
        "    if not (resnet_loaded or args.skip_resnet):\n",
        "        print(f\"  Final Test Acc: {resnet_history['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(f\"\\nVision Transformer (Baseline):\")\n",
        "    print(f\"  Parameters: {count_parameters(vit_model):,}\")\n",
        "    print(f\"  Best Test Acc: {vit_best_acc:.2f}%\")\n",
        "    if not (vit_loaded or args.skip_vit):\n",
        "        print(f\"  Final Test Acc: {vit_history['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(f\"\\nDeiT (Student with Distillation):\")\n",
        "    print(f\"  Parameters: {count_parameters(deit_model):,}\")\n",
        "    print(f\"  Best Test Acc: {deit_best_acc:.2f}%\")\n",
        "    if not (deit_loaded or args.skip_deit):\n",
        "        print(f\"  Final Test Acc: {deit_history['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(f\"  Improvement over ViT: {deit_best_acc - vit_best_acc:+.2f}%\")\n",
        "\n",
        "    print(f\"\\n{'=' * 60}\")\n",
        "    print(f\"All models saved in '{args.save_dir}' directory\")\n",
        "    print(f\"{'=' * 60}\")\n",
        "\n",
        "    if args.plot_results:\n",
        "        generate_plots(args.plots_dir, metrics_data)\n",
        "        print(f\"Plots saved under '{args.plots_dir}'.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gII2Br0c7wIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Create directory for saved models\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "\n",
        "# ==================== Data Augmentation ====================\n",
        "\n",
        "class RandAugment:\n",
        "    \"\"\"RandAugment for FashionMNIST.\"\"\"\n",
        "    def __init__(self, n=2, m=9):\n",
        "        self.n = n  # Number of augmentations\n",
        "        self.m = m  # Magnitude\n",
        "\n",
        "    def __call__(self, img):\n",
        "        ops = [\n",
        "            transforms.RandomRotation(30),\n",
        "            transforms.RandomAffine(0, translate=(0.1, 0.1)),\n",
        "            transforms.RandomAffine(0, shear=15),\n",
        "        ]\n",
        "\n",
        "        for _ in range(self.n):\n",
        "            op = np.random.choice(ops)\n",
        "            img = op(img)\n",
        "\n",
        "        return img\n",
        "\n",
        "class RandomErasing:\n",
        "    \"\"\"Random erasing augmentation.\"\"\"\n",
        "    def __init__(self, p=0.5, scale=(0.02, 0.33)):\n",
        "        self.p = p\n",
        "        self.scale = scale\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if np.random.rand() > self.p:\n",
        "            return img\n",
        "\n",
        "        h, w = img.shape[-2:]\n",
        "        area = h * w\n",
        "        target_area = np.random.uniform(*self.scale) * area\n",
        "        aspect_ratio = np.random.uniform(0.3, 3.3)\n",
        "\n",
        "        h_erase = int(round(np.sqrt(target_area * aspect_ratio)))\n",
        "        w_erase = int(round(np.sqrt(target_area / aspect_ratio)))\n",
        "\n",
        "        if h_erase < h and w_erase < w:\n",
        "            i = np.random.randint(0, h - h_erase)\n",
        "            j = np.random.randint(0, w - w_erase)\n",
        "            img[:, i:i+h_erase, j:j+w_erase] = 0\n",
        "\n",
        "        return img\n",
        "\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    \"\"\"Mixup augmentation.\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index]\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0):\n",
        "    \"\"\"CutMix augmentation.\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "\n",
        "    # Get random box\n",
        "    W = x.size()[2]\n",
        "    H = x.size()[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = int(W * cut_rat)\n",
        "    cut_h = int(H * cut_rat)\n",
        "\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
        "\n",
        "    # Adjust lambda\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
        "    y_a, y_b = y, y[index]\n",
        "\n",
        "    return x, y_a, y_b, lam\n",
        "\n",
        "# ==================== Vision Transformer Implementation ====================\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Split image into patches and embed them.\"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=1, embed_dim=384):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim,\n",
        "                             kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-head self-attention mechanism.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        assert embed_dim % num_heads == 0\n",
        "\n",
        "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        scale = self.head_dim ** -0.5\n",
        "        attn = (q @ k.transpose(-2, -1)) * scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \"\"\"MLP block with GELU activation.\"\"\"\n",
        "    def __init__(self, in_features, hidden_features, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Transformer encoder block with pre-normalization.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer for image classification.\"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=1,\n",
        "                 num_classes=10, embed_dim=384, num_heads=6,\n",
        "                 num_blocks=6, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size,\n",
        "                                         in_channels, embed_dim)\n",
        "        n_patches = self.patch_embed.n_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x[:, 0])\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ==================== DeiT Implementation ====================\n",
        "\n",
        "class DeiT(nn.Module):\n",
        "    \"\"\"Data-efficient Image Transformer with distillation token.\"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=1,\n",
        "                 num_classes=10, embed_dim=384, num_heads=6,\n",
        "                 num_blocks=6, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size,\n",
        "                                         in_channels, embed_dim)\n",
        "        n_patches = self.patch_embed.n_patches\n",
        "\n",
        "        # CLS and distillation tokens\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 2, embed_dim))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Two heads: one for cls token, one for dist token\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        self.head_dist = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Add both cls and dist tokens\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        dist_tokens = self.dist_token.expand(B, -1, -1)\n",
        "        x = torch.cat([cls_tokens, dist_tokens, x], dim=1)\n",
        "\n",
        "        x = x + self.pos_embed\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Separate outputs from cls and dist tokens\n",
        "        x_cls = self.head(x[:, 0])\n",
        "        x_dist = self.head_dist(x[:, 1])\n",
        "\n",
        "        return x_cls, x_dist\n",
        "\n",
        "# ==================== ResNet18 Implementation ====================\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Basic residual block for ResNet.\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                              kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                              kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels,\n",
        "                         kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    \"\"\"ResNet18 architecture.\"\"\"\n",
        "    def __init__(self, in_channels=1, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64,\n",
        "                              kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        elif isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, 0, 0.01)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# ==================== Data Loading ====================\n",
        "\n",
        "def get_dataloaders(batch_size=128, use_augmentation=False):\n",
        "    \"\"\"Create data loaders with optional heavy augmentation.\"\"\"\n",
        "\n",
        "    if use_augmentation:\n",
        "        # Heavy augmentation for DeiT\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.Resize(32),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            RandAugment(n=2, m=9),\n",
        "            transforms.ToTensor(),\n",
        "            RandomErasing(p=0.25),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "    else:\n",
        "        # Standard transform for ViT\n",
        "        transform_train = transforms.Compose([\n",
        "            transforms.Resize(32),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize(32),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Transform for ResNet (96x96)\n",
        "    transform_resnet_train = transforms.Compose([\n",
        "        transforms.Resize(96),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    transform_resnet_test = transforms.Compose([\n",
        "        transforms.Resize(96),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.FashionMNIST(\n",
        "        root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.FashionMNIST(\n",
        "        root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_dataset_resnet = datasets.FashionMNIST(\n",
        "        root='./data', train=True, download=True, transform=transform_resnet_train)\n",
        "    test_dataset_resnet = datasets.FashionMNIST(\n",
        "        root='./data', train=False, download=True, transform=transform_resnet_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                             shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                            shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    train_loader_resnet = DataLoader(train_dataset_resnet, batch_size=batch_size,\n",
        "                                    shuffle=True, num_workers=2, pin_memory=True)\n",
        "    test_loader_resnet = DataLoader(test_dataset_resnet, batch_size=batch_size,\n",
        "                                   shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader, train_loader_resnet, test_loader_resnet\n",
        "\n",
        "# ==================== Training Functions ====================\n",
        "\n",
        "def count_parameters(model):\n",
        "    \"\"\"Count the number of trainable parameters.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def save_model(model, path, epoch, optimizer, acc):\n",
        "    \"\"\"Save model checkpoint.\"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'accuracy': acc,\n",
        "    }, path)\n",
        "    print(f\"Model saved to {path}\")\n",
        "\n",
        "def load_model_if_exists(model, model_name, device):\n",
        "    \"\"\"Load model if checkpoint exists.\"\"\"\n",
        "    final_path = f'saved_models/{model_name}_final.pth'\n",
        "    best_path = f'saved_models/{model_name}_best.pth'\n",
        "\n",
        "    if os.path.exists(final_path):\n",
        "        print(f\"Loading existing model from {final_path}\")\n",
        "        checkpoint = torch.load(final_path, map_location=device, weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        model = model.to(device)\n",
        "        print(f\"Loaded model - Epoch: {checkpoint['epoch']}, Accuracy: {checkpoint['accuracy']:.2f}%\")\n",
        "        return model, checkpoint['accuracy'], True\n",
        "\n",
        "    return model, 0.0, False\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device, use_augment=False):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Apply mixup or cutmix\n",
        "        if use_augment and np.random.rand() < 0.5:\n",
        "            if np.random.rand() < 0.5:\n",
        "                inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "            else:\n",
        "                inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "        else:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def train_epoch_deit(model, loader, teacher, criterion, optimizer, device,\n",
        "                     alpha=0.5, beta=0.5, temperature=3.0, use_augment=True):\n",
        "    \"\"\"Train DeiT for one epoch with distillation.\"\"\"\n",
        "    model.train()\n",
        "    teacher.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, targets in loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Apply mixup or cutmix\n",
        "        if use_augment and np.random.rand() < 0.5:\n",
        "            if np.random.rand() < 0.5:\n",
        "                inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)\n",
        "            else:\n",
        "                inputs, targets_a, targets_b, lam = cutmix_data(inputs, targets)\n",
        "\n",
        "            # Resize for teacher if needed\n",
        "            inputs_teacher = F.interpolate(inputs, size=(96, 96), mode='bilinear')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher(inputs_teacher)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs_cls, outputs_dist = model(inputs)\n",
        "\n",
        "            # Hard label loss with mixup\n",
        "            loss_cls = lam * criterion(outputs_cls, targets_a) + (1 - lam) * criterion(outputs_cls, targets_b)\n",
        "            loss_dist_hard = lam * criterion(outputs_dist, targets_a) + (1 - lam) * criterion(outputs_dist, targets_b)\n",
        "\n",
        "            # Soft label loss (distillation)\n",
        "            loss_kl = F.kl_div(\n",
        "                F.log_softmax(outputs_dist / temperature, dim=1),\n",
        "                F.softmax(teacher_outputs / temperature, dim=1),\n",
        "                reduction='batchmean'\n",
        "            ) * (temperature ** 2)\n",
        "\n",
        "            loss = (1 - alpha) * loss_cls + alpha * loss_dist_hard + beta * loss_kl\n",
        "        else:\n",
        "            # Resize for teacher if needed\n",
        "            inputs_teacher = F.interpolate(inputs, size=(96, 96), mode='bilinear')\n",
        "\n",
        "            with torch.no_grad():\n",
        "                teacher_outputs = teacher(inputs_teacher)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs_cls, outputs_dist = model(inputs)\n",
        "\n",
        "            # Hard label losses\n",
        "            loss_cls = criterion(outputs_cls, targets)\n",
        "            loss_dist_hard = criterion(outputs_dist, targets)\n",
        "\n",
        "            # Soft label loss (distillation)\n",
        "            loss_kl = F.kl_div(\n",
        "                F.log_softmax(outputs_dist / temperature, dim=1),\n",
        "                F.softmax(teacher_outputs / temperature, dim=1),\n",
        "                reduction='batchmean'\n",
        "            ) * (temperature ** 2)\n",
        "\n",
        "            loss = (1 - alpha) * loss_cls + alpha * loss_dist_hard + beta * loss_kl\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = outputs_cls.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def evaluate(model, loader, criterion, device, is_deit=False):\n",
        "    \"\"\"Evaluate the model.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            if is_deit:\n",
        "                outputs_cls, outputs_dist = model(inputs)\n",
        "                # Average predictions from both tokens\n",
        "                outputs = (outputs_cls + outputs_dist) / 2\n",
        "            else:\n",
        "                outputs = model(inputs)\n",
        "\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def train_model(model, train_loader, test_loader, num_epochs,\n",
        "                lr, device, model_name, use_augment=False,\n",
        "                teacher=None, is_deit=False):\n",
        "    \"\"\"Train and evaluate a model.\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Parameters: {count_parameters(model):,}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler with warmup\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < 5:\n",
        "            return (epoch + 1) / 5\n",
        "        else:\n",
        "            return 0.5 * (1 + np.cos(np.pi * (epoch - 5) / (num_epochs - 5)))\n",
        "\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "    best_acc = 0.0\n",
        "    history = {'train_loss': [], 'train_acc': [],\n",
        "               'test_loss': [], 'test_acc': []}\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_start = time.time()\n",
        "\n",
        "        if is_deit and teacher is not None:\n",
        "            train_loss, train_acc = train_epoch_deit(\n",
        "                model, train_loader, teacher, criterion,\n",
        "                optimizer, device, use_augment=use_augment\n",
        "            )\n",
        "        else:\n",
        "            train_loss, train_acc = train_epoch(\n",
        "                model, train_loader, criterion, optimizer,\n",
        "                device, use_augment=use_augment\n",
        "            )\n",
        "\n",
        "        test_loss, test_acc = evaluate(model, test_loader, criterion,\n",
        "                                      device, is_deit=is_deit)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "\n",
        "        epoch_time = time.time() - epoch_start\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.4f} Train Acc: {train_acc:.2f}% \"\n",
        "              f\"Test Loss: {test_loss:.4f} Test Acc: {test_acc:.2f}% \"\n",
        "              f\"Time: {epoch_time:.2f}s LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            # Save best model\n",
        "            save_path = f'saved_models/{model_name.replace(\" \", \"_\")}_best.pth'\n",
        "            save_model(model, save_path, epoch, optimizer, best_acc)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Save final model\n",
        "    save_path = f'saved_models/{model_name.replace(\" \", \"_\")}_final.pth'\n",
        "    save_model(model, save_path, num_epochs, optimizer, test_acc)\n",
        "\n",
        "    print(f\"\\nTraining completed in {total_time:.2f}s\")\n",
        "    print(f\"Best test accuracy: {best_acc:.2f}%\")\n",
        "    print(f\"Final test accuracy: {history['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "    return history, best_acc\n",
        "\n",
        "# ==================== Main Execution ====================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Hyperparameters\n",
        "    batch_size = 128\n",
        "    num_epochs = 30\n",
        "    lr_vit = 0.001\n",
        "    lr_resnet = 0.01\n",
        "    lr_deit = 0.001\n",
        "\n",
        "    # Get data loaders\n",
        "    print(\"Loading FashionMNIST dataset...\")\n",
        "    train_loader_vit, test_loader_vit, train_loader_resnet, test_loader_resnet = \\\n",
        "        get_dataloaders(batch_size, use_augmentation=False)\n",
        "\n",
        "    # Get augmented loaders for DeiT\n",
        "    train_loader_deit, test_loader_deit, _, _ = \\\n",
        "        get_dataloaders(batch_size, use_augmentation=True)\n",
        "\n",
        "    # ========== Train ResNet18 (Teacher) ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PHASE 1: Training ResNet18 (Teacher)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    resnet_model = ResNet18(in_channels=1, num_classes=10)\n",
        "    resnet_model, resnet_best_acc, resnet_loaded = load_model_if_exists(\n",
        "        resnet_model, \"ResNet18_Teacher\", device\n",
        "    )\n",
        "\n",
        "    if not resnet_loaded:\n",
        "        resnet_history, resnet_best_acc = train_model(\n",
        "            resnet_model, train_loader_resnet, test_loader_resnet,\n",
        "            num_epochs, lr_resnet, device, \"ResNet18_Teacher\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"Skipping training - using loaded model\")\n",
        "        resnet_history = {'test_acc': [resnet_best_acc]}\n",
        "\n",
        "    # ========== Train ViT (Baseline) ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PHASE 2: Training Vision Transformer (Baseline)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    vit_model = VisionTransformer(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=1,\n",
        "        num_classes=10,\n",
        "        embed_dim=384,\n",
        "        num_heads=6,\n",
        "        num_blocks=6,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    vit_model, vit_best_acc, vit_loaded = load_model_if_exists(\n",
        "        vit_model, \"Vision_Transformer\", device\n",
        "    )\n",
        "\n",
        "    if not vit_loaded:\n",
        "        vit_history, vit_best_acc = train_model(\n",
        "            vit_model, train_loader_vit, test_loader_vit,\n",
        "            num_epochs, lr_vit, device, \"Vision_Transformer\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"Skipping training - using loaded model\")\n",
        "        vit_history = {'test_acc': [vit_best_acc]}\n",
        "\n",
        "    # ========== Train DeiT with Distillation ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PHASE 3: Training DeiT with Knowledge Distillation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    deit_model = DeiT(\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "        in_channels=1,\n",
        "        num_classes=10,\n",
        "        embed_dim=384,\n",
        "        num_heads=6,\n",
        "        num_blocks=6,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    )\n",
        "\n",
        "    deit_model, deit_best_acc, deit_loaded = load_model_if_exists(\n",
        "        deit_model, \"DeiT_Distilled\", device\n",
        "    )\n",
        "\n",
        "    if not deit_loaded:\n",
        "        # Load best ResNet as teacher\n",
        "        if not resnet_loaded:\n",
        "            checkpoint = torch.load('saved_models/ResNet18_Teacher_best.pth',\n",
        "                                   map_location=device, weights_only=False)\n",
        "            resnet_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        resnet_model = resnet_model.to(device)\n",
        "        resnet_model.eval()\n",
        "\n",
        "        deit_history, deit_best_acc = train_model(\n",
        "            deit_model, train_loader_deit, test_loader_deit,\n",
        "            num_epochs, lr_deit, device, \"DeiT_Distilled\",\n",
        "            use_augment=True, teacher=resnet_model, is_deit=True\n",
        "        )\n",
        "    else:\n",
        "        print(\"Skipping training - using loaded model\")\n",
        "        deit_history = {'test_acc': [deit_best_acc]}\n",
        "\n",
        "    # ========== Final Comparison ==========\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"FINAL COMPARISON\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nResNet18 (Teacher):\")\n",
        "    print(f\"  Parameters: {count_parameters(resnet_model):,}\")\n",
        "    print(f\"  Best Test Acc: {resnet_best_acc:.2f}%\")\n",
        "    if not resnet_loaded:\n",
        "        print(f\"  Final Test Acc: {resnet_history['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(f\"\\nVision Transformer (Baseline):\")\n",
        "    print(f\"  Parameters: {count_parameters(vit_model):,}\")\n",
        "    print(f\"  Best Test Acc: {vit_best_acc:.2f}%\")\n",
        "    if not vit_loaded:\n",
        "        print(f\"  Final Test Acc: {vit_history['test_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(f\"\\nDeiT (Student with Distillation):\")\n",
        "    print(f\"  Parameters: {count_parameters(deit_model):,}\")\n",
        "    print(f\"  Best Test Acc: {deit_best_acc:.2f}%\")\n",
        "    if not deit_loaded:\n",
        "        print(f\"  Final Test Acc: {deit_history['test_acc'][-1]:.2f}%\")\n",
        "    print(f\"  Improvement over ViT: {deit_best_acc - vit_best_acc:+.2f}%\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"All models saved in 'saved_models/' directory\")\n",
        "    print(f\"{'='*60}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "7Cr-d6UauxRL",
        "outputId": "2b311436-0347-4395-9905-5e4dc9556e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3343419509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2262\u001b[0m \u001b[0;31m# quantization depends on torch.fx and torch.ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m \u001b[0;31m# Import quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquantization\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;31m# Import the quasi random sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mypy: allow-untyped-defs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfake_quantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuse_modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuse_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/quantization/fake_quantize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch.ao.quantization.fake_quantize import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_is_fake_quant_script_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_is_per_channel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuser_method_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m from .pt2e._numeric_debugger import (  # noqa: F401\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mcompare_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mCUSTOM_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/_numeric_debugger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_sqnr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpt2e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbfs_trace_with_node_process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/pt2e/graph_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExportedProgram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m from torch.fx.passes.utils.source_matcher_utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/export/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPassResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfra\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpass_manager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPassManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFileLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mgraph_drawer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgraph_manipulation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnet_min_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moperator_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/graph_drawer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_format_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_qualified_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator_schemas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_prop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/passes/shape_prop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect_fake_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefinitely_contiguous_for_memory_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_sparse_any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compatibility\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_aggregate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from torch._subclasses.fake_tensor import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mDynamicOutputShapeException\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mFakeTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mFakeTensorMode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_class_registry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFakeScriptObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfake_profile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMissingOpProfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtrace_structured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuggest_memory_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m from torch._subclasses.meta_utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_logging/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 2. register any artifacts (<artifact_name> below) in torch._logging._registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#   a. call getArtifactLogger(__name__, <artifact_name>) at your logging site instead of the standard logger to log your artifact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m from ._internal import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_logging/_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# flake8: noqa: B950\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_artifact\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m DYNAMIC = [\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iNwGfaUcWJC"
      },
      "outputs": [],
      "source": []
    }
  ]
}